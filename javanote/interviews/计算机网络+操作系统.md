## 1. 计算机网络

#### 1.1 请介绍七层网络体系结构。

**参考回答**

1. 为什么分七层

    支持异构网络的互联互通。

2. 七层分别负责的内容（功能）

    ​    OSI 模型把网络通信的工作分为 7 层，从下到上分别是**物理层、数据链路层、网络层、传输层、会话层、表示层和应用层**。

    (1) 物理层

    ​    任务：透明地传输比特流。

    ​    功能：为数据段设备提供传送数据通路

    ​    传输单位：比特

    ​    所实现的硬件：集线器，中继器

    （2）数据链路层

    ​    任务：将网络层传输下来的IP数据报组装成帧

    ​    功能：a. 链路连接的建立、拆除和分离

    ​                b. 帧定界和帧同步

    ​                c.差错检测

    ​    传输单位：帧

    ​    所实现的硬件：交换机、网桥

    ​    协议：PPP,HDLC、SDLC、STP、ARQ

    (3)网络层

    ​    任务：a. 将传输层传下来的报文段封装成分组

    ​                b.选择合适的路由，使得传输层传下来的分组能够交付到目的主机

    ​    功能：a. 为传输层提供服务

    ​                b. 组包和拆包

    ​                c. 路由选择

    ​                d.拥塞控制

    ​    传输单位：数据段

    ​    所实现的硬件：路由器

    ​    协议：ICMP、ARP、RARP、IP、IGMP、OSPF

    (4)传输层

    ​    任务：负责主机中两个进程之间的通信

    ​    功能：

    ​            a. 为端到端连接提供可靠的服务

    ​            b. 为端到端连接提供流量控制、差错控制、服务质量等管理服务

    ​    传输单位：报文段（TCP）或用户数据报（UDP）

    ​    协议：TCP、UDP

    (5)会话层

    ​    任务：不同主机上各进程间的对话

    ​    功能：管理主机间的会话进程，包括建立、管理以及终止进程间的会话。是一种端到端的服务

    (6)表示层

    ​    负责处理在两个内部数据表示结构不同的通信系统之间交换信息的表示格式，为数据加密和解密以及为提高传输效率提供必需的数据压缩以及解压等功能。

    (7)应用层

    ​    任务：提供系统与用户的接口

    ​    功能：

    ​            a.文件传输

    ​            b. 访问和管理

    ​            c. 电子邮件服务

    ​    协议：FTP、SMTP、POP3、HTTP、DNS、TELnet

#### 1.2 请介绍五层网络体系结构。

**参考回答**

五层网络体系结构分分别为：**应用层、运输层、网络层、数据链路层、物理层**。各层功能分别如下：

1. 第五层——**应用层**(application layer)

    (1) 应用层(application layer)：是体系结构中的最高。直接为用户的应用进程提供服务。

    (2) 在因特网中的应用层协议很多，如支持万维网应用的HTTP协议，支持电子邮件的SMTP协议，支持文件传送的FTP协议等等。

2. 第四层——**运输层**(transport layer)

    (1) 运输层(transport layer)：负责向两个主机中进程之间的通信提供服务。由于一个主机可同时运行多个进程，因此运输层有**复用**和**分用**的功能。

    ​    a. 复用，就是多个应用层进程可同时使用下面运输层的服务。

    ​    b. 分用，就是把收到的信息分别交付给上面应用层中相应的进程。

    (2) **运输层主要使用以下两种协议：**     **(1) 传输控制协议TCP(Transmission Control Protocol)：**面向连接的，数据传输的单位是报文段，能够提供可靠的交付。     **(2) 用户数据包协议UDP(User Datagram Protocol)：**无连接的，数据传输的单位是用户数据报，不保证提供可靠的交付，只能提供“尽最大努力交付”。

3. 第三层——**网络层**(network layer)

    ​    网络层(network layer)主要包括以下两个任务：

    ​    (1) 负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层残生的报文段或用户数据报封装成分组或包进行传送。在TCP/IP体系中，由于网络层使用IP协议，因此分组也叫做IP数据报，或简称为数据报。

    ​    (2) 选中合适的路由，使源主机运输层所传下来的分组，能够通过网络中的路由器找到目的主机。

4. 第二层——**数据链路层**(data link layer)

    ​    **数据链路层(data link layer)：**常简称为链路层，我们知道，两个主机之间的数据传输，总是在一段一段的链路上传送的，也就是说，在两个相邻结点之间传送数据是直接传送的(点对点)，这时就需要使用专门的链路层的协议。

    ​    在两个相邻结点之间传送数据时，数据链路层将网络层交下来的IP数据报组装成帧(framing)，在两个相邻结点之间的链路上“透明”地传送帧中的数据。

    ​    每一帧包括数据和必要的控制信息(如同步信息、地址信息、差错控制等)。典型的帧长是几百字节到一千多字节。

    **注意**：”透明”是一个很重要的术语。它表示，某一个实际存在的事物看起来却好像不存在一样。”在数据链路层透明传送数据”表示无力什么样的比特组合的数据都能够通过这个数据链路层。因此，对所传送的数据来说，这些数据就“看不见”数据链路层。或者说，数据链路层对这些数据来说是透明的。     (1) 在接收数据时，控制信息使接收端能知道一个帧从哪个比特开始和到哪个比特结束。这样，数据链路层在收到一个帧后，就可从中提取出数据部分，上交给网络层。     (2) 控制信息还使接收端能检测到所收到的帧中有无差错。如发现有差错，数据链路层就简单地丢弃这个出了差错的帧，以免继续传送下去白白浪费网络资源。如需改正错误，就由运输层的TCP协议来完成。

5. 第一层——**物理层**(physical layer)

    **物理层(physical layer)：**在物理层上所传数据的单位是比特。物理层的任务就是透明地传送比特流。    

#### 1.3 了解网络编程协议吗？客户端发送给服务器的请求，怎么确定具体的协议？

**参考回答**

​    了解，客户端发送给服务器端的请求，可以根据统一资源定位系统（uniform resource locator，URL）来确定具体使用的协议。

**答案解析**

​    一个完整的URL包括–协议部分、网址、文件地址部分。协议部分以//为分隔符，在interner中，我们可以使用多种协议：

​    HTTP——HyperText Transfer Protocol（超文本传输协议）

​    FTP——File Transfer Protocol（文件传输协议）

​    Gopher——The Internet Gopher Protocol(网际Gopher协议)

​    File——本地文件传输协议

​    HTTPS——安全套接字层超文本传输协议(http的安全版)

​    **例如**百度网址：[http://baidu.com](http://baidu.com/)，可以看出使用的是http协议。

#### 1.4 TCP、HTTP、FTP分别属于哪一层？

**参考回答**

​    TCP、HTTP、FTP分别属于传输层、应用层、应用层。

**答案解析**

1. TCP协议简介

    （1）TCP协议的特性

    ​    TCP是面向连接的，提供全双工的服务，数据流可以双向传输。也是点对点的，即在单个发送放方和单个接收方之间的连接。

    （2）TCP 报文段结构

    ​    **序号**：TCP 的序号是数据流中的字节数，不是分组的序号。表示该报文段数据字段首字节的序号。

    ​    **确认号**：TCP 使用累积确认，确认号是第一个未收到的字节序号，表示希望接收到的下一个字节。

    ​    **首部长度**：通常选项字段为空，所以一般 TCP 首部的长度是 **20** **字节**。

    ​    **选项字段**(可选与变长的)：用于发送方与接收方协商 MSS(最大报文段长)，或在高速网络环境下用作窗口

    调节因子。

    ​    **标志字段**

    ​        **ACK**：指示确认字段中的值是有效的

    ​        **RST,SYN,FIN**：连接建立与拆除

    ​        **PSH**：指示接收方应立即将数据交给上层

    ​        **URG**：报文段中存在着(被发送方的上层实体置位)“紧急”的数据

    ​    **接收窗口**：用于流量控制（表示接收方还有多少可用的缓存空间）。

    ​    TCP RFC 并没有规定失序到达的分组应该如何处理，而是交给程序员。可以选择丢弃或保留。如果发生超时，TCP **只重传第一个已发送而未确认的分组**，超时时间间隔会设置为原来的 2 倍。

    （3）流量控制

    ​    如果应用程序读取数据相当慢，而发送方发送数据太多、太快，会很容易使接收方的接收缓存溢出，流量控制就是用来进行发送速度和接收速度的匹配。发送方维护一个“接收窗口”变量，这个变量表示接收方当前可用的缓存空间。

2. HTTP（超文本传输协议）简介

    （1）HTTP协议的特性

    ​    HTTP 协议一共有**五大特点**：a. 支持客户/服务器模式；b. 简单快速；c. 灵活；d. 无连接；e. 无状态。

    ​    **无连接含义：**限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。

    ​    **无状态含义：**指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。即我们给服务器发送 HTTP 请求之后，服务器根据请求，会给我们发送数据过来，但是，发送完，不会记录任何信息。

    （2）HTTP 客户机及服务器

    ​    HTTP 客户机：web 浏览器

    ​    HTTP 服务器：web 服务器，包含 web 对象（HTML 文件、JPEG 文件、java 小程序、视频片段等）

    （3）HTTP 方法字段：

    ​    **GET**：绝大部分 HTTP 请求报文使用 GET 方法

    ​    **POST**：用户提交表单时（如向搜索引擎提供关键字），但提交表单不一定要用 POST 方法

    ​    **HEAD**：类似于 GET，区别在于服务器返回的响应报文中不包含请求对象（常用于故障跟踪）

    ​    **PUT**：用于向服务器上传对象

    ​    **DELETE**：用于删除服务器上的对象

    （4）HTTP 状态信息

    ​    **301 Permanently Moved** 被请求的资源已永久移动到新位置，新的URL在Location头中给出，浏览器应该自动地访问新的URL。

    ​    **302 Found** 请求的资源现在临时从不同的URL响应请求。301是永久重定向，而302是临时重定向。

    ​    **200 OK** 表示从客户端发来的请求在服务器端被正确处理

    ​    **304 Not Modified** 304状态码是告诉浏览器可以从缓存中获取所请求的资源。

    ​    **400 bad request** 请求报文存在语法错误

    ​    **403 forbidden** 表示对请求资源的访问被服务器拒绝

    ​    **404 not found** 表示在服务器上没有找到请求的资源

    ​    **500 internal sever error** 表示服务器端在执行请求时发生了错误

    ​    **503 service unavailable** 表明服务器暂时处于超负载或正在停机维护，无法处理请求

    （4）HTTP中常见的文件格式

    ​    text/html： HTML格式

    ​    text/plain：纯文本格式

    ​    image/jpeg：jpg图片格式

    ​    application/json： JSON数据格式

    ​    application/x-www-form-urlencoded： form表单数据被编码为key/value格式发送到服务器（表单默认的提交数据格式）

    ​    multipart/form-data： 在表单中进行文件上传时使用

3. FTP（文件传输协议简介)

    FTP 使用两个并行的 TCP 连接来传输文件：

    （1）**控制连接（持久）：**传输控制信息，如用户标识、口令、改变远程目录命令、文件获取上传的命令；

    （2）**数据连接（非持久）：**传输实际文件。

    ​    FTP 客户机发起向 FTP 服务器的控制连接，然后在该连接上发送用户标识和口令、改变远程目录的命令。FTP服务器收到命令后，发起一个到客户机的数据连接，在该连接上准确地传送一个文件并关闭连接。

    ​    **有状态的协议**：FTP 服务器在整个会话期间保留用户的状态信息。服务器必须把特定的用户账号和控制连接

    联系起来。

4. 传输层简介

    （1）传输层的服务基本原理

    ​        a. 多路复用和解复用（分路）技术

    ​            **复用：**发送方的不同的应用进程都可以使用同一个传输层协议传送数据；

    ​            **分路技术**：接收方的传输层剥去报文首部之后能把这些数据正确的传输到正确的应用进程上。

    ​        b. 可靠数据传输

    ​        c. 流量控制和拥塞控制

    （2）传输层提供的服务

    ​        a. 传输层寻址和端口

    ​        端口号就是用来标识应用进程的数字标识。其端口号的长度为16Bit;也就是能够标识2^16个不同的端口号。另外端口号根据端口范围分为2类。分别为**服务端使用的端口号（熟知端口号数值范围：0-1023；登记端口号数值范围：1024-49151）**和**客户端使用的端口号（数值范围为49152-65535）**。常见端口号如下：

    ​                **FTP**：21

    ​                **TELNET**：23

    ​                **SMTP**：25

    ​                **DNS**：53

    ​                **TFTP**：69

    ​                **HTTP**：80

    ​                **SNMP**：161

    ​        b. 无连接服务和面向连接服务

    （3）流量控制和拥塞控制

    ​        a. **流量控制：**如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。

    ​        b. **拥塞控制：**拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。

    ​        两者的区别：流量控制是为了预防拥塞。如：在马路上行车，交警跟红绿灯是流量控制，当发生拥塞时，如何进行疏散，是拥塞控制。流量控制指点对点通信量的控制。而拥塞控制是全局性的，涉及到所有的主机和降低网络性能的因素。

5. 应用层简介

    ​    应用层的具体内容就是规定应用进程在通信时所遵循的协议。应用层协议分类如下：

    ​    (1). 域名系统(Domain Name System，DNS)：用于实现网络设备名字到IP地址映射的网络服务。

    ​    (2). 文件传输协议(File Transfer Protocol，FTP)：用于实现交互式文件传输功能。

    ​    (3). 简单邮件传送协议(Simple Mail Transfer Protocol, SMTP)：用于实现电子邮箱传送功能

    ​    (4). 超文本传输协议(HyperText Transfer Protocol，HTTP)：用于实现WWW服务。

    ​    (5). 简单网络管理协议(simple Network Management Protocol，SNMP)：用于管理与监视网络设备。

    ​    (6). 远程登录协议(Telnet)：用于实现远程登录功能。

#### 1.5 讲一下TCP/IP协议。

**参考回答**

1. TCP/IP协议定义

    ​    **TCP/IP**（Transmission Control Protocol/Internet Protocol，传输控制协议/网际协议）是指能够在多个不同网络间实现信息传输的协议簇。TCP/IP协议不仅仅指的是TCP和IP两个协议，而是指一个由FTP、SMTP、TCP、UDP、IP等协议构成的协议簇， 只是因为在TCP/IP协议中TCP协议和IP协议最具代表性，所以被称为TCP/IP协议。

2. TCP/IP协议组成

    TCP/IP结构模型分为**应用层、传输层、网络层、链路层（网络接口层）**四层，以下是各层的详细介绍：

    **（1）应用层**

    ​    应用层是TCP/IP协议的第一层，是直接为应用进程提供服务的。

    ​    a. 对不同种类的应用程序它们会根据自己的需要来使用应用层的不同协议，邮件传输应用使用了SMTP协议、万维网应用使用了HTTP协议、远程登录服务应用使用了有TELNET协议。

    ​    b. 应用层还能加密、解密、格式化数据。

    ​    c. 应用层可以建立或解除与其他节点的联系，这样可以充分节省网络资源。

    **（2）传输层**

    ​    作为TCP/IP协议的第二层，运输层在整个TCP/IP协议中起到了中流砥柱的作用。且在运输层中，TCP和UDP也同样起到了中流砥柱的作用。

    **（3）网络层**

    ​    网络层在TCP/IP协议中的位于第三层。在TCP/IP协议中网络层可以进行网络连接的建立和终止以及IP地址的寻找等功能。

    **（4）链路层（网络接口层）**

    ​    在TCP/IP协议中，网络接口层位于第四层。由于网络接口层兼并了物理层和数据链路层。所以，网络接口层既是传输数据的物理媒介，也可以为网络层提供一条准确无误的线路。

3. TCP/IP协议特点

    ​    TCP/IP协议能够迅速发展起来并成为事实上的标准，是它恰好适应了世界范围内数据通信的需要。它有以下特点：

    （1）协议标准是完全开放的，可以供用户免费使用，并且独立于特定的计算机硬件与操作系统；

    （2）独立于网络硬件系统，可以运行在广域网，更适合于互联网；

    （3）网络地址统一分配，网络中每一设备和终端都具有一个唯一地址；

    （4）高层协议标准化，可以提供多种多样可靠网络服务。

#### 1.6 说一说你对ARP协议的理解。

**参考回答**

​    ARP协议即**地址解析协议**，是根据IP地址获取MAC地址的一个网络层协议。

1. 工作原理

    ​    ARP首先会发起一个请求数据包，数据包的首部包含了目标主机的IP地址，然后这个数据包会在链路层进行再次包装，生成以太网数据包，最终由以太网广播给子网内的所有主机，每一台主机都会接收到这个数据包，并取出标头里的IP地址，然后和自己的IP地址进行比较，如果相同就返回自己的MAC地址，如果不同就丢弃该数据包。ARP接收返回消息，以此确定目标机的MAC地址；与此同时，ARP还会将返回的MAC地址与对应的IP地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。

2. 工作过程

    ​    主机A的IP地址为192.168.1.1，MAC地址为0A-11-22-33-44-01；

    ​    主机B的IP地址为192.168.1.2，MAC地址为0A-11-22-33-44-02；

    ​    当主机A要与主机B通信时，地址解析协议可以将主机B的IP地址（192.168.1.2）解析成主机B的MAC地址，以下为工作流程：

    ​    第1步：根据主机A上的路由表内容，IP确定用于访问主机B的转发IP地址是192.168.1.2。然后A主机在自己的本地ARP缓存中检查主机B的匹配MAC地址。

    ​    第2步：如果主机A在ARP缓存中没有找到映射，它将询问192.168.1.2的硬件地址，从而将ARP请求帧广播到本地网络上的所有主机。源主机A的IP地址和MAC地址都包括在ARP请求中。本地网络上的每台主机都接收到ARP请求并且检查是否与自己的IP地址匹配。如果主机发现请求的IP地址与自己的IP地址不匹配，它将丢弃ARP请求。

    ​    第3步：主机B确定ARP请求中的IP地址与自己的IP地址匹配，则将主机A的IP地址和MAC地址映射添加到本地ARP缓存中。

    ​    第4步：主机B将包含其MAC地址的ARP回复消息直接发送回主机A。

    ​    第5步：当主机A收到从主机B发来的ARP回复消息时，会用主机B的IP和MAC地址映射更新ARP缓存。本机缓存是有生存期的，生存期结束后，将再次重复上面的过程。主机B的MAC地址一旦确定，主机A就能向主机B发送IP通信了。

3. ARP报文格式

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697422455/9C5D6B97044F9A41F70B36D9596156F7)

    ​    硬件类型：指明了发送方想知道的硬件接口类型，以太网的值为1；

    ​    协议类型：指明了发送方提供的高层协议类型，IP为0800（16进制）；

    ​    硬件地址长度和协议长度：指明了硬件地址和高层协议地址的长度，这样ARP报文就可以在任意硬件和任意协议的网络中使用；

    ​    操作类型：用来表示这个报文的类型，ARP请求为1，ARP响应为2，RARP请求为3，RARP响应为4；

    ​    发送方硬件地址（0-3字节）：源主机硬件地址的前3个字节；

    ​    发送方硬件地址（4-5字节）：源主机硬件地址的后3个字节；

    ​    发送方IP地址（0-1字节）：源主机硬件地址的前2个字节；

    ​    发送方IP地址（2-3字节）：源主机硬件地址的后2个字节；

    ​    目标硬件地址（0-1字节）：目的主机硬件地址的前2个字节；

    ​    目标硬件地址（2-5字节）：目的主机硬件地址的后4个字节；

    ​    目标IP地址（0-3字节）：目的主机的IP地址。

#### 1.7 IP协议包含哪些字段？

**参考回答**

​    IP所包含字段结构图如下：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697447355/BD33841E8034815033F8177D23000A45)

​        **IP协议包含字段如下：**

​        **4位版本号：**指定IP协议的版本，对于IPv4来说就是4

​        **4位头部长度：**IP头部长度有多少个4字节，所以头部最大长度就是15*4=60字节

​        **8位服务类型：**3位优先权(已弃用)，4位TOS字段，1位保留字段(必须设置为0)。4为TOS为：最小延时，最大吞吐量，最高可靠性，最小成本，这四个只能选择一个

​        **16位总长度：**IP数据报整体占多少字节

​        **16为标识：**唯一的标识主机发送的报文，IP报文在数据链路层被分片，那么每一个片中的标识都是相同的

​        **3位标志字段：**第一位保留，第二位置1表示进制分片(报文长度超过MTU，丢弃报文)，第三位更多分片，最后一个分片是1，其他是0

​        **13位分片偏移：**相对于原始IP报文开始处的偏移

​        **8位生存时间：**数据报到达目的地的最大报文跳数，每经过一个路由，TTL-=1，一直到0都没有到达目的地，报文丢弃。

​        **8位协议**：表示上层协议类型，把IP交给TCP还是UDP，其中ICMP是1，TCP是6，UDP是17

​        **16位头部校验和：**使用CRC校验，鉴别头部是否损坏

​        **32位源地址和32位目标地址：**表示发送端和接收端

#### 1.8 应用层都包含什么协议？

**参考回答**

​    应用层包含的协议有**DNS、FTP、SMTP、HTTP、SNMP、Telnet**等，其作用分别如下：

1. **域名系统(Domain Name System，DNS)**：用于实现网络设备名字到IP地址映射的网络服务。
2. **文件传输协议(File Transfer Protocol，FTP)**：用于实现交互式文件传输功能。
3. **简单邮件传送协议(Simple Mail Transfer Protocol, SMTP)**：用于实现电子邮箱传送功能。
4. **超文本传输协议(HyperText Transfer Protocol，HTTP)**：用于实现WWW服务。
5. **简单网络管理协议(simple Network Management Protocol，SNMP)**：用于管理与监视网络设备。
6. **远程登录协议(Telnet)**：用于实现远程登录功能。

**答案解析**

​    应用层协议定义了运行在不同端系统上的应用程序进程如何相互传递消息。特别是定义了:

1. 交换的消息类型，如请求消息和响应消息。
2. 各种消息类型的语法，如消息中的各个字段及其详细描述。
3. 字段的语义，即包含在字段中的信息的含义。
4. 进程何时、如何发送消息及对消息进行响应的规则。
5. 有些应用层协议是由RFC文档定义的，因此它们位于公共领域，例如HTTP。
6. 有些应用层协议是公司或者个人私有的，位于私人领域，例如QQ。

#### 1.9 应用层报文怎么传输到另一个应用层？

**参考回答**

​    **应用层数据（报文）向外发送时，数据是由最上面的应用层向下经过一层层封装后发送给物理层；而接收数据时，数据是由物理层向上经过一层层解封后发给应用层。**数据的封装和解封过程如下:

1. 数据的封装过程简介

    ​    传输层及其以下的机制由内核提供， 应用层由用户进程提供， 应用程序对通讯数据的含义进行解释， 而传输层及其以下处理通讯的细节，将数据从一台计算机通过一定的路径发送到另一台计算机。 应用层数据通过协议栈发到网络上时，每层协议都要加上一个相对应的头部（header ），该过程称为**封装**封装过程如下图所示：

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697463102/73450CE33939E337E331B56A458F0850)

2. 数据的解封过程简介

    ​    不 同 的 协 议 层 对 数 据 包 有 不 同的 称 谓 ，在 传 输 层 叫 做 段（segment ），在网络层叫做数据报（ datagram) ，在链路层叫做帧（frame ）。数据封装成帧后发到传输介质上，到达目的主机后，每层协议再剥掉相应的头部，最后将应用层数据交给应用程序处理，该过程称为**解封**。解封过程如下图所示：

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697473792/56AAA732F1992ACAAD8B8AEF27B73C3F)

3. 举例说明数据封装和解封装过程

    （1）从计算机 A 的应用层内网通软件向计算机 B 发出一个消息，生成数据；

    （2）请求从计算机 A 的应用层下到 计算机A 的传输层，传输层在上层数据前面加上 TCP报头，报头中包括目标端口以及源端口；

    （3）传输层数据下到网络层，计算机A 在网络层封装，源 IP地址为 计算机A地址，目标 IP地址为 计算机 B 地址；

    （4）计算机 A 将计算机B 的 IP 地址和子网掩码与自己做比对，可以发现 计算机 B 与自己处于相同的子网。所以数据传输不必经过网关设备；

    （5）数据包下到 计算机 A 的数据链路层进行封装，源 MAC 地址为 计算机A的 MAC地址，目标 MAC 地址查询自己的 ARP 表。

    （6）计算机 A 把帧转换成 bit 流，从物理接口网卡发出；

    （7）物理层接收到电信号，把它交给数据链路层进行查看帧的目标 MAC 地址，和自己是否相等，如果相等说明该帧是发送给自己的，于是将MAC帧头解开并接着上传到网络层；

    （8）网络层查看目标 IP 地址和自己是否匹配，如果匹配即解开 IP 头封装。然后再把数据上传到传输层；

    （9）传输层解开对应的包头之后，继续把数据传给应用层，计算机 B 即可接收到计算机 A 发的消息。    

**答案解析**

**报文、报文段、分组、包、数据报、帧、数据流的概念区别如下：**

1. 报文（message）

    ​    我们将位于**应用层**的信息分组称为报文。报文是网络中交换与传输的数据单元，也是网络传输的单元。报文包含了将要发送的完整的数据信息，其长短不需一致。报文在传输过程中会不断地封装成分组、包、帧来传输，封装的方式就是添加一些控制信息组成的首部，那些就是报文头。

2. 报文段（segment）

    ​    通常是指起始点和目的地都是**传输层**的信息单元。

3. 分组/包（packet）

    ​    分组是在网络中传输的二进制格式的单元，为了提供通信性能和可靠性，每个用户发送的数据会被分成多个更小的部分。在每个部分的前面加上一些必要的控制信息组成的首部，有时也会加上尾部，就构成了一个分组。它的起始和目的地是**网络层**。

4. 数据报（datagram）

    ​    面向无连接的数据传输，其工作过程类似于报文交换。采用数据报方式传输时，被传输的分组称为数据报。通常是指起始点和目的地都使用无连接网络服务的的**网络层**的信息单元。

5. 帧（frame）

    ​    帧是**数据链路层**的传输单元。它将上层传入的数据添加一个头部和尾部，组成了帧。它的起始点和目的点都是数据链路层。

6. 数据单元（data unit）

    ​    指许多信息单元。常用的数据单元有服务数据单元（SDU）、协议数据单元（PDU）。

    ​    SDU是在同一机器上的两层之间传送信息。PDU是发送机器上每层的信息发送到接收机器上的相应层（同等层间交流用的）。

#### 1.10 介绍一下tcp的三次握手。

**参考回答**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697492737/6AE9153E66B4345D455E8C5842352205)

1. 第一次握手：建立连接时，客户端发送syn包（syn=x）到服务器，并进入**SYN_SENT**状态，等待服务器确认；SYN：同步序列编号（Synchronize Sequence Numbers）。
2. 第二次握手：服务器收到syn包，必须确认客户的SYN（ack=x+1），同时自己也发送一个SYN包（syn=y），即SYN+ACK包，此时服务器进入**SYN_RECV**状态；
3. 第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=y+1），此包发送完毕，客户端和服务器进入**ESTABLISHED**（TCP连接成功）状态，完成三次握手。

#### 1.11 介绍一下tcp的四次挥手。

**参考回答**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697582360/95BEBBEC5A8D1A75D8FE435BD5FDBA12)

1. 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。
2. 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。
3. 客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。
4. 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。
5. 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。
6. 服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。

#### 1.12 为什么需要四次挥手？

**参考回答**

1. 四次挥手示意图

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697595713/CEA78F620A6D63E63B355FA2C0373E33)

2. 四次挥手过程

    （1）客户端向服务器发送FIN控制报文段（首部中的 FIN 比特被置位）；

    （2）服务端收到FIN，回复ACK。服务器进入关闭等待状态，发送FIN;

    （3）客户端收到FIN，给服务器回复ACK，客户端进入等待状态（进入“等待”，以确保服务器收到ACK真正关闭连接）;

    （4）服务端收到ACK，链接关闭。

3. **四次挥手原因**

    ​    TCP协议是一种**面向连接的、可靠的、基于字节流的**运输层通信协议。TCP是**全双工模式**，这就意味着，当客户端发出FIN报文段时，只是表示客户端已经没有数据要发送了，客户端告诉服务器，它的数据已经全部发送完毕了；但是，这个时候客户端还是可以接受来自服务端的数据；当服务端返回ACK报文段时，表示它已经知道客户端没有数据发送了，但是服务端还是可以发送数据到客户端的；当服务端也发送了FIN报文段时，这个时候就表示服务端也没有数据要发送了，就会告诉客户端，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。

    ​    简单地说，前 2 次挥手用于关闭一个方向的数据通道，后两次挥手用于关闭另外一个方向的数据通道。

#### 1.13 为什么要有最后一次ACK？

**参考回答**

1. 三次握手示意图

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697608907/3A588F923BE7D29034DABEB3AB473BCE)

2. 四次挥手过程

    （1）客户端发送一个SYN0给服务器（选择初始序列号，不携带任何数据）

    （2）服务器收到SYN0，回复SYN1和ACK（服务器分配缓存，选择自己初始序列号）

    （3）客户端收到SYN1、ACK，回复ACK（可以包含数据）

3. **为什么要有最后一次ACK**

    ​    客户端首先向服务器发送一个连接请求，但是可能这个连接请求走了远路，等了很长时间，服务器都没有收到，那么客户端可能会再次发送，此时服务器端收到并且回复SYN、ACK；在这个时候最先发送的那个连接请求到达服务器，那么服务器会回复一个SYN，ACK；但是客户端表示自己已经收到确认了，并不搭理这个回复，那么服务器可能陷入等待，如果这种情况多了，那么会导致服务器瘫痪，所以要发送第三个确认。

#### 1.14 介绍一下tcp粘包、拆包的机制。

**参考回答**

1. TCP粘包和拆包问题

    ​    TCP是一个“流”协议，所谓流，就是**没有界限的一长串二进制数据**。TCP作为传输层协议并不了解上层业务数据的具体含义，它会根据TCP缓冲区的实际情况进行数据包的划分，所以在业务上认为是一个完整的包，可能会被TCP拆分成多个包进行发送，也有可能把多个小的包封装成一个大的数据包发送，这就是所谓的TCP粘包和拆包问题。

2. 产生TCP粘包和拆包的原因

    ​    我们知道TCP是以流动的方式传输数据的，传输的**最小单位**为一个报文段（Segment）。TCP Header中有个Options标识位。常见的标识位为MSS（Maximum Segment Size）指的是，连接层每次传输的数据有个最大限制MTU（Maximum Transmission Unit），一般是1500bit，超过这个量要分成多个报文段，MSS则是这个最大限制减去TCP的header，光是要传输的数据的大小，一般为1460bit。换算成字节，也就是180多字节。     TCP为提高性能，发送端会将需要发送的数据发送到缓冲区，等待缓冲区满了以后，再将缓冲中的数据发送到接收方。同理，接收方也有缓冲区这样的机制来接受数据。 发生TCP粘包、拆包主要是以下原因：     （1）应用程序写入数据大于套接字缓冲区大小，会发生拆包；     （2）应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发送粘包；     （3）进行MSS（最大报文长度）大小的TCP分段，当TCP报文长度——TCP header长度>MSS 的时候会发生拆包；     （4）接收方法不及时读取套接字缓冲区数据，这将发生粘包。

3. 如何处理粘包和拆包

    ​    **假设应用层协议是http**

    ​    我从浏览器中访问了一个网站，网站服务器给我发了200k的数据。建立连接的时候，通告的MSS是50k，所以为了防止ip层分片，tcp每次只会发送50k的数据，一共发了4个tcp数据包。如果我又访问了另一个网站，这个网站给我发了100k的数据，这次tcp会发出2个包，问题是，客户端收到6个包，怎么知道前4个包是一个页面，后两个是一个页面。既然是tcp将这些包分开了，那tcp会将这些包重组吗，它送给应用层的是什么？这是我自己想的一个场景，正式一点讲的话，**这个现象叫拆包**。

    ​    我们再考虑一个问题。

    ​    tcp中有一个negal算法，用途是这样的：通信两端有很多小的数据包要发送，虽然传送的数据很少，但是流程一点没少，也需要tcp的各种确认，校验。这样小的数据包如果很多，会造成网络资源很大的浪费，negal算法做了这样一件事，当来了一个很小的数据包，我不急于发送这个包，而是等来了更多的包，将这些小包组合成大包之后一并发送，不就提高了网络传输的效率的嘛。这个想法收到了很好的效果，但是我们想一下，如果是分属于两个不同页面的包，被合并在了一起，那客户那边如何区分它们呢？**这就是粘包问题。**

    ​    从粘包问题我们更可以看出为什么tcp被称为流协议，因为它就跟水流一样，是没有边界的，没有消息的边界保护机制，所以**tcp只有流的概念，没有包的概念**。

    ​    我们还需要有两个概念：

    ​    （1）**长连接**： Client方与Server方先建立通讯连接，连接建立后不断开， 然后再进行报文发送和接收。     （2）**短连接**：Client方与Server每进行一次报文收发交易时才进行通讯连接，交易完毕后立即断开连接。此种方式常用于一点对多点 通讯，比如多个Client连接一个Server。

    ​    **实际**，我想象的**关于粘包的场景是不对的**，http连接是短连接，请求之后，收到回答，立马断开连接，不会出现粘包。 拆包现象是有可能存在的。

    ​    **处理拆包**这里提供两种方法：

    ​    （1）通过包头+包长+包体的协议形式，当服务器端获取到指定的包长时才说明获取完整。     （2） 指定包的结束标识，这样当我们获取到指定的标识时，说明包获取完整。

    ​    **处理粘包**我们从上面的分析看到，虽然像http这样的短连接协议不会出现粘包的现象，但是一旦建立了长连接，粘包还是有可能会发生的。处理粘包的方法如下：

    ​    （1）发送方对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法。

    ​    （2）接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。应用层的解决办法简单可行，不仅能解决接收方的粘包问题，还可以解决发送方的粘包问题。解决办法：循环处理，应用程序从接收缓存中读取分组时，读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成，判断每条数据的长度的方法有两种：

    ​        a. 格式化数据：每条数据有固定的格式（开始符，结束符），这种方法简单易行，但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。

    ​        b. 发送长度：发送每条数据时，将数据的长度一并发送，例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。

**答案解析**

​    **扩展资料**

​    **UDP会不会产生粘包问题呢？**

​    TCP为了保证可靠传输并减少额外的开销（每次发包都要验证），采用了基于流的传输，基于流的传输不认为消息是一条一条的，是无保护消息边界的（保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，接收端一次只能接受一条独立的消息）。UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。

​    举个例子：有三个数据包，大小分别为2k、4k、6k，如果采用UDP发送的话，不管接受方的接收缓存有多大，我们必须要进行至少三次以上的发送才能把数据包发送完，但是使用TCP协议发送的话，我们只需要接受方的接收缓存有12k的大小，就可以一次把这3个数据包全部发送完毕。

#### 1.15 介绍一下TCP和UDP的区别。

**参考回答**

​    TCP和UDP有如下区别：

1. 连接：TCP面向连接的传输层协议，即传输数据之前必须先建立好连接；UDP无连接。
2. 服务对象：TCP点对点的两点间服务，即一条TCP连接只能有两个端点；UDP支持一对一，一对多，多对一，多对多的交互通信。
3. 可靠性：TCP可靠交付：无差错，不丢失，不重复，按序到达；UDP尽最大努力交付，不保证可靠交付。
4. 拥塞控制/流量控制：有拥塞控制和流量控制保证数据传输的安全性；UDP没有拥塞控制，网络拥塞不会影响源主机的发送效率。
5. 报文长度：TCP动态报文长度，即TCP报文长度是根据接收方的窗口大小和当前网络拥塞情况决定的；UDP面向报文，不合并，不拆分，保留上面传下来报文的边界。
6. 首部开销：TCP首部开销大，首部20个字节；UDP首部开销小，8字节（源端口，目的端口，数据长度，校验和）。
7. 适用场景（由特性决定）：数据完整性需让位于通信实时性，则应该选用TCP 协议（如文件传输、重要状态的更新等）；反之，则使用 UDP 协议（如视频传输、实时通信等）。

#### 1.16 TCP和UDP对于网络稳定性有什么要求？

**参考回答**

1. TCP优缺点

    ​    **优点：可靠、稳定**

    ​    TCP的可靠体现在TCP在传输数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完之后，还会断开连接用来节约系统资源。

    ​    **缺点：慢，效率低，占用系统资源高，易被攻击**

    ​    在传递数据之前要先建立连接，这会消耗时间，而且在数据传递时，确认机制、重传机制、拥塞机制等都会消耗大量时间，而且要在每台设备上维护所有的传输连接。然而，每个链接都会占用系统的CPU、内存等硬件资源。因为TCP有确认机制、三次握手机制，这些也导致TCP容易被利用，实现DOS、DDOS、CC等攻击。

2. UDP优缺点

    ​    **优点：快，比TCP稍安全**

    ​    UDP没有TCP拥有的各种机制，是一个无状态的传输协议，所以传递数据非常快，没有TCP的这些机制，被攻击利用的机制就少一些，但是也无法避免被攻击。

    ​    **缺点：不可靠，不稳定**

    ​    因为没有TCP的那些机制，UDP在传输数据时，如果网络质量不好，就会很容易丢包，造成数据的缺失。

3. 适用场景（**网络稳定性要求**）

    ​    TCP：当对网络通讯质量有要求时，比如HTTP、HTTPS、FTP等传输文件的协议， POP、SMTP等邮件传输的协议

    ​    UDP：对网络通讯质量要求不高时，要求网络通讯速度要快的场景。

    ​    **所以，TCP对网络稳定性要求高，而UDP相对弱一些。**

#### 1.17 如何让UDP可靠一些？

**参考回答**

1. 为什么需要可靠的UDP

    ​    在弱网（2G、3G、信号不好）环境下，使用 TCP 连接的延迟很高，影响体验。使用 UDP 是很好的解决方案，既然把 UDP 作为弱网里面的 TCP 来使用，就必须保证数据传输能像 TCP 一样可靠

2. 如何实现可靠的UDP

    ​    UDP它不属于连接型协议，因而具有资源消耗小，处理速度快的优点，所以通常音频、视频和普通数据在传送时使用UDP较多，因为它们即使偶尔丢失一两个数据包，也不会对接收结果产生太大影响。传输层无法保证数据的可靠传输，只能通过**应用层**来实现了。实现的方式可以参照tcp可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。关键在于两点，从应用层角度考虑：

    （1）提供超时重传，能避免数据报丢失。

    （2）提供确认序列号，可以对数据报进行确认和排序。

    ​     **本端**：首先在UDP数据报定义一个首部，首部包含确认序列号和时间戳，时间戳是用来计算RTT(数据报传输的往返时间)，计算出合适的RTO(重传的超时时间)。然后以等-停的方式发送数据报，即收到对端的确认之后才发送下一个的数据报。当时间超时，本端重传数据报，同时RTO扩大为原来的两倍，重新开始计时。

    ​    **对端**：接受到一个数据报之后取下该数据报首部的时间戳和确认序列号，并添加本端的确认数据报首部之后发送给对端。根据此序列号对已收到的数据报进行排序并丢弃重复的数据报。

**答案解析**

​    **扩展资料**

1. **已经实现的可靠UDP：**

​    （1）RUDP 可靠数据报传输协议；

​    （2）RTP 实时传输协议

为数据提供了具有实时特征的端对端传送服务；

Eg：组播或单播网络服务下的交互式视频、音频或模拟数据

​    （3）UDT

​         基于UDP的数据传输协议，是一种互联网传输协议；

​    主要目的是支持高速广域网上的海量数据传输，引入了新的拥塞控制和数据可靠性控制机制（互联网上的标准数据传输协议TCP在高带宽长距离的网络上性能很差）；

​     UDT是面向连接的双向的应用层协议，同时支持可靠的数据流传输和部分可靠的数据报服务；

​        应用：高速数据传输，点到点技术(P2P)，防火墙穿透，多媒体数据传输；

#### 1.18 TCP报文首部中序号占多少字节？

**参考回答**

​    序号字段占4个字节（32位）。

**答案解析**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697628029/B238C95751C34672AE79F902AD6A83CF)

​                                                                        TCP首部字段详细图

​    TCP首部包括20字节的固定首部部分及长度可变的其他选项，所以TCP首部长度可变。20个字节又分为5部分，每部分4个字节32位，如图中的5行，每行表示32位。

1. **源端口和目的端口**字段——各占 2 字节（16位）。端口是运输层与应用层的服务接口。运输层的复用和分用功能都要通过端口才能实现。

2. **序号**字段——占 4 字节。TCP 连接中传送的数据流中的每一个字节都编上一个序号。序号字段的值则**指的是本报文段所发送的数据的第一个字节的序号。**比如分组的第一个数据包由文件的14个字节数据组成，那么该数据包所添加的序号就是1，同理第二个数据包由文件的59个字节数据组成，那么该数据包所添加的序号就是5；

3. **确认号**字段——占 4 字节，是期望收到对方的下一个报文段的数据的第一个字节的序号。比如接收端收到由文件14个字节数据+TCP首部组成的数据包后，删除首部提取14个字节数据，返回的确认号为5，即告诉发送端下一次应该发送文件的第5个字节及其之后字节组成的数据包过来。

4. **数据偏移**（即首部长度）——占 4 位，它指出 TCP 报文段的数据起始处距离 TCP 报文段的起始处有多远，也就是TCP首部的长度。“数据偏移”的单位是 32 位字（以 4 字节为计算单位），最大1111表示15x4=60个字节，即表示TCP首部最大长度为60个字节，因此“选项”部分最多40个字节。

5. **保留**字段——占 6 位，保留为今后使用，但目前应置为 0。

6. 这里的六位二进制位，分别表示不同含义：

    （1）**紧急 URG** —— 当 URG = 1 时，表明紧急指针字段有效。它告诉系统此报文段中有紧急数据，应尽快传送(相当于高优先级的数据)。 即URG=1的数据包不用排队直接优先传输。

    （2）**同步 SYN** —— 同步 SYN = 1 表示这是一个连接请求或连接接受报文。即A想与B建立连接，发送过去的第一个数据包（第一次握手）中SYN=1；B返回的数据包（第二次握手）中SYN=1表示同意建立连接。

    （3）**确认 ACK** —— 只有当 ACK = 1 时确认号字段才有效。当 ACK = 0 时，确认号无效。

7. **窗口**字段 —— 占 2 字节，用来让对方设置发送窗口的依据，单位为字节。

8. **检验和** —— 占 2 字节。检验和字段检验的范围包括首部和数据这两部分。在计算检验和时，要在 TCP 报文段的前面加上 12 字节的伪首部。

9. **紧急指针**字段 —— 占 16 位，指出在本报文段中紧急数据共有多少个字节（紧急数据放在本报文段数据的最前面）。

10. **选项**字段 —— 长度可变。TCP 最初只规定了一种选项，即最大报文段长度 MSS (Maximum Segment Size)是 TCP 报文段中的数据字段的最大长度。数据字段加上 TCP 首部才等于整个的 TCP 报文段。MSS 告诉对方 TCP：“我的缓存所能接收的报文段的数据字段的最大长度是 MSS 个字节。”**其他选项**有：窗口扩大选项、时间戳选项、选择确认选项（SACK）。

11. **填充**字段 —— 这是为了使整个首部长度是 4 字节的整数倍。

#### 1.19 TCP中的缓存有什么作用？

**参考回答**

1. TCP缓冲区是什么

    每个 socket 被创建后，都会分配两个缓冲区，输入缓冲区和输出缓冲区。

2. **缓冲区的意义（作用）**

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697641584/50D7F6CDDD9A36ED91D4F78DFABF1DF3)

    ​                                                                TCP套接字的I/O缓冲区示意图    

    ​    **TCP的发送缓冲区是用来缓存应用程序的数据，发送缓冲区的每个字节都有序列号，被应答确认的序列号对应的数据会从发送缓冲区删除掉。**

    ​    write()/send() 并不立即向网络中传输数据，而是先将数据写入缓冲区中，再由TCP协议将数据从缓冲区发送到目标机器。一旦将数据写入到缓冲区，函数就可以成功返回，不管它们有没有到达目标机器，也不管它们何时被发送到网络，这些都是TCP协议负责的事情。     TCP协议独立于 write()/send() 函数，数据有可能刚被写入缓冲区就发送到网络，也可能在缓冲区中不断积压，多次写入的数据被一次性发送到网络，比如nagle算法，这取决于当时的网络情况、当前线程是否空闲等诸多因素，不由程序员控制。     read()/recv() 函数也是如此，也从输入缓冲区中读取数据，而不是直接从网络中读取。

3. I/O缓冲区特性

    （1）I/O缓冲区在每个TCP套接字中单独存在；

    （2）I/O缓冲区在创建套接字时自动生成；

    （3）即使关闭套接字也会继续传送输出缓冲区中遗留的数据；

    （4）关闭套接字将丢失输入缓冲区中的数据。

    输入输出缓冲区的默认大小一般都是 8K，可以通过 getsockopt() 函数获取：

    ```
    //代码实例（缓冲区大小获取） int servSock = socket(PF_INET, SOCK_STREAM, 0); unsigned optVal; int optLen = sizeof(int); getsockopt(servSock, SOL_SOCKET, SO_SNDBUF, (char*)&optVal, &optLen); /*  运行结果：  Buffer length: 8192  */
    ```

#### 1.20 说一说TCP是怎么控制流量的？

**参考回答**

1. 所谓**流量控制**就是让发送发送速率不要过快，让接收方来得及接收。

2. TCP控制流量的方法

    ​    利用**滑动窗口机制**就可以实施流量控制。

    ​    **原理**就是运用TCP报文段中的窗口大小字段来控制，发送方的发送窗口不可以大于接收方发回的窗口大小。考虑一种特殊的情况，就是接收方若没有缓存足够使用，就会发送零窗口大小的报文，此时发送放将发送窗口设置为0，停止发送数据。之后接收方有足够的缓存，发送了非零窗口大小的报文，但是这个报文在中途丢失的，那么发送方的发送窗口就一直为零导致死锁。

    ​     解决这个问题，TCP为每一个连接设置一个持续计时器（persistence timer）。只要TCP的一方收到对方的零窗口通知，就启动该计时器，周期性的发送一个零窗口探测报文段。对方就在确认这个报文的时候给出现在的窗口大小（注意：TCP规定，即使设置为零窗口，也必须接收以下几种报文段：零窗口探测报文段、确认报文段和携带紧急数据的报文段）。

**答案解析**

1. TCP的滑动窗口

    ​    为了提高信道的利用率TCP协议不使用停止等待协议，而是使用连续ARQ协议，意思就是可以连续发出若干个分组然后等待确认，而不是发送一个分组就停止并等待该分组的确认。

    TCP的两端都有发送/接收缓存和发送/接收窗口。TCP的缓存是一个循环队列，其中发送窗口可以用3个指针表示。而发送窗口的大小受TCP数据报中窗口大小的影响，TCP数据报中的窗口大小是接收端通知发送端其还可以接收多少数据，所以发送窗口根据接收的的窗口大小的值动态变化。

    以下的几张图片就帮助理解一下滑动窗口的机制：

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697654563/D3EA8A21E7664298BC83A4FFB4A255E9)

    ​                                        图1 根据B给出的窗口值，A构造出自己的发送窗口

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697692938/DF25FC92A4CFE348C7AA332BF153F3B4)

    ​                                                                图2 A发送了11个字节的数据

    ​     注意上图中的3个指针P1、P2、P3！此时接收窗口中接收的数据可能是失序的，但是也先存储在接收缓存之中。发送确认号的时候依然发送31，表示B期望接收的下一个数据报的标示符是31。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697717871/F7DC635726D680CED6FCA799D5C7BC52)

    ​                                                                图3 A收到新的确认号，发送窗口向前滑动

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697723406/B84CD9BCDA7C6A7D076AC8C88B911813)

    ​                                                            图4 发送窗口内的序号都属于已经发送但未被确认

    ​    如果发送窗口中的数据报都属于已发送但未被确认的话，那么A就不能再继续发送数据，而需要进行等待。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697732819/73B35B509B890B4352DF1DE9791D2BCB)

    ​                                            图5 TCP的发送缓存和发送窗口（a）与接收缓存和接收窗口（b）

2. 传输效率及Nagle算法

    ​    TCP的数据传输分为交互数据流和成块数据流，交互数据流一般是一些交互式应用程序的命令，所以这些数据很小，而考虑到TCP报头和IP报头的总和就有40字节，如果数据量很小的话，那么网络的利用效率就较低。

    数据传输使用Nagle算法，Nagle算法很简单，就是规定一个TCP连接最多只能有一个**未被确认的未完成的小分组**。在该分组的确认到达之前不能发送其他的小分组。

    但是也要考虑另一个问题，叫做糊涂窗口综合症。当接收方的缓存已满的时候，交互应用程序一次只从缓存中读取一个字节（这时候缓存中腾出一个字节），然后向发送方发送确认信息，此时发送方再发送一个字节（收到的窗口大小为1），这样网络的效率很低。

    ​    要解决这个问题，可以让接收方等待一段时间，使得接收缓存已有最够的空间容纳一个最长报文段，或者等到接收缓存已有一半的空间。只要这两种情况出现一种，就发送确认报文，同时发送方可以把数据积累成大的报文段发送。

#### 1.21 HTTP2.0中TCP阻塞了怎么办？

**参考回答**

​    HTTP2.0中TCP阻塞了有如下两种方法可以解决：

​    **（1）并发TCP连接**（浏览器一个域名采用6-8个TCP连接，并发HTTP请求）     **（2）域名分片**（多个域名，可以建立更多的TCP连接，从而提高HTTP请求的并发）

**答案解析**

​    **1. TCP队头阻塞**

​        TCP数据包是有序传输，中间一个数据包丢失，会等待该数据包重传，造成后面的数据包的阻塞。

​    **2. HTTP队头阻塞**

​        http队头阻塞和TCP队头阻塞完全不是一回事。

​        http1.x采用长连接(Connection:keep-alive)，可以在一个TCP请求上，发送多个http请求。

​        有非管道化和管道化，两种方式。

​        **非管道化**，完全串行执行，请求->响应->请求->响应...，后一个请求必须在前一个响应之后发送。

​        **管道化**，请求可以并行发出，但是响应必须串行返回。后一个响应必须在前一个响应之后。原因是，没有序号标明顺序，只能串行接收。

​        **管道化请求的致命弱点**:

​    （1）会造成队头阻塞，前一个响应未及时返回，后面的响应被阻塞     （2）请求必须是幂等请求，不能修改资源。因为，意外中断时候，客户端需要把未收到响应的请求重发，非幂等请求，会造成资源破坏。

​        由于这个原因，目前大部分浏览器和Web服务器，都关闭了管道化，采用非管道化模式。

​        无论是非管道化还是管道化，都会造成队头阻塞(请求阻塞)。

​    **解决http队头阻塞的方法：**

​    **（1）并发TCP连接**（浏览器一个域名采用6-8个TCP连接，并发HTTP请求）     **（2）域名分片**（多个域名，可以建立更多的TCP连接，从而提高HTTP请求的并发）

**2. HTTP2方式**

​    http2使用一个域名单一TCP连接发送请求，请求包被二进制分帧，不同请求可以互相穿插，避免了http层面的请求队头阻塞。     但是不能避免TCP层面的队头阻塞。

#### 1.22 TCP如何保证可靠性？

**参考回答**

​    TCP协议保证数据传输可靠性的方式主要有：**校验和、序列号、确认应答、超时重传、连接管理、流量控制、拥塞控制**。

1. 校验和

    **计算方式：**在数据传输的过程中，将发送的数据段都当做一个16位的整数。将这些整数加起来。并且前面的进位不能丢弃，补在后面，最后取反，得到校验和。 **发送方：**在发送数据之前计算检验和，并进行校验和的填充。 **接收方：**收到数据后，对数据以同样的方式进行计算，求出校验和，与发送方的进行比对。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697750584/615A581F8A18065E509E57958679912F)

    **注意：**如果接收方比对校验和与发送方不一致，那么数据一定传输有误。但是如果接收方比对校验和与发送方一致，**数据不一定传输成功。**

2. 序列号和确认应答

    **序列号：**TCP传输时将每个字节的数据都进行了编号，这就是序列号。 **确认应答：**TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文。这个ACK报文当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697763966/15FA1994E2C2180DFF6C75070B4CA4C5)

    ​    序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。这也是TCP传输可靠性的保证之一。

3. 超时重传

    ​    在进行TCP传输时，由于确认应答与序列号机制，也就是说发送方发送一部分数据后，都会等待接收方发送的ACK报文，并解析ACK报文，判断数据是否传输成功。如果发送方发送完数据后，迟迟没有等到接收方的ACK报文，这该怎么办呢？而没有收到ACK报文的原因可能是什么呢？

    ​    首先，发送方没有接收到响应的ACK报文原因可能有两点：

    ​    （1）数据在传输过程中由于网络原因等直接全体丢包，接收方根本没有接收到。

    ​    （2）接收方接收到了响应的数据，但是发送的ACK报文响应却由于网络原因丢包了。

    ​    TCP在解决这个问题的时候引入了一个新的机制，叫做超时重传机制。**简单理解就是发送方在发送完数据后等待一个时间，时间到达没有接收到ACK报文，那么对刚才发送的数据进行重新发送。**如果是刚才第一个原因，接收方收到二次重发的数据后，便进行ACK应答。如果是第二个原因，接收方发现接收的数据已存在（判断存在的根据就是序列号，所以上面说序列号还有去除重复数据的作用），那么直接丢弃，仍旧发送ACK应答。

4. 连接管理

    ​    连接管理就是三次握手与四次挥手的过程，保证可靠的连接，是保证可靠性的前提。

5. 流量控制

    ​    收端在接收到数据后，对其进行处理。如果发送端的发送速度太快，导致接收端的结束缓冲区很快的填充满了。此时如果发送端仍旧发送数据，那么接下来发送的数据都会丢包，继而导致丢包的一系列连锁反应，超时重传呀什么的。而TCP根据接收端对数据的处理能力，决定发送端的发送速度，这个机制就是流量控制。

    ​    在TCP协议的报头信息当中，有一个16位字段的窗口大小。在介绍这个窗口大小时我们知道，窗口大小的内容实际上是接收端接收数据缓冲区的剩余大小。这个数字越大，证明接收端接收缓冲区的剩余空间越大，网络的吞吐量越大。接收端会在确认应答发送ACK报文时，将自己的即时窗口大小填入，并跟随ACK报文一起发送过去。而发送方根据ACK报文里的窗口大小的值的改变进而改变自己的发送速度。如果接收到窗口大小的值为0，那么发送方将停止发送数据。并定期的向接收端发送窗口探测数据段，让接收端把窗口大小告诉发送端。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697783115/80D43DFCBDB0DA0BDA5BE34F674B666D)

6. 拥塞控制

    ​    TCP传输的过程中，发送端开始发送数据的时候，如果刚开始就发送大量的数据，那么就可能造成一些问题。网络可能在开始的时候就很拥堵，如果给网络中在扔出大量数据，那么这个拥堵就会加剧。拥堵的加剧就会产生大量的丢包，就对大量的超时重传，严重影响传输。

    ​    所以TCP引入了慢启动的机制，在开始发送数据时，先发送少量的数据探路。探清当前的网络状态如何，再决定多大的速度进行传输。这时候就引入一个叫做拥塞窗口的概念。发送刚开始定义拥塞窗口为 1，每次收到ACK应答，拥塞窗口加 1。在发送数据之前，首先将拥塞窗口与接收端反馈的窗口大小比对，取较小的值作为实际发送的窗口。

    ​    拥塞窗口的增长是指数级别的。慢启动的机制只是说明在开始的时候发送的少，发送的慢，但是增长的速度是非常快的。为了控制拥塞窗口的增长，不能使拥塞窗口单纯的加倍，设置一个拥塞窗口的阈值，当拥塞窗口大小超过阈值时，不能再按照指数来增长，而是线性的增长。在慢启动开始的时候，慢启动的阈值等于窗口的最大值，一旦造成网络拥塞，发生超时重传时，慢启动的阈值会为原来的一半（这里的原来指的是发生网络拥塞时拥塞窗口的大小），同时拥塞窗口重置为 1。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697798002/4996A810BDF7664405E609AA8B4E4441)

    ​    拥塞控制是TCP在传输时尽可能快的将数据传输，并且避免拥塞造成的一系列问题。是可靠性的保证，同时也是维护了传输的高效性。

#### 1.23 说一说TCP里的reset状态。

**参考回答**

1. TCP异常终止（reset报文）

    ​    TCP的异常终止是相对于正常释放TCP连接的过程而言的，我们都知道，TCP连接的建立是通过三次握手完成的，而TCP正常释放连接是通过四次挥手来完成，但是有些情况下，TCP在交互的过程中会出现一些意想不到的情况，导致TCP无法按照正常的四次挥手来释放连接，如果此时不通过其他的方式来释放TCP连接的话，这个TCP连接将会一直存在，占用系统的部分资源。在这种情况下，我们就需要有一种能够释放TCP连接的机制，这种机制就是TCP的reset报文。reset报文是指TCP报头的标志字段中的reset位置一的报文。

2. **RST标志位（Reset）**

    ​    RST表示复位，用来异常的关闭连接，在TCP的设计中它是不可或缺的。就像上面说的一样，发送RST包关闭连接时，不必等缓冲区的包都发出去（不像上面的FIN包），直接就丢弃缓存区的包发送RST包。而接收端收到RST包后，也不必发送ACK包来确认。

    ​    TCP处理程序会在自己认为的异常时刻发送RST包。例如，A向B发起连接，但B之上并未监听相应的端口，这时B操作系统上的TCP处理程序会发RST包。

    ​    又比如，AB正常建立连接了，正在通讯时，A向B发送了FIN包要求关连接，B发送ACK后，网断了，A通过若干原因放弃了这个连接（例如进程重启）。网通了后，B又开始发数据包，A收到后表示压力很大，不知道这野连接哪来的，就发了个RST包强制把连接关了，B收到后会出现connect reset by peer错误。

**答案解析**

1. TCP异常终止的常见情形

    （1）客户端尝试与服务器未对外提供服务的端口建立TCP连接，服务器将会直接向客户端发送reset报文。

    （2）客户端和服务器的某一方在交互的过程中发生异常（如程序崩溃等），该方系统将向对端发送TCP reset报文，告之对方释放相关的TCP连接。

    （3）接收端收到TCP报文，但是发现该TCP的报文，并不在其已建立的TCP连接列表内，则其直接向对端发送reset报文。

    （4）在交互的双方中的某一方长期未收到来自对方的确认报文，则其在超出一定的重传次数或时间后，会主动向对端发送reset报文释放该TCP连接。

    （5）有些应用开发者在设计应用系统时，会利用reset报文快速释放已经完成数据交互的TCP连接，以提高业务交互的效率。

#### 1.24 如何利用UDP实现可靠传输？

**参考回答**

1. 实现方法：

    （1）将实现放到应用层，然后类似于TCP，实现确认机制、重传机制和窗口确认机制；

    （2）给数据包进行编号，按顺序接收并存储，接收端收到数据包后发送确认信息给发送端，发送端接收到确认信息后继续发送，若接收端接收的数据不是期望的顺序编号，则要求重发；（主要解决丢包和包无序的问题）

2. 已经实现的可靠UDP：

    （1）RUDP 可靠数据报传输协议；

    （2）RTP 实时传输协议

    ​    为数据提供了具有实时特征的端对端传送服务；例如：组播或单播网络服务下的交互式视频、音频或模拟数据。

    （3）UDT

    ​    基于UDP的数据传输协议，是一种互联网传输协议； 主要目的是支持高速广域网上的海量数据传输，引入了新的拥塞控制和数据可靠性控制机制（互联网上的标准数据传输协议TCP在高带宽长距离的网络上性能很差）；

UDT是面向连接的双向的应用层协议，同时支持可靠的数据流传输和部分可靠的数据报服务；

应用：高速数据传输，点到点技术(P2P)，防火墙穿透，多媒体数据传输；

**答案解析**

​    无

#### 1.25 报文乱序怎么办？

**参考回答**

​    数据包会因为IP层所规划的路由链路的不同导致数据包的接收顺序与发送顺序会有所不同。另外因为TCP是一种全双工的协议，乱序可能发生在正向链路，也可能发生在反向链路，这两种不同的情况给TCP带来的影响也会略有差异。

1. **正向链路乱序**

    ​    **此时TCP会**无法判断**是数据包丢失还是乱序**，因为丢包和乱序都会导致接收端收到次序混乱的数据包，造成接收端的数据空洞。**TCP会将这种情况暂定为数据包的乱序**，**因为乱序是时间问题（可能是数据包的迟到），而丢包则意味着重传**。当TCP意识到包出现乱序的情况时，会立即ACK，该ACK的TSER部分包含的TSEV值会记录当前接收端收到有序报文段的时刻。这会使得数据包的RTT样本值增大，进一步导致RTO时间延长。这对TCP来说无疑是有益的，因为TCP有充分的时间判断数据包到底是失序还是丢了来防止不必要的数据重传。当然严重的乱序则会让发送端以为是丢包**一旦重复的ACK超过TCP的阈值，便会触发超时重传机制**，以及时解决这种问题。

2. **反向链路（ACK）乱序**

    ​    顾名思义，如果发生这种情况，就会导致发送端窗口快速前移，这会导致发送端出现不必要的流量突发，影响网络带宽。

**答案解析**

​    无

#### 1.26 说一说你对IP分类的了解。

**参考回答**

​    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697869711/3A1B72345635FE06A1B8B82672EEA03C)

​                                                                            五类互联网地址

​    IP地址根据网络号和主机号来分，分为A、B、C三类及特殊地址D、E。 全0和全1的都保留不用。

1. A类：(1.0.0.0-126.0.0.0)（默认子网掩码：255.0.0.0或 0xFF000000）第一个字节为网络号，后三个字节为主机号。该类IP地址的最前面为“0”，所以地址的网络号取值于1~126之间。一般用于大型网络。
2. B类：(128.0.0.0-191.255.0.0)（默认子网掩码：255.255.0.0或0xFFFF0000）前两个字节为网络号，后两个字节为主机号。该类IP地址的最前面为“10”，所以地址的网络号取值于128~191之间。一般用于中等规模网络。
3. C类：(192.0.0.0-223.255.255.0)（子网掩码：255.255.255.0或 0xFFFFFF00）前三个字节为网络号，最后一个字节为主机号。该类IP地址的最前面为“110”，所以地址的网络号取值于192~223之间。一般用于小型网络。
4. D类：是多播地址。该类IP地址的最前面为“1110”，所以地址的网络号取值于224~239之间。一般用于多路广播用户 。
5. E类：是保留地址。该类IP地址的最前面为“1111”，所以地址的网络号取值于240~255之间。

#### 1.27 IP为什么要分类？

**参考回答**

​    根据IP地址访问终端是通过路由器，路由设备当中有一张路由表，该路由表记录了所有IP地址的位置，这样就可以进行包的转发了，如果我们不区分网络地址，那么这张路由表当中就要保存有所有IP地址的方向，这张路由表就会很大，就像下面说的那样：如果不分网络位和主机位，路由器的路由表就是都是32位的地址，那所有的路由器维护的路由表会很大，转发速度会变慢（因为查询变慢）。而且所有的路由器都要有全Internet的地址，所有人的路由器都要有足够的性能来存下全网地址。估计建造这样的Internet成本是现在的几万倍，甚至更高。

​    **有了网络地址，就可以限定拥有相同网络地址的终端都在同一个范围内，那么路由表只需要维护这个网络地址的方向，就可以找到相应的终端了。**

#### 1.28 IPV4和IPV6有什么区别？

**参考回答**

​    IPv4和IPv6是是目前使用的两种Internet协议版本，IPv4和IPv6协议之间存在各种差异，包括它们的功能，但关键的一点是它生成的地址（地址空间）的数量的区别。

1. 协议地址的区别

    （1）地址长度

    ​    IPv4协议具有32位（4字节）地址长度；IPv6协议具有128位（16字节）地址长度。

    （2）地址的表示方法

    ​    IPv4地址是以小数表示的二进制数。 IPv6地址是以十六进制表示的二进制数。

    （3）地址配置

    ​    IPv4协议的地址可以通过手动或DHCP配置的。

    ​    IPv6协议需要使用Internet控制消息协议版本6（ICMPv6）或DHCPv6的无状态地址自动配置（SLAAC）。

2. 数据包的区别

    （1）包的大小

    ​    IPv4协议的数据包需要576个字节，碎片可选 ；IPv6协议的数据包需要1280个字节，不会碎片。

    （2）包头

    ​    IPv4协议的包头的长度为20个字节，不识别用于QoS处理的数据包流，包含checksum，包含最多40个字节的选项字段。

    ​    IPv6协议的包头的长度为40个字节，包含指定QoS处理的数据包流的Flow Label字段，不包含checksum；IPv6协议没有字段，但IPv6扩展标头可用。

    （3）数据包碎片

    IPv4协议的数据包碎片会由转发路由器和发送主机完成。IPv6协议的数据包碎片仅由发送主机完成。

3. DNS记录

    IPv4协议的地址（A）记录，映射主机名；指针（PTR）记录，IN-ADDR.ARPA DNS域。

    IPv6协议的地址（AAAA）记录，映射主机名；指针（PTR）记录，IP6.ARPA DNS域

4. IPSec支持

    IPv4协议的IPSec支持只是可选的；IPv6协议有内置的IPSec支持。

5. 地址解析协议

    IPv4协议：地址解析协议（ARP）可用于将IPv4地址映射到MAC地址。

    IPv6协议：地址解析协议（ARP）被邻居发现协议（NDP）的功能所取代。

6. 身份验证和加密

    Pv6提供身份验证和加密；但IPv4不提供。

**答案解析**

​    无

#### 1.29 说一下http和https的区别。

**参考回答**

​    https和https主要存在以下的区别：

1. HTTPS 协议需要到 CA （Certificate Authority，证书颁发机构）申请证书，一般免费证书较少，因而需要一定费用。(以前的网易官网是http，而网易邮箱是 https 。)
2. HTTP 是超文本传输协议，信息是明文传输，HTTPS 则是具有安全性的 SSL 加密传输协议。
3. HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
4. HTTP 的连接很简单，是无状态的。HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 协议安全。(无状态的意思是其数据包的发送、传输和接收都是相互独立的。无连接的意思是指通信双方都不长久的维持对方的任何信息。)

**答案解析**

1. **超文本传输协议**（HTTP，HyperText Transfer Protocol）是互联网上应用最为广泛的一种网络协议。设计 HTTP 最初的目的是为了提供一种发布和接收 HTML 页面的方法。它可以使浏览器更加高效。HTTP 协议是以明文方式发送信息的，如果黑客截取了 Web 浏览器和服务器之间的传输报文，就可以直接获得其中的信息。

2. **HTTP原理**

    （1）客户端的浏览器首先要通过网络与服务器建立连接，该连接是通过 TCP 来完成的，一般 TCP 连接的端口号是80。 建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是 MIME 信息包括请求修饰符、客户机信息和许可内容。

    （2）服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是 MIME 信息包括服务器信息、实体信息和可能的内容。

3. **HTTPS**（Hyper Text Transfer Protocol over SecureSocket Layer）是以安全为目标的 HTTP 通道，是 HTTP 的安全版。HTTPS 的安全基础是 SSL。SSL 协议位于 TCP/IP 协议与各种应用层协议之间，为数据通讯提供安全支持。SSL 协议可分为两层：SSL 记录协议（SSL Record Protocol），它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。SSL 握手协议（SSL Handshake Protocol），它建立在 SSL 记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。

4. **HTTPS的工作原理**

    ​    我们都知道HTTPS能够加密信息，以免敏感信息被第三方获取，所以很多银行网站或电子邮箱等等安全级别较高的服务都会采用HTTPS协议。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697886891/B7268EF66524898CEF0E068EA5F1BA26)

    客户端在使用HTTPS方式与Web服务器通信时有以下几个步骤，如图上图所示：

    （1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。

    （2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。

    （3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。

    （4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。

    （5）Web服务器利用自己的私钥解密出会话密钥。

    （6）Web服务器利用会话密钥加密与客户端之间的通信。

5. **HTTPS的优点**

    尽管HTTPS并非绝对安全，掌握根证书的机构、掌握加密算法的组织同样可以进行中间人形式的攻击，但HTTPS仍是现行架构下最安全的解决方案，主要有以下几个好处：

    （1）使用HTTPS协议可认证用户和服务器，确保数据发送到正确的客户机和服务器；

    （2）HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全，可防止数据在传输过程中不被窃取、改变，确保数据的完整性。

    （3）HTTPS是现行架构下最安全的解决方案，虽然不是绝对安全，但它大幅增加了中间人攻击的成本。

    （4）谷歌曾在2014年8月份调整搜索引擎算法，并称“比起同等HTTP网站，采用HTTPS加密的网站在搜索结果中的排名将会更高”。

6. **HTTPS的缺点**

    ​    虽然说HTTPS有很大的优势，但其相对来说，还是存在不足之处的：

    ​    （1）HTTPS协议握手阶段比较费时，会使页面的加载时间延长近50%，增加10%到20%的耗电；

    （2）HTTPS连接缓存不如HTTP高效，会增加数据开销和功耗，甚至已有的安全措施也会因此而受到影响；

    （3）SSL证书需要钱，功能越强大的证书费用越高，个人网站、小网站没有必要一般不会用。

    （4）SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗。

    （5）HTTPS协议的加密范围也比较有限，在黑客攻击、拒绝服务攻击、服务器劫持等方面几乎起不到什么作用。最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。

#### 1.30 https为什么采用混合加密机制？

**参考回答**

​    一方面，第一阶段的非对称加密，保证了对称密钥的安全性；另一方面，第二阶段的对称加密，可以提高加密/解密处理的速度，提高数据传输的效率。

**答案解析**

1. **为什么需要加密？**

    ​    因为http的内容是明文传输的，明文数据会经过中间代理服务器、路由器、wifi热点、通信服务运营商等多个物理节点，如果信息在传输过程中被劫持，传输的内容就完全暴露了，他还可以篡改传输的信息且不被双方察觉，这就是**中间人攻击**。所以我们才需要对信息进行加密。最简单容易理解的就是**对称加密**。

2. **什么是对称加密？**

    ​    就是有一个密钥，它可以对一段内容加密，加密后只能用它才能解密看到原本的内容，和我们日常生活中用的钥匙作用差不多。

3. **用对称加密可行吗？**

    ​    **如果通信双方都各自持有同一个密钥，且没有别人知道，这两方的通信安全当然是可以被保证的（除非密钥被破解）。**然而最大的问题就是**这个密钥怎么让传输的双方知晓，同时不被别人知道**。如果由服务器生成一个密钥并传输给浏览器，那这个传输过程中密钥被别人劫持弄到手了怎么办？之后他就能用密钥解开双方传输的任何内容了，所以这么做当然不行。     换种思路？试想一下，如果浏览器内部就预存了网站A的密钥，且可以确保除了浏览器和网站A，不会有任何外人知道该密钥，那理论上用对称加密是可以的，这样浏览器只要预存好世界上所有HTTPS网站的密钥就行啦！这么做显然不现实。     所以我们就需要神奇的**非对称加密**。

4. **什么是非对称加密？**

    ​    有两把密钥，通常一把叫做公钥、一把叫做私钥，用公钥加密的内容必须用私钥才能解开，同样，私钥加密的内容只有公钥能解开。

5. **用非对称加密可行吗？** 鉴于非对称加密的机制，我们可能会有这种思路：服务器先把公钥直接明文传输给浏览器，之后浏览器向服务器传数据前都先用这个公钥加密好再传，这条数据的安全似乎可以保障了！**因为只有服务器有相应的私钥能解开这条数据**。     然而**由服务器到浏览器的这条路怎么保障安全？**如果服务器用它的的私钥加密数据传给浏览器，那么浏览器用公钥可以解密它，而这个公钥是一开始通过明文传输给浏览器的，这个公钥被谁劫持到的话，他也能用该公钥解密服务器传来的信息了。所以**目前似乎只能保证由浏览器向服务器传输数据时的安全性**（其实仍有漏洞，下文会说）。

6. **混合加密**

    ​    非对称加密耗时，非对称加密+对称加密结合可以吗？而且得尽量减少非对称加密的次数。当然是可以的，而且非对称加密、解密各只需用一次即可。以下就是加密过程：

    （1）某网站拥有用于非对称加密的公钥A、私钥A’。

    （2）浏览器像网站服务器请求，服务器把公钥A明文给传输浏览器。

    （3）浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。

    （4）服务器拿到后用私钥A’解密得到密钥X。

    （5）这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都用密钥X加密解密。

    完美！HTTPS基本就是采用了这种方案。

#### 1.31 https支持什么加密算法？

**参考回答**

​    常见的**对称加密算法**有：DES、3DES、Blowfish、IDEA、RC4、RC5、RC6和AES ；

​    常见的**非对称加密算法**有：RSA、ECC（移动设备用）、Diffie-Hellman、El Gamal、DSA（数字签名用）；

​    常见的**Hash算法**有：MD2、MD4、MD5、HAVAL、SHA；

**答案解析**

1. **对称加密技术**

    ​    对称加密采用了对称密码编码技术，它的**特点**是文件加密和解密使用相同的密钥加密。也就是密钥也可以用作解密密钥，这种方法在密码学中叫做**对称加密算法**，对称加密算法使用起来简单快捷，密钥较短，且破译困难，除了数据加密标准（DES），另一个对称密钥加密系统是国际数据加密算法（IDEA），它比DES的加密性好，而且对计算机功能要求也没有那么高。对称加密算法在电子商务交易过程中存在几个问题：

（1）要求提供一条安全的渠道使通讯双方在首次通讯时协商一个共同的密钥。直接的面对面协商可能是不现实而且难于实施的，所以双方可能需要借助于邮件和电话等其它相对不够安全的手段来进行协商；

（2）密钥的数目难于管理。因为对于每一个合作者都需要使用不同的密钥，很难适应开放社会中大量的信息交流；

（3）对称加密算法一般不能提供信息完整性的鉴别。它无法验证发送者和接受者的身份；

（4）对称密钥的管理和分发工作是一件具有潜在危险的和烦琐的过程。对称加密是基于共同保守秘密来实现的，采用对称加密技术的贸易双方必须保证采用的是相同的密钥，保证彼此密钥的交换是安全可靠的，同时还要设定防止密钥泄密和更改密钥的程序。

​     假设两个用户需要使用对称加密方法加密然后交换数据，则用户最少需要2个密钥并交换使用，如果企业内用户有n个，则整个企业共需要n×(n-1) 个密钥，密钥的生成和分发将成为企业信息部门的恶梦。

​    常见的对称加密算法有DES、3DES、Blowfish、IDEA、RC4、RC5、RC6和AES

1. **非对称加密技术**

    ​    与对称加密算法不同，非对称加密算法需要两个密钥：**公开密钥（publickey）和私有密钥（privatekey）**。

    ​    公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密。因为加密和解密使用的是两个不同的密钥，所以这种算法叫作非对称加密算法。

    ​    非对称加密算法实现机密信息交换的基本过程是：甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开；得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方；甲方再用自己保存的另一把专用密钥对加密后的信息进行解密。甲方只能用其专用密钥解密由其公用密钥加密后的任何信息。

    ​     非对称加密的**典型应用是数字签名**。

    常见的非对称加密算法有：RSA、ECC（移动设备用）、Diffie-Hellman、El Gamal、DSA（数字签名用）

Hash算法（摘要算法）

1. **Hash算法**

    ​    Hash算法特别的地方在于它是一种单向算法，用户可以通过hash算法对目标信息生成一段特定长度的唯一hash值，却不能通过这个hash值重新获得目标信息。因此Hash算法常用在不可还原的密码存储、信息完整性校验等。

    常见的Hash算法有MD2、MD4、MD5、HAVAL、SHA

#### 1.32 说一说HTTPS的秘钥交换过程。

**参考回答**

​    HTTPS的密钥交换过程如下：

1. 客户端要访问一个网站，向支持https的服务器发起请求。

2. 客户端向服务器发送自己支持的秘钥交换算法列表。

3. 服务器选取一种秘钥交换算法加上CA证书返回给客户端。

4. 客户端验证服务器是否合法，并生成一个随机数然后用协商好的加密算法加密生成随机秘钥，并用刚才从CA证书中拿到的公钥对其加密后发送给服务器。

5. 服务器收到后用自己的私钥解密（中间人没有服务器的私钥，所以没有办法看到传输的数据，另外确认秘钥交换算法是在第一步，中间人是不知道秘钥交换算法（中间人是无法在第一步做手脚的，那等同于它自己就是一个真实客户端发起了一个新的请求，唯一一种情况攻击人有一个合法CA下发的证书，且客户端（一般为安卓设备）没有对CA下发的证书中的内容网站地址和当前请求地址做对比校验），就算攻击者有公钥，因为不知道协商协议，所以做不出来随机秘钥，顶多就是在传输过程中将报文拦截下来，乱改，但是给到服务器后，私钥是解不开乱改之后的密文的）。

6. 服务器私钥解密之后，拿到对称秘钥，并且用它再加密一个信息，返回给浏览器。

    **注意：**最关键的一步就是在客户端采用 RSA 或 Diffie-Hellman 等加密算法生成 Pre-master，这个随机秘钥是用来计算最终的对称秘钥的，用公钥加密之后攻击人是不知道这个这个随机秘钥的，只有服务器才能解的开。

#### 1.33 说一说HTTPS的证书认证过程。

**参考回答**

​    HTTPS的证书认证过程如下：

1. 浏览器将自己支持的一套加密规则发送给网站。

2. 网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。

3. 浏览器获得网站证书之后浏览器要做以下工作：

    （1） 验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。 （2）如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密。 （3）使用约定好的HASH算法计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。

4. 网站接收浏览器发来的数据之后要做以下的操作：

    （1） 使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。 （2） 使用密码加密一段握手消息，发送给浏览器。

5. 浏览器解密并计算握手消息的HASH，如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码并利用对称加密算法进行加密。

#### 1.34 HTTP请求头中包含什么内容？

**参考回答**

​    HTTP请求头中包含如下三个内容：

​          **User-Agent**：产生请求的浏览器类型。

​     **Accept**：客户端可识别的内容类型列表。

​     **Host**：主机地址。

**答案解析**

1. 请求报文(请求行/请求头/请求数据/空行)

    （1） 请求行

    求方法字段、URL字段和HTTP协议版本

    例如：GET /index.html HTTP/1.1

    get方法将数据拼接在url后面，传递参数受限

    请求方法：

    GET、POST、HEAD、PUT、DELETE、OPTIONS、TRACE、CONNECT

    （2） 请求头(key value形式)

    User-Agent：产生请求的浏览器类型。

    Accept：客户端可识别的内容类型列表。

    Host：主机地址

    （3） 请求数据

    post方法中，会把数据以key value形式发送请求

    （4） 空行

    发送回车符和换行符，通知服务器以下不再有请求头

2. 响应报文(状态行、消息报头、响应正文)

    状态行

​    消息报头

​    响应正文

#### 1.35 HTTP是基于TCP还是UDP？

**参考回答**

​    **HTTP是基于TCP的。**

​    HTTP协议是建立在请求/响应模型上的。首先由客户建立一条与服务器的TCP链接，并发送一个请求到服务器，请求中包含请求方法、URI、协议版本以及 相关的MIME样式的消息。服务器响应一个状态行，包含消息的协议版本、一个成功和失败码以及相关的MIME式样的消息。     HTTP/1.0为每一次HTTP的请求/响应建立一条新的TCP链接，因此一个包含HTML内容和图片的页面将需要建立多次的短期的TCP链接。一次TCP链接的建立将需要3次握手。     另 外，为了获得适当的传输速度，则需要TCP花费额外的回路链接时间（RTT）。每一次链接的建立需要这种经常性的开销，而其并不带有实际有用的数据，只是 保证链接的可靠性，因此HTTP/1.1提出了可持续链接的实现方法。HTTP/1.1将只建立一次TCP的链接而重复地使用它传输一系列的请求/响应消 息，因此减少了链接建立的次数和经常性的链接开销。

**答案解析**

​    无

#### 1.36 HTTP1.1和HTTP2.0有什么区别？

**参考回答**

1. HTTP2.0使用了**多路复用**的技术，做到同一个连接并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级。HTTP1.1也可以多建立几个TCP连接，来支持处理更多并发的请求，但是创建TCP连接本身也是有开销的。

2. 在HTTP1.1中，HTTP请求和响应都是由状态行、请求/响应头部、消息主体三部分组成。一般而言，消息主体都会经过gzip压缩，或者本身传输的就是压缩过后的二进制文件，但状态行和头部却没有经过任何压缩，直接以纯文本传输。随着Web功能越来越复杂，每个页面产生的请求数也越来越多，导致消耗在头部的流量越来越多，尤其是每次都要传输UserAgent、Cookie这类不会频繁变动的内容，完全是一种浪费。 **HTTP1.1不支持header数据的压缩，HTTP2.0使用HPACK算法对header的数据进行压缩，这样数据体积小了，在网络上传输就会更快。**

3. 服务端推送是一种在客户端请求之前发送数据的机制。网页使用了许多资源：HTML、样式表、脚本、图片等等。在HTTP1.1中这些资源每一个都必须明确地请求。这是一个很慢的过程。浏览器从获取HTML开始，然后在它解析和评估页面的时候，增量地获取更多的资源。因为服务器必须等待浏览器做每一个请求，网络经常是空闲的和未充分使用的。

    **为了改善延迟，HTTP2.0引入了server push**，它允许服务端推送资源给浏览器，在浏览器明确地请求之前，免得客户端再次创建连接发送请求到服务器端获取。这样客户端可以直接从本地加载这些资源，不用再通过网络。

**答案解析**

​    无

#### 1.37 HTTP2.0和HTTP3.0有什么区别？

**参考回答**

​    HTTP2.0和HTTP3.0的区别在于前者使用tcp协议而后者使用udp协议。

**答案解析**

​    **http发展历程：从http0.9 到 http3.0**

1. HTTP0.9

    ​    最简单的只有请求行 GET index.html

2. HTTP1.0

    （1）增加请求头、响应头，让请求和相应都更清晰

    （2）增加状态码，让响应更清晰

    （3）增加缓存功能，已请求过的内容再次请求时就可直接使用缓存

```
GET index.html HTTP/1.0  accept: application/html accept-charset: utf-8  accept-encoding: gzip  accept-language: zh-CN
HTTP/1.0 200 OK  <!DOCTYPE html>  <html>  <head></head>  <body>hello world!</body>  </html>
```

​        ￼a. accept 解决文件格式问题，是json还是html，浏览器根据不同文件格式来解析文件；

​        b. accept-charset 解决文件编码问题，告知浏览器如何将字符流解析成字节流；

​        c. accept-encoding 解决大文件压缩问题，浏览器采用指定的解压方式来解压；

​        d. accept-language 解决国际化问题，不同国家请求不同语言的文件。

1. HTTP1.1

    （1）持久连接，多个http请求使用同一个tcp连接，减少了tcp建立连接时的开销

    （2）客户端和服务器之间可以建立多个tcp连接以解决队头阻塞的问题

    （3）响应体可以分块传输，无需一次传输全部内容

    （4）响应头增加content-length字段满足动态内容无法一次计算出长度和无法一次传输完成的问题

    （5）增加了安全机制和cookie机制

2. HTTP2.0 多路复用，客户端和服务器之间只建立一条tcp，每个http请求被切分成多帧，多个http的帧混合在一起在一个tcp连接上传送

3. HTTP3.0 不再使用tcp协议，因为tcp依然是顺序发送，顺序接收的，依然有队头堵塞问题，干掉tcp才能解决队头堵塞问题。google的QUIC就使用了udp协议。

#### 1.38 谈谈HTTP的缓存机制，服务器如何判断当前缓存过期？

**参考答案**：

- **http 缓存策略**

    浏览器每次发起请求时，先在本地缓存中查找结果以及缓存标识，根据缓存标识来判断是否使用本地缓存。如果缓存有效，则使用本地缓存；否则，则向服务器发起请求并携带缓存标识。根据是否需向服务器发起HTTP请求，将缓存过程划分为两个部分： 强制缓存和协商缓存，强缓优先于协商缓存。

    - 强缓存，服务器通知浏览器一个缓存时间，在缓存时间内，下次请求，直接用缓存，不在时间内，执行比较缓存策略。
    - 协商缓存，让客户端与服务器之间能实现缓存文件是否更新的验证、提升缓存的复用率，将缓存信息中的Etag和Last-Modified 通过请求发送给服务器，由服务器校验，返回304状态码时，浏览器直接使用缓存。

    HTTP缓存都是从第二次请求开始的：

    - 第一次请求资源时，服务器返回资源，并在response header中回传资源的缓存策略；

    - 第二次请求时，浏览器判断这些请求参数，击中强缓存就直接200，否则就把请求参数加到request header头中传给服务器，看是否击中协商缓存，击中则返回304，否则服务器会返回新的资源。这是缓存运作的一个整体流程图：

        

- **强缓存**

    - 强缓存命中则直接读取浏览器本地的资源，在network中显示的是from memory或者from disk
    - 控制强制缓存的字段有：Cache-Control（http1.1）和Expires（http1.0）
    - Cache-control是一个相对时间，用以表达自上次请求正确的资源之后的多少秒的时间段内缓存有效。
    - Expires是一个绝对时间。用以表达在这个时间点之前发起请求可以直接从浏览器中读取数据，而无需发起请求
    - Cache-Control的优先级比Expires的优先级高。前者的出现是为了解决Expires在浏览器时间被手动更改导致缓存判断错误的问题。 如果同时存在则使用Cache-control。

- **强缓存-expires**

    - 该字段是服务器响应消息头字段，告诉浏览器在过期时间之前可以直接从浏览器缓存中存取数据。
    - Expires 是 HTTP 1.0 的字段，表示缓存到期时间，是一个绝对的时间 (当前时间+缓存时间)。在响应消息头中，设置这个字段之后，就可以告诉浏览器，在未过期之前不需要再次请求。
    - 由于是绝对时间，用户可能会将客户端本地的时间进行修改，而导致浏览器判断缓存失效，重新请求该资源。此外，即使不考虑修改，时差或者误差等因素也可能造成客户端与服务端的时间不一致，致使缓存失效。
    - 优势特点
        - HTTP 1.0 产物，可以在HTTP 1.0和1.1中使用，简单易用。
        - 以时刻标识失效时间。
    - 劣势问题
        - 时间是由服务器发送的(UTC)，如果服务器时间和客户端时间存在不一致，可能会出现问题。
        - 存在版本问题，到期之前的修改客户端是不可知的。

- **强缓存-cache-control**

    - 已知Expires的缺点之后，在HTTP/1.1中，增加了一个字段Cache-control，该字段表示资源缓存的最大有效时间，在该时间内，客户端不需要向服务器发送请求。
    - 这两者的区别就是前者是绝对时间，而后者是相对时间。下面列举一些 Cache-control 字段常用的值：(完整的列表可以查看MDN)
        - max-age：即最大有效时间。
        - must-revalidate：如果超过了 max-age 的时间，浏览器必须向服务器发送请求，验证资源是否还有效。
        - no-cache：不使用强缓存，需要与服务器验证缓存是否新鲜。
        - no-store: 真正意义上的“不要缓存”。所有内容都不走缓存，包括强制和对比。
        - public：所有的内容都可以被缓存 (包括客户端和代理服务器， 如 CDN)
        - private：所有的内容只有客户端才可以缓存，代理服务器不能缓存。默认值。
    - **Cache-control 的优先级高于 Expires**，为了兼容 HTTP/1.0 和 HTTP/1.1，实际项目中两个字段都可以设置。
    - 该字段可以在请求头或者响应头设置，可组合使用多种指令：
        - 可缓存性
            - public：浏览器和缓存服务器都可以缓存页面信息
            - private：default，代理服务器不可缓存，只能被单个用户缓存
            - no-cache：浏览器器和服务器都不应该缓存页面信息，但仍可缓存，只是在缓存前需要向服务器确认资源是否被更改。可配合private， 过期时间设置为过去时间。
            - only-if-cache：客户端只接受已缓存的响应
        - 到期
            - max-age=：缓存存储的最大周期，超过这个周期被认为过期。
            - s-maxage=：设置共享缓存，比如can。会覆盖max-age和expires。
            - max-stale[=]：客户端愿意接收一个已经过期的资源
            - min-fresh=：客户端希望在指定的时间内获取最新的响应
            - stale-while-revalidate=：客户端愿意接收陈旧的响应，并且在后台一部检查新的响应。时间代表客户端愿意接收陈旧响应 的时间长度。
            - stale-if-error=：如新的检测失败，客户端则愿意接收陈旧的响应，时间代表等待时间。
        - 重新验证和重新加载
            - must-revalidate：如页面过期，则去服务器进行获取。
            - proxy-revalidate：用于共享缓存。
            - immutable：响应正文不随时间改变。
        - 其他
            - no-store：绝对禁止缓存
            - no-transform：不得对资源进行转换和转变。例如，不得对图像格式进行转换。
    - 优势特点
        - HTTP 1.1 产物，以时间间隔标识失效时间，解决了Expires服务器和客户端相对时间的问题。
        - 比Expires多了很多选项设置。
    - 劣势问题
        - 存在版本问题，到期之前的修改客户端是不可知的。

- **协商缓存**

    - 协商缓存的状态码由服务器决策返回200或者304
    - 当浏览器的强缓存失效的时候或者请求头中设置了不走强缓存，并且在请求头中设置了If-Modified-Since 或者 If-None-Match 的时候，会将这两个属性值到服务端去验证是否命中协商缓存，如果命中了协商缓存，会返回 304 状态，加载浏览器缓存，并且响应头会设置 Last-Modified 或者 ETag 属性。
    - 对比缓存在请求数上和没有缓存是一致的，但如果是 304 的话，返回的仅仅是一个状态码而已，并没有实际的文件内容，因此 在响应体体积上的节省是它的优化点。
    - 协商缓存有 2 组字段(不是两个)，控制协商缓存的字段有：Last-Modified/If-Modified-since（http1.0）和Etag/If-None-match（http1.1）
    - Last-Modified/If-Modified-since表示的是服务器的资源最后一次修改的时间；Etag/If-None-match表示的是服务器资源的唯一标 识，只要资源变化，Etag就会重新生成。
    - Etag/If-None-match的优先级比Last-Modified/If-Modified-since高。

- **协商缓存-协商缓存-Last-Modified/If-Modified-since**

    - 服务器通过 Last-Modified 字段告知客户端，资源最后一次被修改的时间，例如 Last-Modified: Mon, 10 Nov 2018 09:10:11 GMT
    - 浏览器将这个值和内容一起记录在缓存数据库中。
    - 下一次请求相同资源时时，浏览器从自己的缓存中找出“不确定是否过期的”缓存。因此在请求头中将上次的 Last-Modified 的值写入到请求头的 If-Modified-Since 字段
    - 服务器会将 If-Modified-Since 的值与 Last-Modified 字段进行对比。如果相等，则表示未修改，响应 304；反之，则表示修改了，响应 200 状态码，并返回数据。
    - 优势特点
        - 不存在版本问题，每次请求都会去服务器进行校验。服务器对比最后修改时间如果相同则返回304，不同返回200以及资源内容。
    - 劣势问题
        - 只要资源修改，无论内容是否发生实质性的变化，都会将该资源返回客户端。例如周期性重写，这种情况下该资源包含的数据实际上一样的。
        - 以时刻作为标识，无法识别一秒内进行多次修改的情况。 如果资源更新的速度是秒以下单位，那么该缓存是不能被使用的，因为它的时间单位最低是秒。
        - 某些服务器不能精确的得到文件的最后修改时间。
        - 如果文件是通过服务器动态生成的，那么该方法的更新时间永远是生成的时间，尽管文件可能没有变化，所以起不到缓存的作用。

- **协商缓存-Etag/If-None-match**

    - 为了解决上述问题，出现了一组新的字段 Etag 和 If-None-Match
    - Etag 存储的是文件的特殊标识(一般都是 hash 生成的)，服务器存储着文件的 Etag 字段。之后的流程和 Last-Modified 一致，只是 Last-Modified 字段和它所表示的更新时间改变成了 Etag 字段和它所表示的文件 hash，把 If-Modified-Since 变成了 If-None-Match。服务器同样进行比较，命中返回 304, 不命中返回新资源和 200。
    - 浏览器在发起请求时，服务器返回在Response header中返回请求资源的唯一标识。在下一次请求时，会将上一次返回的Etag值赋值给If-No-Matched并添加在Request Header中。服务器将浏览器传来的if-no-matched跟自己的本地的资源的ETag做对比，如果匹配，则返回304通知浏览器读取本地缓存，否则返回200和更新后的资源。
    - **Etag 的优先级高于 Last-Modified**。
    - 优势特点
        - 可以更加精确的判断资源是否被修改，可以识别一秒内多次修改的情况。
        - 不存在版本问题，每次请求都回去服务器进行校验。
    - 劣势问题
        - 计算ETag值需要性能损耗。
        - 分布式服务器存储的情况下，计算ETag的算法如果不一样，会导致浏览器从一台服务器上获得页面内容后到另外一台服务器上进行验证时现ETag不匹配的情况。

#### 1.39 介绍一下HTTP协议中的长连接和短连接。

**参考回答**

​    HTTP协议的底层使用TCP协议，所以HTTP协议的长连接和短连接在本质上是TCP层的长连接和短连接。由于TCP建立连接、维护连接、释放连接都是要消耗一定的资源，浪费一定的时间。所对于服务器来说，频繁的请求释放连接会浪费大量的时间，长时间维护太多的连接的话又需要消耗资源。所以长连接和短连接并不存在优劣之分，只是适用的场合不同而已。长连接和短连接分别有如下优点和缺点：

**长连接优点：**可以节省较多的TCP连接和释放的操作，节约时间，对于频繁请求资源的用户来说，适合长连接。

**长连接缺点：**由于有保活功能，当遇到大量的恶意连接时，服务器的压力会越来越大。这时服务器需要采取一些策略，关闭一些长时间没有进行读写事件的的连接。

**短连接优点：**短连接对服务器来说管理比较简单，只要存在的连接都是有效连接，不需要额外的控制手段，而且不会长时间占用资源 。

**短连接缺点：**如果客户端请求频繁的话，会在TCP的建立和释放上浪费大量的时间。

​    **注意：**从**HTTP/1.1版本起**，默认使用长连接用以保持连接特性。使用长连接的HTTP协议，会在响应消息报文段加入: Connection: keep-alive。TCP中也有keep alive，但是TCP中的keep alive只是探测TCP连接是否活着，而HTTP中的keep-alive是让一个TCP连接获得更久一点。

#### 1.40 介绍一下HTTPS的流程。

**参考回答**

​    **HTTPS在传输的过程中会涉及到三个密钥：**服务器端的公钥和私钥，用来进行非对称加密；客户端生成的随机密钥，用来进行对称加密。一个HTTPS请求实际上包含了两次HTTP传输，如下图可以细分为以下8步：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697976462/458D639F0DE2FDB51585834A999C051C)

1. 客户端向服务器发起HTTPS请求，连接到服务器的443端口
2. 服务器端有一个密钥对，即公钥和私钥，是用来进行非对称加密使用的，服务器端保存着私钥，不能将其泄露，公钥可以发送给任何人。
3. 服务器将自己的公钥发送给客户端。
4. 客户端收到服务器端的公钥之后，会对公钥进行检查，验证其合法性，如果发现发现公钥有问题，那么HTTPS传输就无法继续。严格的说，这里应该是验证服务器发送的数字证书的合法性，关于客户端如何验证数字证书的合法性，下文会进行说明。如果公钥合格，那么客户端会生成一个随机值，这个随机值就是用于进行对称加密的密钥，我们将该密钥称之为client key，即客户端密钥，这样在概念上和服务器端的密钥容易进行区分。然后用服务器的公钥对客户端密钥进行非对称加密，这样客户端密钥就变成密文了，至此，HTTPS中的第一次HTTP请求结束。
5. 客户端会发起HTTPS中的第二个HTTP请求，将加密之后的客户端密钥发送给服务器。
6. 服务器接收到客户端发来的密文之后，会用自己的私钥对其进行非对称解密，解密之后的明文就是客户端密钥，然后用客户端密钥对数据进行对称加密，这样数据就变成了密文。
7. 然后服务器将加密后的密文发送给客户端。
8. 客户端收到服务器发送来的密文，用客户端密钥对其进行对称解密，得到服务器发送的数据。这样HTTPS中的第二个HTTP请求结束，整个HTTPS传输完成。

**答案解析**

​    无

#### 1.41 介绍一下HTTP的失败码。

**参考回答**

​    HTTP的错误码包含**客户端错误4XX** 和**服务端错误5XX** ，两种错误分别如下：

1. 客户端错误 4XX

    ​    这类的状态码是适用于客户端似乎有错误的情况。除了响应给HEAD请求外，服务器应该包含一个包括错误情况描述的实体，和它是暂时的还是永久性的。这些状态码适用于任何请求方法。用户代理应该展示所有包含的实体给用户。

    如果客户端正在发送数据，使用TCP的服务器应该在服务器关闭输出链接时，仔细确保客户端确认收到包含响应的数据包（receipt of the packet(s) ） 。如果客户端继续在服务器关闭后发送数据，服务器的TCP栈将会发生一个重置包给客户端，这可能会在 HTTP 应用程序读取和解释客户端的未确认输入缓冲区（input buffers）之前将其擦除。

    **400**(错误请求) 服务器不理解请求的语法。

    **401**(未授权) 请求要求进行身份验证。登录后，服务器可能会返回对页面的此响应。

    **403**(已禁止) 服务器拒绝请求。如果在 Googlebot 尝试抓取您网站上的有效网页时显示此状态代码(您可在 Google 网站管理员工具中诊断下的网络抓取页面上看到此状态代码)，那么，这可能是您的服务器或主机拒绝 Googlebot 对其进行访问。

    **404**(未找到) 服务器找不到请求的网页。例如，如果请求是针对服务器上不存在的网页进行的，那么，服务器通常会返回此代码。

    如果您的网站上没有 robots.txt 文件，而您在 Google 网站管理员工具”诊断”标签的 robots.txt 页上发现此状态，那么，这是正确的状态。然而，如果您有 robots.txt 文件而又发现了此状态，那么，这说明您的 robots.txt 文件可能是命名错误或位于错误的位置。(该文件应当位于顶级域名上，且应当名为 robots.txt)。

    如果您在 Googlebot 尝试抓取的网址上发现此状态(位于”诊断”标签的 HTTP 错误页上)，那么，这表示 Googlebot 所追踪的可能是另一网页中的无效链接(旧链接或输入有误的链接)。

    **405**(方法禁用) 禁用请求中所指定的方法。

    **406**(不接受) 无法使用请求的内容特性来响应请求的网页。

    **407**(需要代理授权) 此状态代码与 401(未授权)类似，但却指定了请求者应当使用代理进行授权。如果服务器返回此响应，那么，服务器还会指明请求者应当使用的代理。

    **408**(请求超时) 服务器等候请求时超时。

    **409**(冲突) 服务器在完成请求时发生冲突。服务器必须包含有关响应中所发生的冲突的信息。服务器在响应与前一个请求相冲突的 PUT 请求时可能会返回此代码，同时会提供两个请求的差异列表。

    **410**(已删除) 如果请求的资源已被永久删除，那么，服务器会返回此响应。该代码与 404(未找到)代码类似，但在资源以前有但现在已经不复存在的情况下，有时会替代 404 代码出现。如果资源已被永久删除，那么，您应当使用 301 代码指定该资源的新位置。

    **411**(需要有效长度) 服务器不会接受包含无效内容长度标头字段的请求。

    **412**(未满足前提条件) 服务器未满足请求者在请求中设置的其中一个前提条件。

    **413**(请求实体过大) 服务器无法处理请求，因为请求实体过大，已超出服务器的处理能力。

    **414**(请求的 URI 过长) 请求的 URI(通常为网址)过长，服务器无法进行处理。

    **415**(不支持的媒体类型) 请求的格式不受请求页面的支持。

    **416**(请求范围不符合要求) 如果请求是针对网页的无效范围进行的，那么，服务器会返回此状态代码。

    **417**(未满足期望值) 服务器未满足”期望”请求标头字段的要求。

2. 服务端错误 5XX

    ​    响应状态码已数字5开头，表明了这类服务器知道其错误或者无法执行请求的情况。出了响应HEAD请求外，服务器应该包括一个包含错误情况说明的实体，以及他是暂时地还是永久性的，用户代理应该将所有包含的实体展示给用户。这些响应代码适用于任何请求方法。

    **500**(服务器内部错误) 服务器遇到错误，无法完成请求。

    **501**(尚未实施) 服务器不具备完成请求的功能。例如，当服务器无法识别请求方法时，服务器可能会返回此代码。

    **502**(错误网关) 服务器作为网关或代理，从上游服务器收到了无效的响应。

    **503**(服务不可用) 目前无法使用服务器(由于超载或进行停机维护)。通常，这只是一种暂时的状态。

    **504**(网关超时) 服务器作为网关或代理，未及时从上游服务器接收请求。

    **505**(HTTP 版本不受支持) 服务器不支持请求中所使用的 HTTP 协议版本。

#### 1.42 说一说你知道的http状态码。

**参考回答**

​    HTTP状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。HTTP状态码共分为5种类型，分类及分类描述如下表：

| 分类 | 分类描述                                       |
| :--- | :--------------------------------------------- |
| 1**  | 信息，服务器收到请求，需要请求者继续执行操作   |
| 2**  | 成功，操作被成功接收并处理                     |
| 3**  | 重定向，需要进一步的操作以完成请求             |
| 4**  | 客户端错误，请求包含语法错误或无法完成请求     |
| 5**  | 服务器错误，服务器在处理请求的过程中发生了错误 |

**各类别常见状态码**有如下几种：

1. **2xx （3种）**

    **200 OK：**表示从客户端发送给服务器的请求被正常处理并返回；

    **204 No Content：**表示客户端发送给客户端的请求得到了成功处理，但在返回的响应报文中不含实体的主体部分（没有资源可以返回）；

    **206 Patial Content：**表示客户端进行了范围请求，并且服务器成功执行了这部分的GET请求，响应报文中包含由Content-Range指定范围的实体内容。

2. **3xx （5种）**

    **301 Moved Permanently：**永久性重定向，表示请求的资源被分配了新的URL，之后应使用更改的URL；

    **302 Found：**临时性重定向，表示请求的资源被分配了新的URL，希望本次访问使用新的URL；

    301与302的区别：前者是永久移动，后者是临时移动（之后可能还会更改URL）

    **303 See Other：**表示请求的资源被分配了新的URL，应使用GET方法定向获取请求的资源；

    302与303的区别：后者明确表示客户端应当采用GET方式获取资源

    **304 Not Modified：**表示客户端发送附带条件（是指采用GET方法的请求报文中包含if-Match、If-Modified-Since、If-None-Match、If-Range、If-Unmodified-Since中任一首部）的请求时，服务器端允许访问资源，但是请求为满足条件的情况下返回该状态码；

    **307 Temporary Redirect：**临时重定向，与303有着相同的含义，307会遵照浏览器标准不会从POST变成GET；（不同浏览器可能会出现不同的情况）；

3. **4xx （4种）**

    **400 Bad Request：**表示请求报文中存在语法错误；

    **401 Unauthorized：**未经许可，需要通过HTTP认证；

    **403 Forbidden：**服务器拒绝该次访问（访问权限出现问题）

    **404 Not Found：**表示服务器上无法找到请求的资源，除此之外，也可以在服务器拒绝请求但不想给拒绝原因时使用；

4. **5xx （2种）**

    **500 Inter Server Error：**表示服务器在执行请求时发生了错误，也有可能是web应用存在的bug或某些临时的错误时；

    **503 Server Unavailable：**表示服务器暂时处于超负载或正在进行停机维护，无法处理请求；

**答案解析**

​    无

#### 1.43 301和302有什么区别？

**参考回答**

​    301和302的区别在于，**301重定向是永久的重定向**，搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址。**302重定向是暂时的重定向**，搜索引擎会抓取新的内容而保存旧的网址。由于效劳器前往302代码，搜索引擎以为新的网址只是暂时的。

#### 1.44 302和304有什么区别？

**参考回答**

​    302和304是网页请求的两个不同的响应状态码。302 （临时移动）表示 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 304 （未修改）表示 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。

**答案解析**

​    无

#### 1.45 请描述一次完整的HTTP请求的过程。

**参考回答**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697994855/F62F028E87333BF49CEC5C06C122FB73)

​                                                                                         DNS解析流程图

1. 首先客户端位置是一台电脑或手机，在打开浏览器以后，比如输入[http://www.zdns.cn](http://www.zdns.cn/)的域名，它首先是由浏览器发起一个DNS解析请求，如果本地缓存服务器中找不到结果，则首先会向根服务器查询，根服务器里面记录的都是各个顶级域所在的服务器的位置，当向根服务器请求[http://www.zdns.cn](http://www.zdns.cn/)的时候，根服务器就会返回.cn服务器的位置信息；
2. 递归服务器拿到.cn的权威服务器地址以后，就会寻问.cn的权威服务器，知不知道[http://www.zdns.cn](http://www.zdns.cn/)的位置。这个时候.cn权威服务器查找并返回[http://zdns.cn](http://zdns.cn/)服务器的地址；
3. 继续向[http://zdns.cn](http://zdns.cn/)的权威服务器去查询这个地址，由[http://zdns.cn](http://zdns.cn/)的服务器给出了地址：202.173.11.10；
4. 最终进入http的链接，顺利访问网站；

**补充说明**：一旦递归服务器拿到解析记录以后，就会在本地进行缓存，如果下次客户端再请求本地的递归域名服务器相同域名的时候，就不会再这样一层一层查了，因为本地服务器里面已经有缓存了，这个时候就直接把[http://www.zdns.cn](http://www.zdns.cn/)的记录返回给客户端就可以了。

#### 1.46 什么是重定向？

**参考回答**

​     **重定向(Redirect)**就是通过各种方法将各种网络请求重新定个方向转到其它位置（如：网页重定向、域名的重定向、路由选择的变化也是对数据报文经由路径的一种重定向）。

**答案解析**

1. 需要重定向的情况

    （1）网站调整（如改变网页目录结构）；

    （2）网页被移到一个新地址；

    （3）网页扩展名改变(如应用需要把.php改成.Html或.shtml)。

    这几种情况下，如果不做重定向，则用户收藏夹或搜索引擎数据库中旧地址只能让访问客户得到一个404 页面错误信息，访问流量白白丧失；再者某些注册了多个域名的网站，也需要通过重定向让访问这些域名的用户自动跳转到主站点等。

2. 常用的重定向的方式

    （1）301 redirect-----永久性转移

    ​    当用户或搜索引擎向网站服务器发出浏览请求时，服务器返回的HTTP数据流中头信息(header)中的状态码的一种，表示本网页永久性转移到另一个地址。

    （2）302 redirect-----暂时性转移 (Temporarily Moved )

    ​    也被认为是**暂时重定向**（temporary redirect），一条对网站浏览器的指令来显示浏览器被要求显示的不同的URL，当一个网页经历过短期的URL的变化时使用。一个暂时重定向是一种服务器端的重定向，能够被搜索引擎蜘蛛正确地处理。

3. 新旧重定向方式的区别

    ​    **302重定向是暂时的重定向**，搜索引擎会抓取新的内容而保存旧的网址。由于效劳器前往302代码，搜索引擎以为新的网址只是暂时的；

    ​    **301重定向是永久的重定向**，搜索引擎在抓取新内容的同时也将旧的网址交换为重定向之后的网址。

4. 为什么302 重定向和网址劫持有关联

    ​    从网址A 做一个302 重定向到网址B 时，主机服务器的隐含意思是网址A 随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到302 重定向时，一般只要去抓取目标网址就可以了，也就是说网址B。如果搜索引擎在遇到302 转向时，百分之百的都抓取目标网址B 的话，就不用担心网址URL 劫持了。问题就在于，有的时候搜索引擎，尤其是Google，并不能总是抓取目标网址。

    ​    比如说，有的时候A 网址很短，但是它做了一个302 重定向到B 网址，而B 网址是一个很长的乱七八糟的URL 网址，甚至还有可能包含一些问号之类的参数。很自然的，A 网址更加用户友好，而B 网址既难看，又不用户友好。这时Google 很有可能会仍然显示网址A。由于搜索引擎排名算法只是程序而不是人，在遇到302 重定向的时候，并不能像人一样的去准确判定哪一个网址更适当，这就造成了网址URL 劫持的可能性。也就是说，一个不道德的人在他自己的网址A 做一个302 重定向到你的网址B，出于某种原因， Google 搜索结果所显示的仍然是网址A，但是所用的网页内容却是你的网址B 上的内容，这种情况就叫做网址URL 劫持。你辛辛苦苦所写的内容就这样被别人偷走了。

    ​    302 重定向所造成的网址URL 劫持现象，已经存在一段时间了。不过到目前为止，似乎也没有什么更好的解决方法。在正在进行的数据中心转换中，302 重定向问题也是要被解决的目标之一。从一些搜索结果来看，网址劫持现象有所改善，但是并没有完全解决。

#### 1.47 重定向和请求转发有什么区别？

**参考回答**

1. 请求转发

    ​    客户首先发送一个请求到服务器端，服务器端发现匹配的servlet，并指定它去执行，当这个servlet执行完之后，它要调用getRequestDispacther()方法，把请求转发给指定的student_list.jsp,整个流程都是在服务器端完成的，而且是在同一个请求里面完成的，因此servlet和jsp共享的是同一个request，在servlet里面放的所有东西，在student_list中都能取出来，因此，student_list能把结果getAttribute()出来，getAttribute()出来后执行完把结果返回给客户端。整个过程是一个请求，一个响应。

2. 重定向

    ​    客户发送一个请求到服务器，服务器匹配servlet，servlet处理完之后调用了sendRedirect()方法，立即向客户端返回这个响应，响应行告诉客户端你必须要再发送一个请求，去访问student_list.jsp，紧接着客户端收到这个请求后，立刻发出一个新的请求，去请求student_list.jsp,这里两个请求互不干扰，相互独立，在前面request里面setAttribute()的任何东西，在后面的request里面都获得不了。可见，在sendRedirect()里面是两个请求，两个响应。（服务器向浏览器发送一个302状态码以及一个location消息头，浏览器收到请求后会向再次根据重定向地址发出请求）

3. **二者区别**

    （1）请求次数：重定向是浏览器向服务器发送一个请求并收到响应后再次向一个新地址发出请求，转发是服务器收到请求后为了完成响应跳转到一个新的地址；重定向至少请求两次，转发请求一次；

    （2）地址栏不同：重定向地址栏会发生变化，转发地址栏不会发生变化；

    （3）是否共享数据：重定向两次请求不共享数据，转发一次请求共享数据（在request级别使用信息共享，使用重定向必然出错）；

    （4）跳转限制：重定向可以跳转到任意URL，转发只能跳转本站点资源；

    （5）发生行为不同：重定向是客户端行为，转发是服务器端行为。

**答案解析**

​    无

#### 1.48 介绍一下DNS寻址的过程。

**参考回答**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645698014644/28D4DE3BCFF84220284A4A692BB2ECB4)

​                                                                                         DNS解析流程图

1. 首先客户端位置是一台电脑或手机，在打开浏览器以后，比如输入[http://www.zdns.cn](http://www.zdns.cn/)的域名，它首先是由浏览器发起一个DNS解析请求，如果本地缓存服务器中找不到结果，则首先会向根服务器查询，根服务器里面记录的都是各个顶级域所在的服务器的位置，当向根服务器请求[http://www.zdns.cn](http://www.zdns.cn/)的时候，根服务器就会返回.cn服务器的位置信息；
2. 递归服务器拿到.cn的权威服务器地址以后，就会寻问.cn的权威服务器，知不知道[http://www.zdns.cn](http://www.zdns.cn/)的位置。这个时候.cn权威服务器查找并返回[http://zdns.cn](http://zdns.cn/)服务器的地址；
3. 继续向[http://zdns.cn](http://zdns.cn/)的权威服务器去查询这个地址，由[http://zdns.cn](http://zdns.cn/)的服务器给出了地址：202.173.11.10；
4. 最终进入http的链接，顺利访问网站；

**补充说明**：一旦递归服务器拿到解析记录以后，就会在本地进行缓存，如果下次客户端再请求本地的递归域名服务器相同域名的时候，就不会再这样一层一层查了，因为本地服务器里面已经有缓存了，这个时候就直接把[http://www.zdns.cn](http://www.zdns.cn/)的记录返回给客户端就可以了。

**答案解析**

1. 什么是DNS

    ​    DNS就是域名系统，是因特网中的一项核心服务，是用于实现域名和IP地址相互映射的一个分布式数据库，能够使用户更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。通过主机名，得到该主机名对应的IP地址的过程叫做域名解析（或主机名解析）。

2. 域名解析结构

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645698033312/F5395EA0B6DFBE3BE2CD369BB867098C)

    ​    如上图所示，域名结构是树状结构，树的最顶端代表根服务器，根的下一层就是由我们所熟知的.com、.net、.cn等通用域和.cn、.uk等国家域组成，称为顶级域。网上注册的域名基本都是二级域名，比如[http://baidu.com](http://baidu.com/)、[http://taobao.com](http://taobao.com/)等等二级域名，它们基本上是归企业和运维人员管理。接下来是三级或者四级域名，这里不多赘述。总体概括来说域名是由整体到局部的机制结构。

#### 1.49 说一说你对TIME_WAIT的理解。

**参考回答**

1. 出现 TIME_WAIT的状态原因

    ​    TIME_WAIT状态之所以存在,是为了保证网络的可靠性。由于TCP连接是双向的，所以在关闭连接的时候，两个方向各自都需要关闭。先发FIN包的一方执行的是主动关闭，后发送FIN包的一方执行的是被动关闭。主动关闭的一方会进入TIME_WAIT状态，并且在此状态停留2MSL时长。如果Server端一直没有向client端发送FIN消息(调用close() API)，那么这个CLOSE_WAIT会一直存在下去。

2. MSL概念

    ​    其指的是报文段的最大生存时间。如果报文段在网络中活动了MSL时间，还没有被接收，那么就会被丢弃。关于MSL的大小，RFC 793协议中给出的建议是2分钟，不过Linux中，通常是半分钟。

3. TIME_WAIT持续两个MSL的作用

    ​    首先，可靠安全地关闭TCP连接。比如网络拥塞，如果主动关闭方最后一个ACK没有被被动关闭方接收到，这时被动关闭方会对FIN进行超时重传，在这时尚未关闭的TIME_WAIT就会把这些尾巴问题处理掉，不至于对新连接及其他服务产生影响。其次，防止由于没有持续TIME_WAIT时间导致的新的TCP连接建立起来，延迟的FIN重传包会干扰新的连接。

4. TIME_WAIT占用的资源

    ​    少量内存（大概4K）和一个文件描述符fd。

5. TIME_WAIT关闭的危害

    ​    首先，当网络情况不好时，如果主动方无TIME_WAIT等待，关闭前个连接后，主动方与被动方又建立起新的TCP连接，这时被动方重传或延时过来的FIN包到达后会直接影响新的TCP连接；其次，当网络情况不好时，同时没有TIME_WAIT等待时，关闭连接后无新连接，那么当接收到被动方重传或延迟的FIN包后，会给被动方回送一个RST包，可能会影响被动方其他的服务连接。

**答案解析**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645698050364/44ADD6D4E513016FCA28EB10181126F0)

当client端传输完成数据，或者需要断开连接时：

1. Client端发送一个FIN报文给Server端。表示要终止Client到Server这个方向的连接。通过调用close(socket) API。表示Client不再会发送数据到Server端。(但Server还能继续发给Client端)。Client状态变为FIN_WAIT_1。
2. Server端收到FIN后，发送一个ACK报文给Client端(序号为M+1)。Server状态变为CLOSE_WAIT，Client收到序号为(M+1)的ACK后状态变为FIN_WAIT_2。Server端也发送一个FIN报文给Client端。(序号为N) 表示Server也要终止到Client端这个方向的连接。通过调用close(socket) API。Server端状态变为LAST_ACK。
3. Client端收到报文FIN后，也发送一个ACK报文给服务器。(序号N+1)，Client状态变为TIME_WAIT。
4. Server端收到序号为(N+1)的ACK， Server的状态变为CLOSED。
5. 等带2MSL之后，Client的状态也变为CLOSE。

至此，一个完整的TCP连接就关闭了。

#### 1.50 TIME_WAIT、CLOSE_WAIT状态发生在哪一步？

**参考回答**

1. **TIME_WAIT状态**发生在客户端主动关闭连接时，发送最后一个ack后；**CLOSE_WAIT状态**发生在在Sever端收到Client的FIN消息之后。

2. 出现 TIME_WAIT的状态原因

    ​    TIME_WAIT状态之所以存在,是为了保证网络的可靠性。由于TCP连接是双向的，所以在关闭连接的时候，两个方向各自都需要关闭。先发FIN包的一方执行的是主动关闭，后发送FIN包的一方执行的是被动关闭。主动关闭的一方会进入TIME_WAIT状态，并且在此状态停留2MSL时长。如果Server端一直没有向client端发送FIN消息(调用close() API)，那么这个CLOSE_WAIT会一直存在下去。

3. 出现CLOSE_WAIT的状态原因

    ​    假设最终的ACK丢失，server将重发FIN，client必须维护TCP状态信息以便可以重发最终的ACK，否则会发送RST，结果server认为发生错误。TCP实现必须可靠地终止连接的两个方向(全双工关闭)，client必须进入 TIME_WAIT 状态，因为client可能面临重发最终ACK的情形。

4. 为什么 TIME_WAIT 状态需要保持 2MSL 这么长的时间？

    ​    如果 TIME_WAIT 状态保持时间不足够长(比如小于2MSL)，第一个连接就正常终止了。第二个拥有相同相关五元组的连接出现，而第一个连接的重复报文到达，干扰了第二个连接。TCP实现必须防止某个连接的重复报文在连接终止后出现，所以让TIME_WAIT状态保持时间足够长(2MSL)，连接相应方向上的TCP报文要么完全响应完毕，要么被丢弃。建立第二个连接的时候，不会混淆。

**答案解析**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645698066180/FAAE6BAE6215C267D988957CFCF5B2A8)

当client端传输完成数据，或者需要断开连接时：

1. Client端发送一个FIN报文给Server端。表示要终止Client到Server这个方向的连接。通过调用close(socket) API。表示Client不再会发送数据到Server端。(但Server还能继续发给Client端)。Client状态变为FIN_WAIT_1。
2. Server端收到FIN后，发送一个ACK报文给Client端(序号为M+1)。Server状态变为CLOSE_WAIT，Client收到序号为(M+1)的ACK后状态变为FIN_WAIT_2。Server端也发送一个FIN报文给Client端。(序号为N) 表示Server也要终止到Client端这个方向的连接。通过调用close(socket) API。Server端状态变为LAST_ACK。
3. Client端收到报文FIN后，也发送一个ACK报文给服务器。(序号N+1)，Client状态变为TIME_WAIT。
4. Server端收到序号为(N+1)的ACK， Server的状态变为CLOSED。
5. 等带2MSL之后，Client的状态也变为CLOSE。

至此，一个完整的TCP连接就关闭了。

#### 1.51 有大量的TIME_WAIT状态怎么办？

**参考回答**

1. time_wait 状态的影响

    ​    TCP 连接中，主动发起关闭连接的一端，会进入 time_wait 状态，time_wait 状态，默认会持续 2 MSL（报文的最大生存时间），一般是 2x2 mins，time_wait 状态下，TCP 连接占用的端口，无法被再次使用，TCP 端口数量，上限是 6.5w（65535，16 bit），**大量 time_wait 状态存在，会导致新建 TCP 连接会出错，address already in use : connect异常**。

2. **解决办法**

    （1）**客户端**：HTTP 请求的头部，connection 设置为 keep-alive，保持存活一段时间：现在的浏览器，一般都这么进行了 。

    （2）**服务器端**

    ​    a. 允许 time_wait状态的 socket 被**重用**

    ​    b. 缩减 time_wait 时间，设置为 1 MSL（即，2 mins）

**答案解析**

​    无

#### 1.52 请介绍socket通信的具体步骤。

**参考回答**

​     sockets（套接字）编程有三种：**流式套接字（SOCK_STREAM），数据报套接字（SOCK_DGRAM），原始套接字（SOCK_RAW）**；基于TCP的socket编程是采用的流式套接字。

1. 服务器端编程的步骤

    （1）加载套接字库，创建套接字(WSAStartup()/socket())；

    （2）绑定套接字到一个IP地址和一个端口上(bind())；

    （3）将套接字设置为监听模式等待连接请求(listen())；

    （4）请求到来后，接受连接请求，返回一个新的对应于此次连接的套接字(accept())；

    （5）用返回的套接字和客户端进行通信(send()/recv())；

    （6）返回，等待另一连接请求；

    （7）关闭套接字，关闭加载的套接字库(closesocket()/WSACleanup())。

2. 客户端编程的步骤：

    （1）加载套接字库，创建套接字(WSAStartup()/socket())；

    （2）向服务器发出连接请求(connect())；

    （3）和服务器端进行通信(send()/recv())；

    （4）关闭套接字，关闭加载的套接字库(closesocket()/WSACleanup())。    

**答案解析**

```
//代码实例（服务器） #include <stdio.h> #include <Winsock2.h> void main() {  WORD wVersionRequested;  WSADATA wsaData;  int err;     wVersionRequested = MAKEWORD( 1, 1 );     err = WSAStartup( wVersionRequested, &wsaData );  if ( err != 0 ) {   return;  }     if ( LOBYTE( wsaData.wVersion ) != 1 ||         HIBYTE( wsaData.wVersion ) != 1 ) {   WSACleanup( );   return;  }  SOCKET sockSrv=socket(AF_INET,SOCK_STREAM,0);    SOCKADDR_IN addrSrv;  addrSrv.sin_addr.S_un.S_addr=htonl(INADDR_ANY);  addrSrv.sin_family=AF_INET;  addrSrv.sin_port=htons(6000);     bind(sockSrv,(SOCKADDR*)&addrSrv,sizeof(SOCKADDR));    listen(sockSrv,5);    SOCKADDR_IN addrClient;  int len=sizeof(SOCKADDR);  while(1)  {   SOCKET sockConn=accept(sockSrv,(SOCKADDR*)&addrClient,&len);   char sendBuf[50];   sprintf(sendBuf,"Welcome %s to here!",inet_ntoa(addrClient.sin_addr));   send(sockConn,sendBuf,strlen(sendBuf)+1,0);   char recvBuf[50];   recv(sockConn,recvBuf,50,0);   printf("%s\n",recvBuf);   closesocket(sockConn);  }   }
//代码实例（客户端） #include <stdio.h> #include <Winsock2.h> void main() {  WORD wVersionRequested;  WSADATA wsaData;  int err;    wVersionRequested = MAKEWORD( 1, 1 );    err = WSAStartup( wVersionRequested, &wsaData );  if ( err != 0 ) {   return;  }    if ( LOBYTE( wsaData.wVersion ) != 1 ||         HIBYTE( wsaData.wVersion ) != 1 ) {   WSACleanup( );   return;  }  SOCKET sockClient=socket(AF_INET,SOCK_STREAM,0);    SOCKADDR_IN addrSrv;  addrSrv.sin_addr.S_un.S_addr=inet_addr("127.0.0.1");  addrSrv.sin_family=AF_INET;  addrSrv.sin_port=htons(6000);  connect(sockClient,(SOCKADDR*)&addrSrv,sizeof(SOCKADDR));  send(sockClient,"hello",strlen("hello")+1,0);  char recvBuf[50];  recv(sockClient,recvBuf,50,0);  printf("%s\n",recvBuf);    closesocket(sockClient);  WSACleanup(); }
```

#### 1.53 服务端怎么提高处理socket连接的性能？

**参考回答**

​    提高处理socket连接的性能，请遵循以下技巧：

1. 最小化报文传输的延时。
2. 最小化系统调用的负载。
3. 为 Bandwidth Delay Product 调节 TCP 窗口。
4. 动态优化 GNU/Linux TCP/IP 栈。

**答案解析**

1. 最小化报文传输的延时。

    ​    在通过 TCP socket 进行通信时，数据都拆分成了数据块，这样它们就可以封装到给定连接的 TCP payload（指 TCP 数据包中的有效负荷）中了。TCP payload 的大小取决于几个因素（例如最大报文长度和路径），但是这些因素在连接发起时都是已知的。为了达到最好的性能，我们的目标是使用尽可能多的可用数据来填充每个报文。当没有足够的数据来填充 payload 时（也称为最大报文段长度（maximum segment size）或 MSS），TCP 就会采用 Nagle 算法自动将一些小的缓冲区连接到一个报文段中。这样可以通过最小化所发送的报文的数量来提高应用程序的效率，并减轻整体的网络拥塞问题。

2. 最小化系统调用的负载。

    ​    任何时候通过一个 socket 来读写数据时，都是在使用一个系统调用（system call）。这个调用（例如 read 或 write）跨越了用户空间应用程序与内核的边界。另外，在进入内核之前，该调用会通过 C 库来进入内核中的一个通用函数（system_call()）。从 system_call()中，这个调用会进入文件系统层，内核会在这儿确定正在处理的是哪种类型的设备。最后，调用会进入 socket 层，数据就是在这里进行读取或进行排队从而通过 socket 进行传输的（这涉及数据的副本）。

    ​    这个过程说明**系统调用不仅仅是在应用程序和内核中进行操作的，而且还要经过应用程序和内核中的很多层次。**这个过程耗费的资源很高，因此调用次数越多，通过这个调用链进行的工作所需要的时间就越长，应用程序的性能也就越低。由于我们**无法避免这些系统调用**，因此**唯一的选择是最小化使用这些调用的次数**。

3. 为 Bandwidth Delay Product 调节 TCP 窗口。

    ​    TCP 的性能取决于几个方面的因素。两个最重要的因素是链接带宽（link bandwidth）（报文在网络上传输的速率）和 往返时间（round-trip time） 或 RTT（发送报文与接收到另一端的响应之间的延时）。这两个值确定了称为 Bandwidth Delay Product（BDP）的内容。

    ​    给定链接带宽和 RTT 之后，就可以计算出 BDP 的值了，不过这代表什么意义呢？BDP 给出了一种简单的方法来计算理论上最优的 TCP socket 缓冲区大小（其中保存了排队等待传输和等待应用程序接收的数据）。如果缓冲区太小，那么 TCP 窗口就不能完全打开，这会对性能造成限制。如果缓冲区太大，那么宝贵的内存资源就会造成浪费。如果设置的缓冲区大小正好合适，那么就可以完全利用可用的带宽。

4. 动态优化 GNU/Linux TCP/IP 栈。

    ​    标准的 GNU/Linux 发行版试图对各种部署情况都进行优化。这意味着标准的发行版可能并没有对现有的环境进行特殊的优化。GNU/Linux 提供了很多可调节的内核参数，可以使用这些参数为自己的操作系统进行动态配置。

#### 1.54 介绍一下流量控制和拥塞控制。

**参考回答**

1. 流量控制和拥塞控制定义

    **流量控制**    

    ​    如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。**流量控制**就是让发送方慢点，要让接收方来得及接收。

    **拥塞控制**

    ​    拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。

2. 流量控制和拥塞控制区别

    ​    流量控制是**端到端**的控制，例如A通过网络给B发数据，A发送的太快导致B没法接收(B缓冲窗口过小或者处理过慢)，这时候的控制就是流量控制，原理是通过滑动窗口的大小改变来实现。

    ​    拥塞控制是A与B之间的网络发生堵塞导致传输过慢或者丢包，来不及传输。防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不至于过载。拥塞控制是一个**全局性**的过程，涉及到所有的主机、路由器，以及与降低网络性能有关的所有因素。

3. TCP流量控制解决方法

    ​    TCP的流量控制是利用**滑动窗口机制**实现的，接收方在返回的数据中会包含自己的接收窗口的大小，以控制发送方的数据发送。

4. TCP拥塞控制解决方法

    ​    TCP拥塞控制的四种算法：**慢开始、拥塞避免、快重传、快恢复。**

    （1）**慢开始算法：**当主机开始发送数据时，并不清楚网络的负载情况，所以由小到大逐渐增大拥塞窗口，每经过一个传输轮次没有出现超时就将拥塞窗口加倍。同时还需要设置一个慢开始门限，在拥塞窗口小于慢开始门限时使用慢开始算法，大于慢开始门限时，使用拥塞避免算法；

    （2）**拥塞避免算法：**在拥塞窗口大于慢开始门限时，让拥塞窗口按线性规律缓慢增长。即每经过一个传输轮次，拥塞窗口增大一个MSS最大报文段尺寸。（拥塞避免并非完全能够避免拥塞，只是使网络比较不容易出现拥塞）

    （3）**快重传算法：**使发送方今早知道发生了个别报文段丢失，并不是出现网络拥塞。

    要求接受不要登塞自己发送数据时才进行捎带确认，而是立即发送确认，即使收到了失序的报文段也要立即发出对已收到报文段的重复确认。而发送方一旦受到三个连续的重读确认，就将相应的报文段立即重传。

    （4）**快恢复算法：**发送方知道只有个别报文段丢失而不是网络拥塞时，不启动慢开始算法，而是执行快恢复算法，将慢开始门限和拥塞窗口值调整为当前窗口的一半，开始执行拥塞避免算法    

**答案解析**

​    无

#### 1.55 对路由协议是否有所了解？

**参考回答**

​    有了解。

1. 路由协议定义

    ​    **路由协议**（英语：Routing protocol）是一种指定数据包转送方式的网上协议。Internet网络的主要节点设备是路由器，路由器通过路由表来转发接收到的数据。转发策略可以是人工指定的（通过静态路由、策略路由等方法）。在具有较小规模的网络中，人工指定转发策略没有任何问题。但是在具有较大规模的网络中（如跨国企业网络、ISP网络），如果通过人工指定转发策略，将会给网络管理员带来巨大的工作量，并且在管理、维护路由表上也变得十分困难。为了解决这个问题，动态路由协议应运而生。动态路由协议可以让路由器自动学习到其他路由器的网络，并且网络拓扑发生改变后自动更新路由表。网络管理员只需要配置动态路由协议即可，相比人工指定转发策略，工作量大大减少。

2. 原理

    ​    路由协议通过在路由器之间共享路由信息来支持可路由协议。路由信息在相邻路由器之间传递，确保所有路由器知道到其它路由器的路径。总之，路由协议创建了路由表，描述了网络拓扑结构；路由协议与路由器，执行路由选择和数据包转发功能。

3. 路由器的作用以及常见的路由协议

    ​    **路由协议主要运行于路由器上，路由协议是用来确定到达路径的，起到一个地图导航，负责找路的作用。**它工作在网络层。它包括RIP，IGRP（Cisco私有协议），EIGRP（Cisco私有协议），OSPF，IS-IS，BGP。以下为这六个协议的详细说明：

    （1）RIP（路由信息协议）

    ​    RIP很早就被用在Internet上，是**最简单的路由协议**。它是“路由信息协议（Route Information Protocol）”的简写，主要传递路由信息，通过每隔30秒广播一次路由表，维护相邻路由器的位置关系，同时根据收到的路由表信息计算自己的路由表信息。RIP是一个**距离矢量路由协议**，最大跳数为15跳，超过15跳的网络则认为目标网络不可达。此协议通常用在网络架构较为简单的小型网络环境。分为RIPv1和RIPv2两个版本，后者支持VLSM技术以及一系列技术上的改进。RIP的收敛速度较慢。

    （2）IGRP（内部网关路由协议）

    ​    IGRP协议是“内部网关路由协议（Interior Gateway Routing Protocol）”的缩写，由Cisco于二十世纪八十年代独立开发，属于Cisco私有协议。IGRP和RIP一样，同属距离矢量路由协议，因此在诸多方面有着相似点，如IGRP也是周期性的广播路由表，也存在最大跳数（默认为100跳，达到或超过100跳则认为目标网络不可达）。IGRP最大的特点是使用了混合度量值，同时考虑了链路的带宽、延迟、负载、MTU、可靠性5个方面来计算路由的度量值，而不像其他IGP协议单纯的考虑某一个方面来计算度量值。IGRP已经被Cisco独立开发的EIGRP协议所取代，版本号为12.3及其以上的Cisco IOS（Internetwork Operating System）已经不支持该协议，已经罕有运行IGRP协议的网络。

    （3）EIGRP（增强型内部网关路由协议）

    ​    由于IGRP协议的种种缺陷以及不足，Cisco开发了EIGRP协议（增强型内部网关路由协议）来取代IGRP协议。EIGRP属于高级距离矢量路由协议（又称混合型路由协议），继承了IGRP的混合度量值，最大特点在于引入了非等价负载均衡技术，并拥有极快的收敛速度。EIGRP协议在Cisco设备网络环境中广泛部署。

    （4）OSPF（开放式最短路径优先）

    ​    OSPF协议是“开放式最短路径优先（Open Shortest Path First）”的缩写，属于链路状态路由协议。OSPF提出了“区域（area）”的概念，每个区域中所有路由器维护着一个相同的链路状态数据库（LSDB）。区域又分为骨干区域（骨干区域的编号必须为0）和非骨干区域（非0编号区域），如果一个运行OSPF的网络只存在单一区域，则该区域可以是骨干区域或者非骨干区域。如果该网络存在多个区域，那么必须存在骨干区域，并且所有非骨干区域必须和骨干区域直接相连。OSPF利用所维护的链路状态数据库，通过最短路径优先算法（SPF算法）计算得到路由表。OSPF的收敛速度较快。由于其特有的开放性以及良好的扩展性，OSPF协议在各种网络中广泛部署。

    （5）IS-IS（中间系统到中间系统）

    ​    IS-IS协议是Intermediate system to intermediate system（中间系统到中间系统）的缩写，属于链路状态路由协议。标准IS-IS协议是由国际标准化组织制定的ISO/IEC 10589:2002所定义的，标准IS-IS不适合用于IP网络，因此IETF制定了适用于IP网络的集成化IS-IS协议（Integrated IS-IS）。和OSPF相同，IS-IS也使用了“区域”的概念，同样也维护着一份链路状态数据库，通过最短生成树算法（SPF）计算出最佳路径。IS-IS的收敛速度较快。集成化IS-IS协议是ISP骨干网上最常用的IGP协议。

    （6）BGP（边界网关协议）

    ​    为了维护各个ISP的独立利益，标准化组织制定了ISP间的路由协议BGP。BGP是“边界网关协议（Border Gateway Protocol）”的缩写，处理各ISP之间的路由传递。但是BGP运行在相对核心的地位，需要用户对网络的结构有相当的了解，否则可能会造成较大损失。

**答案解析**

​    无

#### 1.56 直播可能需要使用到什么样的协议？

**参考回答**

​    视频直播有多种协议，使用rtmp协议的就是rtmp直播。直播流就是视频流，即传递的视频数据。常见的协议有**RTMP、RTSP、HTTP协议**，这三个协议都属于互联网**TCP/IP五层体系结构中应用层**的协议。理论上这三种都可以用来做视频直播或点播。但通常来说，**直播一般用RTMP、RTSP，而点播用HTTP。**下面分别介绍下三者的特点。

1. RTMP协议

    （1）是流媒体协议；

    （2）RTMP协议是Adobe的私有协议，未完全公开；

    （3）RTMP协议一般传输的是flv，f4v格式流；

    （4）RTMP一般在TCP1个通道上传输命令和数据。

2. RTSP协议

    （1）是流媒体协议；

    （2）RTSP协议是共有协议，并有专门机构做维护；

    （3）RTSP协议一般传输的是ts、mp4格式的流；

    （4）RTSP传输一般需要2-3个通道，命令和数据通道分离。

3. HTTP协议

    （1）不是是流媒体协议；

    （2）HTTP协议是共有协议，并有专门机构做维护；

    （3）HTTP协议没有特定的传输流；

    （4）HTTP传输一般需要2-3个通道，命令和数据通道分离。

**答案解析**

​    **扩展资料**

​    一个完整的视频直播过程，包括**采集、处理、编码、封装、推流、传输、转码、分发、解码、播放等**。

1. 采集

    ​    音频采集音频的采集过程主要通过设备将环境中的模拟信号采集成 PCM 编码的原始数据，然后编码压缩成 MP3 等格式的数据分发出去。常见的音频压缩格式有：MP3，AAC，HE-AAC，Opus，FLAC，Vorbis (Ogg)，Speex 和 AMR等。

    ​    图像采集 图像的采集过程主要由摄像头等设备拍摄成 YUV 编码的原始数据，然后经过编码压缩成 H.264 等格式的数据分发出去。常见的视频封装格式有：MP4、3GP、AVI、MKV、WMV、MPG、VOB、FLV、SWF、MOV、RMVB 和 WebM 等。

2. 处理

    ​    视频或者音频完成采集之后得到原始数据，为了增强一些现场效果或者加上一些额外的效果，我们一般会在将其编码压缩前进行处理。

    ​    视频：美颜、水印、路径、自定义。

    ​    音频：混音、降噪、特效、自定义。

3. 编码

    ​    对流媒体传输来说，编码非常重要，它的编码性能、编码速度和编码压缩比会直接影响整个流媒体传输的用户体验和传输成本。

    ​    常见的视频编码器：

    ​    （1）H.264/AVC

    ​    （2）HEVC/H.265

    ​    （3）VP8

    ​    （4）VP9

    ​    （5）FFmpeg

    ​    音频编码器：Mp3, AAC等。

4. 封装

    ​    把编码器生成的多媒体内容(视频，音频，字幕，章节信息等)混合封装在一起几种常见的封装格式：

    ​    （1）AVI 格式(后缀为 .avi)

    ​    （2）DV-AVI 格式(后缀为 .avi)

    ​    （3）QuickTime File Format 格式(后缀为 .mov)

    ​    （4）MPEG 格式(文件后缀可以是 .mpg .mpeg .mpe .dat .vob .asf .3gp .mp4等)

    ​    （5）WMV 格式(后缀为.wmv .asf)

    ​    （6）Real Video 格式(后缀为 .rm .rmvb)

    ​    （7）Flash Video 格式(后缀为 .flv)

    ​    （8）Matroska 格式(后缀为 .mkv)

    ​    （9）MPEG2-TS 格式 (后缀为 .ts)

    ​    目前，我们在流媒体传输，尤其是直播中主要采用的就是 FLV 和 MPEG2-TS 格式，分别用于 RTMP/HTTP-FLV 和 HLS 协议。

5. 推流

    ​    推流是指使用推流工具等内容抓取软件把直播内容传输到服务器的过程。推送协议主要有三种：

    ​    （1）RTSP(Real Time Streaming Protocol)：实时流传送协议，是用来控制声音或影像的多媒体串流协议, 由Real Networks和Netscape共同提出的;

    ​    （2）RTMP(Real Time Messaging Protocol)：实时消息传送协议，是Adobe公司为Flash播放器和服务器之间音频、视频和数据传输 开发的开放协议;

    ​    （3）HLS(HTTP Live Streaming)：是苹果公司(Apple Inc.)实现的基于HTTP的流媒体传输协议;RTMP是目前主流的流媒体传输协议，广泛用于直播领域，市面上绝大多数的直播产品都采用了这个协议。

    ​    RTMP协议基于 TCP，是一种设计用来进行实时数据通信的网络协议，主要用来在 flash/AIR 平台和支持 RTMP 协议的流媒体/交互服务器之间进行音视频和数据通信。支持该协议的软件包括 Adobe Media Server/Ultrant Media Server/red5 等。它有三种变种：

    ​    （1）RTMP工作在TCP之上的明文协议，使用端口1935；

    ​    （2）RTMPT封装在HTTP请求之中，可穿越防火墙;

    ​    （3）RTMPS类似RTMPT，但使用的是HTTPS连接;

    ​    RTMP协议就像一个用来装数据包的容器，这些数据可以是AMF格式的数据,也可以是FLV中的视/音频数据。一个单一的连接可以通过不同的通道传输多路网络流。这些通道中的包都是按照固定大小的包传输的。

6. 传输

    ​    推送出去的流媒体需要传输到观众，整个链路就是传输网络。

7. 转码

    ​    视频直播播流端的码率是根据推流端决定的，即播流端的码率是与推流端的码率一致的。但是遇到以下场景会造成直播效果较差：推流端码率与播流端带宽不相匹配。当推流端码率较高而客户端带宽资源有限就会导致播放出现卡顿，而当推流端码率较低但是客户端对于直播效率要求较高时会导致播放效果较差。播放器插件需要实现多码率切换。前端播放器插件常可以设置码率切换，这就需要同一路推流可以同时提供多种码率的播流地址。因此，视频直播提供了实时转码功能对同一路推流地址同时提供多路不同码率播流地址提供服务。

8. 分发

    ​    流媒体服务器的作用是负责直播流的发布和转播分发功能。

9. 解码

    ​    编码器(Encoder)：压缩信号的设备或程序；

    ​    解码器(Decoder)：解压缩信号的设备或程序；

    ​    编解码器(Codec)：编解码器对。

10. 播放器流播放主要是实现直播节目在终端上的展现。因为这里使用的传输协议是RTMP， 所以只要支持 RTMP 流协议的播放器都可以使用。

#### 1.57 谈谈单工、双工、半双工的通信方式。

**参考回答**

1. **单工：**数据传输只支持数据在一个方向上传输；在同一时间只有一方能接受或发送信息，不能实现双向通信。举例：电视，广播。
2. **半双工：**半双工数据传输允许数据在两个方向上传输,但是,在某一时刻,只允许数据在一个方向上传输,它实际上是一种切换方向的单工通信；在同一时间只可以有一方接受或发送信息，可以实现双向通信。举例：对讲机。
3. **双工：**全双工数据通信允许数据同时在两个方向上传输,因此,全双工通信是两个单工通信方式的结合,它要求发送设备和接收设备都有独立的接收和发送能力；在同一时间可以同时接受和发送信息，实现双向通信。举例：电话通信。

**答案解析**

​    **扩展资料：**

​    单工、半双工和全双工是电信计算机网络中的三种通信信道。这些通信信道可以提供信息传达的途径。通信信道可以是物理传输介质或通过多路复用介质的逻辑连接。物理传输介质是指能够传播能量波的材料物质，例如数据通信中的导线。并且逻辑连接通常指电路交换连接或分组模式虚拟电路连接，例如无线电信通道。由于通信信道的帮助，信息可以无障碍地传输。

​    单工模式一般用在只向一个方向传输数据的场合。例如计算机与打印机之间的通信是单工模式，因为只有计算机向打印机传输数据，而没有相反方向的数据传输。还有在某些通信信道中，如单工无线发送等。

## 1. 操作系统

#### 1.1 Linux里如何查看一个想知道的进程？

**参考回答**

**查看进程运行状态的指令**：ps命令。“**ps -aux | grep PID**”，用来查看某PID进程状态

**答案解析**

```
//ps使用示例 //显示当前所有进程   ps -A   //与grep联用查找某进程   ps -aux | grep apache    //查看进程运行状态、查看内存使用情况的指令均可使用top指令。 top
```

#### 1.2 Linux里如何查看带有关键字的日志文件？

**参考回答**

1. **cat 路径/文件名 | grep 关键词**

```
# 返回test.log中包含http的所有行 cat test.log | grep "http"
```

1. **grep -i 关键词 路径/文件名** （与方法一效果相同，不同写法而已）

```
# 返回test.log中包含http的所有行(-i忽略大小写） grep -i "http" ./test.log 
```

#### 1.3 说说你对grep命令的了解？

**参考回答**

grep 命令。强大的文本搜索命令，grep(Global Regular Expression Print) 全局正则表达式搜索。

grep 的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。

**答案解析**

```
1. //参数   2. -A n --after-context显示匹配字符后n行   3. -B n --before-context显示匹配字符前n行   4. -C n --context 显示匹配字符前后n行   5. -c --count 计算符合样式的列数   6. -i 忽略大小写   7. -l 只列出文件内容符合指定的样式的文件名称   8. -f 从文件中读取关键词   9. -n 显示匹配内容的所在文件中行数   10. -R 递归查找文件夹   11.    12. //grep 的规则表达式:   13. ^       #锚定行的开始 如：'^grep'匹配所有以grep开头的行。    14. $       #锚定行的结束 如：'grep$'匹配所有以grep结尾的行。    15. .       #匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。     16. *       #匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。   17. .*      #一起用代表任意字符。     18. []      #匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。    19. [^]     #匹配一个不在指定范围内的字符，如：'[^A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。     20. \(..\)  #标记匹配字符，如'\(love\)'，love被标记为1。      21. \<      #锚定单词的开始，如:'\<grep'匹配包含以grep开头的单词的行。   22. \>      #锚定单词的结束，如'grep\>'匹配包含以grep结尾的单词的行。   23. x\{m\}  #重复字符x，m次，如：'0\{5\}'匹配包含5个o的行。    24. x\{m,\} #重复字符x,至少m次，如：'o\{5,\}'匹配至少有5个o的行。     25. x\{m,n\}#重复字符x，至少m次，不多于n次，如：'o\{5,10\}'匹配5--10个o的行。     26. \w      #匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。     27. \W      #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。     28. \b      #单词锁定符，如: '\bgrep\b'只匹配grep。    //实例： 1. //查找指定进程   2. ps -ef | grep svn   3.    4. //查找指定进程个数   5. ps -ef | grep svn -c   6.    7. //从文件中读取关键词   8. cat test1.txt | grep -f key.log   9.    10. //显示包含 ed 或者 at 字符的内容行   11. grep -E 'ed|at' test.txt  
```

#### 1.4 Linux修改主机名的命令是什么？

**参考回答**

1. 如果只需要临时更改主机名，可以使用hostname命令。

    ```
    sudo hostname <new-hostname> # 例如： sudo hostname myDebian #myDebian为修改名
    ```

2. 如果想永久改变主机名，可以使用hostnamectl命令

    ```
    sudo hostnamectl set-hostname myDebian #myDebian为修改名
    ```

#### 1.5 Linux开机自动执行命令如何实现？

**参考回答**

1. **方法 #1 - 使用 cron 任务**

    除了常用格式（分 / 时 / 日 / 月 / 周）外，cron 调度器还支持 @reboot 指令。这个指令后面的参数是脚本（启动时要执行的那个脚本）的绝对路径。

    然而，这种方法需要注意两点：

    a) cron 守护进程必须处于运行状态（通常情况下都会运行），同时

    b) 脚本或 crontab 文件必须包含需要的环境变量。

2. **方法 #2 - 使用 /etc/rc.d/rc.local**

    这个方法对于 systemd-based 发行版 Linux 同样有效。不过，使用这个方法，需要授予 /etc/rc.d/rc.local 文件执行权限：

    ```
    # chmod +x /etc/rc.d/rc.local
    ```

    然后在这个文件底部添加脚本。

#### 1.6 Linux查看内存的命令是什么？

**参考回答**

**查看内存使用情况的指令**：**free命令**。“**free -m**”，命令查看内存使用情况。

查看进程运行状态、查看内存使用情况的指令均可使用**top指令**。

**答案解析**

1. **free命令**

    Linux free命令用于**显示内存状态**。

    free指令会显示内存的使用情况，**包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等。**

    参数如下：

    ```
    -b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。 -m 以MB为单位显示内存使用情况。 -h 以合适的单位显示内存使用情况，最大为三位数，自动计算对应的单位值。单位有：         B = bytes         K = kilos         M = megas         G = gigas         T = teras -o 不显示缓冲区调节列。 -s<间隔秒数> 持续观察内存使用状况。 -t 显示内存总和列。 -V 显示版本信息。
    ```

    实例：显示内存使用情况

    ```
    # free //显示内存使用信息
    total used free shared buffers cached
    Mem: 254772 184568 70204 0 5692 89892
    -/+ buffers/cache: 88984 165788
    Swap: 524280 65116 459164
    ```

2. **top命令**

    **top**命令。**显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等**

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696413538/2215040162017B8FD7FF8E78B49D038C)

    **前五行是当前系统情况整体的统计信息区。**

    1. 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：

        00:12:54 — 当前系统时间

        up ？days, 4:49 — 系统已经运行了？天4小时49分钟（在这期间系统没有重启过）

        21users — 当前有1个用户登录系统

        load average: 0.06, 0.02, 0.00 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。

    2. 第二行，Tasks — 任务（进程），具体信息说明如下：

        系统现在共有256个进程，其中处于运行中的有1个，177个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。

    3. 第三行，cpu状态信息，具体属性说明如下：

        0.2%us — 用户空间占用CPU的百分比。

        0.2% sy — 内核空间占用CPU的百分比。

        0.0% ni — 改变过优先级的进程占用CPU的百分比

        99.5% id — 空闲CPU百分比

        0.0% wa — IO等待占用CPU的百分比

        0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比

        0.0% si — 软中断（Software Interrupts）占用CPU的百分比

    4. 第四行，内存状态，具体信息如下：

        2017552 total — 物理内存总量

        720188 used — 使用中的内存总量

        197916 free — 空闲内存总量

        1099448 cached — 缓存的总量

    5. 第五行，swap交换分区信息，具体信息说明如下：

        998396 total — 交换区总量

        989936 free — 空闲交换区总量

        8460 used — 使用的交换区总量

        1044136 cached — 缓冲的交换区总量

#### 1.7 free命令有哪些选项？

**参考回答**

Linux free命令用于**显示内存状态**。

free指令会显示内存的使用情况，包括实体内存，虚拟的交换文件内存，共享内存区段，以及系统核心使用的缓冲区等。

参数如下：

```
-b 　以Byte为单位显示内存使用情况。
-k 　以KB为单位显示内存使用情况。
-m 　以MB为单位显示内存使用情况。
-h 　以合适的单位显示内存使用情况，最大为三位数，自动计算对应的单位值。单位有：
        B = bytes
        K = kilos
        M = megas
        G = gigas
        T = teras
-o 　不显示缓冲区调节列。
-s<间隔秒数> 　持续观察内存使用状况。
-t 　显示内存总和列。
-V 　显示版本信息。
```

**答案解析**

实例：显示内存使用情况

```
# free //显示内存使用信息
total used free shared buffers cached
Mem: 254772 184568 70204 0 5692 89892
-/+ buffers/cache: 88984 165788
Swap: 524280 65116 459164
```

#### 1.8 Linux中压缩文件的命令是什么？

**参考回答**

Linux中压缩文件与解压文件的命令有：**tar命令、gz命令、bz2命令、compress命令、zip命令、unzip命令**。

**答案解析**

1. **tar 命令详解**

    Linux tar（英文全拼：tape archive ）命令用于备份文件。

    tar 是用来建立，还原备份文件的工具程序，它可以加入，解开备份文件内的文件。

    ```
    //命令格式：
    tar [-ABcdgGhiklmMoOpPrRsStuUvwWxzZ][-b <区块数目>][-C <目的目录>][-f <备份文件>][-F <Script文件>][-K <文件>][-L <媒体容量>][-N <日期时间>][-T <范本文件>][-V <卷册名称>][-X <范本文件>][-<设备编号><存储密度>][--after-date=<日期时间>][--atime-preserve][--backuup=<备份方式>][--checkpoint][--concatenate][--confirmation][--delete][--exclude=<范本样式>][--force-local][--group=<群组名称>][--help][--ignore-failed-read][--new-volume-script=<Script文件>][--newer-mtime][--no-recursion][--null][--numeric-owner][--owner=<用户名称>][--posix][--erve][--preserve-order][--preserve-permissions][--record-size=<区块数目>][--recursive-unlink][--remove-files][--rsh-command=<执行指令>][--same-owner][--suffix=<备份字尾字符串>][--totals][--use-compress-program=<执行指令>][--version][--volno-file=<编号文件>][文件或目录...]
        
    //常用参数：
    //必要参数有如下：
    -A 新增压缩文件到已存在的压缩
    -c 建立新的压缩文件
    -d 记录文件的差别
    -r 添加文件到已经压缩的文件
    -u 添加改变了和现有的文件到已经存在的压缩文件
    -x 从压缩的文件中提取文件
    -t 显示压缩文件的内容
    -z 支持gzip解压文件
    -j 支持bzip2解压文件
    -Z 支持compress解压文件
    -v 显示操作过程
    -l 文件系统边界设置
    -k 保留原有文件不覆盖
    -m 保留文件不被覆盖
    -W 确认压缩文件的正确性
        
    //实例
    //1.压缩
    tar -cf hhh.tar hhh       //打包 hhh 文件为 hhh.tar
    tar -jcf hhh.tar.bz2 hhh  //压缩打包 hhh 文件为 hhh.tar.bz2
    tar -czf hhh.tar.gz hhh   //压缩 hhh 文件为 hhh.tar.gz
    tar -tzvf test.tar.gz     //列出压缩文件内容
        
    //2.解压文件  
    tar -tzvf test.tar.gz 
    ```

2. **gz命令详解**

    Linux gzip命令用于压缩文件。

    gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多出".gz"的扩展名。

    ```
    //命令格式：
    gzip [-acdfhlLnNqrtvV][-S &lt;压缩字尾字符串&gt;][-&lt;压缩效率&gt;][--best/fast][文件...] 或 gzip [-acdfhlLnNqrtvV][-S &lt;压缩字尾字符串&gt;][-&lt;压缩效率&gt;][--best/fast][目录]
        
    //常用参数：
    -a或--ascii 　使用ASCII文字模式。
    -c或--stdout或--to-stdout 　把压缩后的文件输出到标准输出设备，不去更动原始文件。
    -d或--decompress或----uncompress 　解开压缩文件。
    -f或--force 　强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接。
    -h或--help 　在线帮助。
    -l或--list 　列出压缩文件的相关信息。
    -L或--license 　显示版本与版权信息。
    -n或--no-name 　压缩文件时，不保存原来的文件名称及时间戳记。
    -N或--name 　压缩文件时，保存原来的文件名称及时间戳记。
    -q或--quiet 　不显示警告信息。
    -r或--recursive 　递归处理，将指定目录下的所有文件及子目录一并处理。
    -S<压缩字尾字符串>或----suffix<压缩字尾字符串> 　更改压缩字尾字符串。
    -t或--test 　测试压缩文件是否正确无误。
    -v或--verbose 　显示指令执行过程。
    -V或--version 　显示版本信息。
    -<压缩效率> 　压缩效率是一个介于1－9的数值，预设值为"6"，指定愈大的数值，压缩效率就会愈高。
    --best 　此参数的效果和指定"-9"参数相同。
    --fast 　此参数的效果和指定"-1"参数相同。
        
    //实例
    //1.压缩
    gzip *            //压缩目录下的所有文件
        
    //2.解压文件  
    gzip -dv *        //解压文件，并列出详细信息   
    ```

3. **bz2命令详解**

    bzip2(选项)（参数）：用于创建和管理.bz2格式的压缩包。

    ```
    //命令格式：
    bzip2 源文件       //压缩不保留源文件
    bzip2 -k 源文件    //压缩保留源文件
    //注意 bzip2 命令不能解压目录
    
    //常用参数：
    -c 将压缩与解压缩的结果送到标准输出
    -d 执行解压缩
    -f 在压缩或解压缩时，若输出文件与现有文件名相同，预设不会覆盖现有文件；使用该选项，可覆盖文件
    -k 在压缩或解压缩后，会删除原是文件；若要保留原是文件，使用该选项
    -v 压缩或解压缩文件时，显示详细的信息
    -z 强制执行压缩
        
    //实例
    //1.压缩
    bzip2 源文件       //压缩不保留源文件
    bzip2 -k 源文件    //压缩保留源文件
        
    //2.解压文件  
    bzip2 -d 源文件   //解压缩 -k 保留压缩文件
    bunzip2  源文件   //解压缩 -k 保留压缩文件      
    ```

4. **compress命令详解**

    Linux compress命令是一个相当古老的 unix 档案压缩指令，压缩后的档案会加上一个 .Z 延伸档名以区别未压缩的档案，压缩后的档案可以以 uncompress 解压。若要将数个档案压成一个压缩档，必须先将档案 tar 起来再压缩。由于 gzip 可以产生更理想的压缩比例，一般人多已改用 gzip 为档案压缩工具。

    ```
    //命令格式：
    compress [-dfvcV] [-b maxbits] [file ...]
     
    //常用参数：    
    -c 输出结果至标准输出设备（一般指荧幕）
    -f 强迫写入档案，若目的档已经存在，则会被覆盖 (force)
    -v 将程序执行的讯息印在荧幕上 (verbose)
    -b 设定共同字串数的上限，以位元计算，可以设定的值为 9 至 16 bits 。由于值越大，能使用的共同字串就 越多，压缩比例就越大，所以一般使用预设值 16 bits (bits)
    -d 将压缩档解压缩
    -V 列出版本讯息    
        
    //实例
    //1.压缩
    compress -f source.dat   //将 source.dat 压缩成 source.dat.Z ，若 source.dat.Z 已经存在，内容则会被压缩档覆盖。    
        
    //2.解压文件  
    compress -d source.dat   //将 source.dat.Z 解压成 source.dat ，若档案已经存在，使用者按 y 以确定覆盖档案，若使用 -df 程序则会自动覆盖档案。 
    ```

5. **zip 命令详解**

    ```
    //命令格式：
    zip [-AcdDfFghjJKlLmoqrSTuvVwXyz$][-b <工作目录>][-ll][-n <字尾字符串>][-t <日期时间>][-<压缩效率>][压缩文件][文件...][-i <范本样式>][-x <范本样式>]
        
    //常用参数：
    -m 将文件压缩并加入压缩文件后，删除原始文件，即把文件移到压缩文件中。
    -o 以压缩文件内拥有最新更改时间的文件为准，将压缩文件的更改时间设成和该文件相同。
    -q 不显示指令执行过程。
    -r 递归处理，将指定目录下的所有文件和子目录一并处理。
    -x<范本样式> 压缩时排除符合条件的文件。
        
    //实例：
    //将 /home/html/ 这个目录下所有文件和文件夹打包为当前目录下的 html.zip：
    zip -q -r html.zip /home/html
        
    //如果在我们在 /home/html 目录下，可以执行以下命令：
    zip -q -r html.zip *
        
    //从压缩文件 cp.zip 中删除文件 a.c
    zip -dv cp.zip a.c
    ```

6. **unzip 命令详解**

    Linux unzip命令用于解压缩zip文件

    unzip为.zip压缩文件的解压缩程序。

    ```
    //命令格式：
    unzip [-cflptuvz][-agCjLMnoqsVX][-P <密码>][.zip文件][文件][-d <目录>][-x <文件>] 或 unzip [-Z]
        
    //常用参数：    
    -c 将解压缩的结果显示到屏幕上，并对字符做适当的转换。
    -f 更新现有的文件。
    -l 显示压缩文件内所包含的文件。
    -p 与-c参数类似，会将解压缩的结果显示到屏幕上，但不会执行任何的转换。
    -t 检查压缩文件是否正确。
    -u 与-f参数类似，但是除了更新现有的文件外，也会将压缩文件中的其他文件解压缩到目录中。
    -v 执行是时显示详细的信息。
    -z 仅显示压缩文件的备注文字。
    -a 对文本文件进行必要的字符转换。
    -b 不要对文本文件进行字符转换。
    -C 压缩文件中的文件名称区分大小写。
    -j 不处理压缩文件中原有的目录路径。
    -L 将压缩文件中的全部文件名改为小写。
    -M 将输出结果送到more程序处理。
    -n 解压缩时不要覆盖原有的文件。
    -o 不必先询问用户，unzip执行后覆盖原有文件。
    -P<密码> 使用zip的密码选项。
    -q 执行时不显示任何信息。
    -s 将文件名中的空白字符转换为底线字符。
    -V 保留VMS的文件版本信息。
    -X 解压缩时同时回存文件原来的UID/GID。
    [.zip文件] 指定.zip压缩文件。
    [文件] 指定要处理.zip压缩文件中的哪些文件。
    -d<目录> 指定文件解压缩后所要存储的目录。
    -x<文件> 指定不要处理.zip压缩文件中的哪些文件。
    -Z unzip -Z等于执行zipinfo指令。
        
    //实例
    unzip text.zip   //将压缩文件text.zip在指定目录/tmp下解压缩，如果已有相同的文件存在，要求unzip命令不覆盖原先的文件。    
    unzip -n text.zip -d /tmp  //查看压缩文件目录，但不解压。
    ```

#### 1.9 Linux查询连接数的命令是什么？

**参考回答**

1. **netstat**

2. ```
    //示例
    查看Web服务器（Nginx Apache）的并发请求数及其TCP连接状态：
    netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
    
    解释:
    返回结果示例： 
    LAST_ACK 5   (正在等待处理的请求数) 
    SYN_RECV 30 
    ESTABLISHED 1597 (正常数据传输状态) 
    FIN_WAIT1 51 
    FIN_WAIT2 504 
    TIME_WAIT 1057 (处理完毕，等待超时结束的请求数) 
     
    状态：描述 
    CLOSED：无连接是活动的或正在进行 
    LISTEN：服务器在等待进入呼叫 
    SYN_RECV：一个连接请求已经到达，等待确认 
    SYN_SENT：应用已经开始，打开一个连接 
    ESTABLISHED：正常数据传输状态 
    FIN_WAIT1：应用说它已经完成 
    FIN_WAIT2：另一边已同意释放 
    ITMED_WAIT：等待所有分组死掉 
    CLOSING：两边同时尝试关闭 
    TIME_WAIT：另一边已初始化一个释放 
    LAST_ACK：等待所有分组死掉
    ```

**答案解析**

无。

#### 1.10 Linux中top命令有哪些参数？

**参考回答**

**top**命令。显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等

**参数**：

```
-d 指定每两次屏幕信息刷新之间的时间间隔。当然用户可以使用s交互命令来改变之。 
-p 通过指定监控进程ID来仅仅监控某个进程的状态。 
-q 该选项将使top没有任何延迟的进行刷新。如果调用程序有超级用户权限，那么top将以尽可能高的优先级运行。 
-S 指定累计模式 
-s 使top命令在安全模式中运行。这将去除交互命令所带来的潜在危险。 
-i 使top不显示任何闲置或者僵死进程。 
-c 显示整个命令行而不只是显示命令名 
```

**答案解析**

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696437695/09E6123299C01A32D45AED87B0CA101F)

**前五行是当前系统情况整体的统计信息区。**

1. 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下：

    00:12:54 — 当前系统时间

    up ？days, 4:49 — 系统已经运行了？天4小时49分钟（在这期间系统没有重启过）

    21users — 当前有1个用户登录系统

    load average: 0.06, 0.02, 0.00 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。

2. 第二行，Tasks — 任务（进程），具体信息说明如下：

    系统现在共有256个进程，其中处于运行中的有1个，177个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。

3. 第三行，cpu状态信息，具体属性说明如下：

    0.2%us — 用户空间占用CPU的百分比。

    0.2% sy — 内核空间占用CPU的百分比。

    0.0% ni — 改变过优先级的进程占用CPU的百分比

    99.5% id — 空闲CPU百分比

    0.0% wa — IO等待占用CPU的百分比

    0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比

    0.0% si — 软中断（Software Interrupts）占用CPU的百分比

4. 第四行，内存状态，具体信息如下：

    2017552 total — 物理内存总量

    720188 used — 使用中的内存总量

    197916 free — 空闲内存总量

    1099448 cached — 缓存的总量

5. 第五行，swap交换分区信息，具体信息说明如下：

    998396 total — 交换区总量

    989936 free — 空闲交换区总量

    8460 used — 使用的交换区总量

    1044136 cached — 缓冲的交换区总量

#### 1.11 Linux中，如何通过端口查进程，如何通过进程查端口？

**参考回答**

1. **linux下通过进程名查看其占用端口**： （1）先查看进程pid

    ```
    ps -ef | grep 进程名
    ```

    （2）通过pid查看占用端口

    ```
    netstat -nap | grep 进程pid
    ```

2. **linux通过端口查看进程**：

    ```
    netstat -nap | grep 端口号
    ```

#### 1.12 请你说说ping命令？

**参考回答**

Linux ping命令用于检测主机。

**执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。**

**答案解析**

语法：

```
ping [-dfnqrRv][-c<完成次数>][-i<间隔秒数>][-I<网络界面>][-l<前置载入>][-p<范本样式>][-s<数据包大小>][-t<存活数值>][主机名称或IP地址]
```

参数说明：

```
-d 使用Socket的SO_DEBUG功能。
-c<完成次数> 设置完成要求回应的次数。
-f 极限检测。
-i<间隔秒数> 指定收发信息的间隔时间。
-I<网络界面> 使用指定的网络接口送出数据包。
-l<前置载入> 设置在送出要求信息之前，先行发出的数据包。
-n 只输出数值。
-p<范本样式> 设置填满数据包的范本样式。
-q 不显示指令执行过程，开头和结尾的相关信息除外。
-r 忽略普通的Routing Table，直接将数据包送到远端主机上。
-R 记录路由过程。
-s<数据包大小> 设置数据包的大小。
-t<存活数值> 设置存活数值TTL的大小。
-v 详细显示指令的执行过程。
```

实例：

```
检测是否与主机连通

# ping www.w3cschool.cc //ping主机
PING aries.m.alikunlun.com (114.80.174.110) 56(84) bytes of data.
64 bytes from 114.80.174.110: icmp_seq=1 ttl=64 time=0.025 ms
64 bytes from 114.80.174.110: icmp_seq=2 ttl=64 time=0.036 ms
64 bytes from 114.80.174.110: icmp_seq=3 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=4 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=5 ttl=64 time=0.028 ms
64 bytes from 114.80.174.110: icmp_seq=6 ttl=64 time=0.028 ms
64 bytes from 114.80.174.110: icmp_seq=7 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=8 ttl=64 time=0.034 ms
64 bytes from 114.80.174.110: icmp_seq=9 ttl=64 time=0.036 ms
64 bytes from 114.80.174.110: icmp_seq=10 ttl=64 time=0.041 ms

--- aries.m.alikunlun.com ping statistics ---
10 packets transmitted, 30 received, 0% packet loss, time 29246ms
rtt min/avg/max/mdev = 0.021/0.035/0.078/0.011 ms

//需要手动终止Ctrl+C
指定接收包的次数

# ping -c 2 www.w3cschool.cc
PING aries.m.alikunlun.com (114.80.174.120) 56(84) bytes of data.
64 bytes from 114.80.174.120: icmp_seq=1 ttl=54 time=6.18 ms
64 bytes from 114.80.174.120: icmp_seq=2 ttl=54 time=15.4 ms

--- aries.m.alikunlun.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1016ms
rtt min/avg/max/mdev = 6.185/10.824/15.464/4.640 ms

//收到两次包后，自动退出
多参数使用

# ping -i 3 -s 1024 -t 255 g.cn //ping主机
PING g.cn (203.208.37.104) 1024(1052) bytes of data.
1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=0 ttl=243 time=62.5 ms
1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=1 ttl=243 time=63.9 ms
1032 bytes from bg-in-f104.1e100.net (203.208.37.104): icmp_seq=2 ttl=243 time=61.9 ms

--- g.cn ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 6001ms
rtt min/avg/max/mdev = 61.959/62.843/63.984/0.894 ms, pipe 2
[root@linux ~]# 

//-i 3 发送周期为 3秒 -s 设置发送包的大小 -t 设置TTL值为 255
```

#### 1.13 什么是协程？

**参考回答**

**协程**：协程是微线程，在子程序内部执行，可在子程序内部中断，转而执行别的子程序，在适当的时候再返回来接着执行。

**答案解析**

1. **线程与协程的区别：**

    （1）协程执行效率极高。协程直接操作栈基本没有内核切换的开销，所以上下文的切换非常快，切换开销比线程更小。

    （2）协程不需要多线程的锁机制，因为多个协程从属于一个线程，不存在同时写变量冲突，效率比线程高。

    （3）一个线程可以有多个协程。

2. **协程的优势：**

    （1）**协程调用跟切换比线程效率高**：协程执行效率极高。协程不需要多线程的锁机制，可以不加锁的访问全局变量，所以上下文的切换非常快。

    （2）**协程占用内存少**：执行协程只需要极少的栈内存（大概是4～5KB），而默认情况下，线程栈的大小为1MB。

    （3）**切换开销更少**：协程直接操作栈基本没有内核切换的开销，所以切换开销比线程少。

#### 1.14 为什么协程比线程切换的开销小？

**参考回答**

（1）协程执行效率极高。协程直接操作栈基本没有内核切换的开销，所以上下文的**切换非常快**，切换开销比线程更小。

（2）协程不需要多线程的锁机制，因为多个协程从属于一个线程，不存在同时写变量冲突，效率比线程高。**避免了加锁解锁的开销。**

#### 1.15 线程和进程的区别？

**参考回答**

（1）一个线程从属于一个进程；一个进程可以包含多个线程。

（2）一个线程挂掉，对应的进程挂掉；一个进程挂掉，不会影响其他进程。

（3）进程是系统资源调度的最小单位；线程CPU调度的最小单位。

（4）进程系统开销显著大于线程开销；线程需要的系统资源更少。

（5）进程在执行时拥有独立的内存单元，多个线程共享进程的内存，如代码段、数据段、扩展段；但每个线程拥有自己的栈段和寄存器组。

（6）进程切换时需要刷新TLB并获取新的地址空间，然后切换硬件上下文和内核栈，线程切换时只需要切换硬件上下文和内核栈。

（7）通信方式不一样。

（8）进程适应于多核、多机分布；线程适用于多核

#### 1.16 进程切换为什么比线程更消耗资源？

**参考回答**

进程切换时需要**刷新TLB**并获取新的地址空间，然后切换硬件上下文和内核栈；线程切换时只需要切换硬件上下文和内核栈。

**答案解析**

**进程是程序的动态表现。** 一个程序进行起来后，会使用很多资源，比如使用寄存器，内存，文件等。每当切换进程时，必须要考虑保存当前进程的状态。状态包括存放在内存中的程序的代码和数据，它的栈、通用目的寄存器的内容、程序计数器、环境变量以及打开的文件描述符的集合，这个状态叫做上下文（Context）。可见，想要切换进程，保存的状态还不少。不仅如此，由于虚拟内存机制，进程切换时需要**刷新TLB**并获取新的地址空间。

线程存在于进程中，一个进程可以有一个或多个线程。**线程是运行在进程上下文中的逻辑流**，这个线程可以独立完成一项任务。同样线程有自己的上下文，包括唯一的整数线程ID， 栈、栈指针、程序计数器、通用目的寄存器和条件码。**可以理解为线程上下文是进程上下文的子集。**

**由于保存线程的上下文明显比进程的上下文小，因此系统切换线程时，必然开销更小。**

#### 1.17 介绍一下进程之间的通信。

**参考回答**

为了提高计算机系统的效率．增强计算机系统内各种硬件的并行操作能力．操作系统要求程序结构必须适应并发处理的需要．为此引入了进程的概念。而进程并行时，需要考虑进程间的通信，进程间通信主要有以下几种方式：匿名管道、命名管道、信号、消息队列、共享内存、信号量、Socket。

1. 匿名管道：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。

```
#include <stdio.h> #include <unistd.h> #include <stdlib.h> #include <string.h> #include <sys/wait.h>  int pipe_default[2];  int main() {   pid_t pid;   char buffer[32];    memset(buffer, 0, 32);   if(pipe(pipe_default) < 0)  {     printf("Failed to create pipe!\n");     return 0;  }    if(0 == (pid = fork()))  {     close(pipe_default[1]); //关闭写端     sleep(2);     if(read(pipe_default[0], buffer, 32) > 0)     {       printf("[Client] Receive data from server: %s \n", buffer);     }     close(pipe_default[0]);    }   else  {     close(pipe_default[0]);  //关闭读端     char msg[32]="== hello world ==";     if(-1 != write(pipe_default[1], msg, strlen(msg)))     {       printf("[Server] Send data to client: %s \n",msg);     }     close(pipe_default[1]);     waitpid(pid, NULL, 0);  }   return 1; }
```

1. 有名管道

    匿名管道，由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道(FIFO)。

    有名管道不同于匿名管道之处在于它提供了一个路径名与之关联，**以有名管道的文件形式存在于文件系统中**，这样，**即使与有名管道的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过有名管道相互通信**，因此，通过有名管道不相关的进程也能交换数据。值的注意的是，有名管道严格遵循**先进先出(first in first out)** ,对匿名管道及有名管道的读总是从开始处返回数据，对它们的写则把数据添加到末尾。它们不支持诸如lseek()等文件定位操作。**有名管道的名字存在于文件系统中，内容存放在内存中。**

2. 信号

- 信号是Linux系统中用于进程间互相通信或者操作的一种机制，信号可以在任何时候发给某一进程，而无需知道该进程的状态。
- 如果该进程当前并未处于执行状态，则该信号就有内核保存起来，知道该进程回复执行并传递给它为止。
- 如果一个信号被进程设置为阻塞，则该信号的传递被延迟，直到其阻塞被取消是才被传递给进程。

以下列出几个常用的信号：

| 信号    | 描述                                                         |
| :------ | :----------------------------------------------------------- |
| SIGHUP  | 当用户退出终端时，由该终端开启的所有进程都退接收到这个信号，默认动作为终止进程。 |
| SIGINT  | 程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl+C)时发出，用于通知前台进程组终止进程。 |
| SIGQUIT | 和SIGINT类似, 但由QUIT字符(通常是Ctrl+\)来控制. 进程在因收到SIGQUIT退出时会产生core文件, 在这个意义上类似于一个程序错误信号。 |
| SIGKILL | 用来立即结束程序的运行. **本信号不能被阻塞、处理和忽略**。   |
| SIGTERM | 程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出。 |
| SIGSTOP | 停止(stopped)进程的执行. 注意它和terminate以及interrupt的区别:该进程还未结束, 只是暂停执行. **本信号不能被阻塞, 处理或忽略**. |

代码示例：

下面的代码收到程序退出信号后会执行用户定义的信号处理函数来替代系统默认的处理程序。

```
#include<stdlib.h> #include<stdio.h> #include<signal.h> #include<sys/types.h> #include<unistd.h>  void sig_handle(int sig) {     printf("received signal: %d, quit.\n", sig);     exit(0); }  int main () {     signal(SIGINT, sig_handle);     signal(SIGKILL, sig_handle);     signal(SIGSEGV, sig_handle);     signal(SIGTERM, sig_handle);       int i = 0;      while (1) {          printf("%d\n", ++i);          sleep(2);      }       printf("main quit.");       return 0; }
```

运行结果：

```
1 2 received signal: 15, quit.
```

1. 消息队列

- 消息队列是**存放在内核中的消息链表**，每个消息队列由消息队列标识符表示。
- 与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列**存放在内核中**，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。
- 另外与管道不同的是，消息队列在某个进程往一个队列写入消息之前，并**不需要另外某个进程在该队列上等待消息的到达**。

> **消息队列特点总结：**
>
> （1）消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识.
>
> （2）消息队列允许一个或多个进程向它写入与读取消息.
>
> （3）管道和消息队列的通信数据都是先进先出的原则。
>
> （4）消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。
>
> （5）消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。
>
> （6）目前主要有两种类型的消息队列：POSIX消息队列以及System V消息队列，系统V消息队列目前被大量使用。系统V消息队列是随内核持续的，只有在内核重起或者人工删除时，该消息队列才会被删除。

1. 共享内存

    进程间本身的内存是相互隔离的，而共享内存机制相当于给两个进程开辟了一块二者均可访问的内存空间，这时，两个进程便可以共享一些数据了。但是，多进程同时占用资源会带来一些意料之外的情况，这时，我们往往会采用上述的信号量来控制多个进程对共享内存空间的访问。

    ```
    #include <iostream> #include <stdlib.h> #include <string.h> #include <sys/shm.h> #include <sys/ipc.h> #include <unistd.h>  using namespace std; int main() {   char *shmaddr;   char *shmaddread;   char str[]="Hello, I am a processing. \n";   int shmid;    key_t key = ftok(".",1);   pid_t pid1 = fork();   if(pid1 == -1){      cout << "Fork error. " << endl;      exit(1);  }   else if(pid1 == 0){      //子进程      shmid = shmget(key,1024,IPC_CREAT | 0600);      shmaddr = (char*)shmat(shmid, NULL, 0);                 strcpy(shmaddr, str);      cout << "[Writer] write: " << shmaddr << endl;      shmdt(shmaddr);  }   else  {      //父进程      pid_t pid2 = fork();      if(pid2 == -1){        cout << "Fork error. " << endl;        exit(1);     }     else if(pid2 == 0){        //子进程        sleep(2);        shmid = shmget(key,1024,IPC_CREAT | 0600);        shmaddread = (char*)shmat(shmid, NULL, 0);                cout << "[Reader] read: " << shmaddread << endl;        shmdt(shmaddread);     }  }   sleep(3);   return 0; }
    ```

2. 信号量

    信号量主要用来解决进程和线程间并发执行时的同步问题，进程同步是并发进程为了完成共同任务采用某个条件来协调他们的活动，这是进程之间发生的一种直接制约关系。

    对信号量的操作分为P操作和V操作，P操作是将信号量的值减一，V操作是将信号量的值加一。当信号量的值小于等于0之后，再进行P操作时，当前进程或线程会被阻塞，直到另一个进程或线程执行了V操作将信号量的值增加到大于0之时。锁也是用的这种原理实现的。

    信号量我们需要定义信号量的数量，设定初始值，以及决定何时进行PV操作。

    ```
     #include <unistd.h>    #include <sys/types.h>    #include <sys/stat.h>    #include <fcntl.h>    #include <stdlib.h>    #include <stdio.h>    #include <string.h>    #include <sys/sem.h>  #define KEY (key_t)15030110070  #define N 20   static void p(int semid ,int semNum);    static void v(int semid ,int semNum);   union semun {        int val;        struct semid_ds *buf;        ushort *array;    };     int main(int argc ,char* argv[])  {    int i;    int semid;     semid = semget(KEY,3,IPC_CREAT|0660);      union semun arg[3];      arg[0].val = 1;                     //mutex  [0]  对缓冲区进行操作的互斥信号量    arg[1].val = N;               //empty  [1]  缓冲区空位个数n    arg[2].val = 0;                     //full [2]  产品个数       for(i=0;i<3;i++)            semctl(semid,i,SETVAL,arg[i]);       pid_t p1,p2;    if((p1=fork()) == 0)    {      //子进程1，消费者      while(1)      {        printf("消费者 1 等待中...\n");        sleep(2);        int product = rand() % 2 + 1;        for(int i = 0; i < product; i++)        {          p(semid ,2);    //消费          p(semid ,0);    //加锁          printf(" [消费者 1] 消费产品 1. 剩余：  %d\n", semctl(semid, 2, GETVAL, NULL));          v(semid ,0);    //开锁          v(semid ,1);    //释放空位        }        sleep(2);      }      }    else    {      if((p2=fork()) == 0)      {        //子进程2，消费者        while(1)        {          printf("消费者 2 等待中...\n");          sleep(2);          int product = rand() % 2 + 1;          for(int i = 0; i < product; i++)          {            p(semid ,2);    //消费            p(semid ,0);    //加锁            printf(" [消费者 2] 消费产品 1. 剩余：  %d\n", semctl(semid, 2, GETVAL, NULL));            v(semid ,0);    //开锁            v(semid ,1);    //释放空位          }          sleep(2);        }      }      else      {        //父进程，生产者        while(1)        {          printf("生产者开始生产...\n");          int product = rand() % 5 + 1;           for(int i = 0; i < product; i++)          {            p(semid ,1);    //占用空位            p(semid ,0);    //加锁            printf(" [生产者] 生产产品 1. 剩余：  %d\n", semctl(semid, 2, GETVAL, NULL) + 1);              v(semid ,0);    //开锁            v(semid, 2);    //生产          }          sleep(2);        }      }    }    return 0;  }   /* p操作 */    void p(int semid ,int semNum)  {        struct sembuf sb;        sb.sem_num = semNum;        sb.sem_op = -1;        sb.sem_flg = SEM_UNDO;        semop(semid, &sb, 1);    }     /* v操作 */    void v(int semid ,int semNum)  {        struct sembuf sb;        sb.sem_num = semNum;        sb.sem_op = 1;        sb.sem_flg = SEM_UNDO;        semop(semid, &sb, 1);    }  
    ```

3. socket

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696682573/2B8A3E4CC50A4115DBF20B80E7C3E9B0)

    套接字可以看做是：不同主机之间的进程进行双向通信的端点。（套接字 = IP地址 + 端口号）

#### 1.18 介绍一下信号量。

**参考回答**

1. 在多进程环境下，为了防止多个进程同时访问一个公共资源而出现问题，需要一种方法来协调各个进程，保证它们能够合理地使用公共资源。信号量就是这样一种机制。

    信号量的数据类型为结构sem_t，它本质上是一个长整型的数。函数sem_init（）用来初始化一个信号量。它的原型为：

    extern int sem_init _*P ((sem_t \*_*sem, int _*pshared, unsigned int _*value));

    sem为指向信号量结构的一个指针；pshared不为０时此信号量在进程间共享，否则只能为当前进程的所有线程共享；value给出了信号量的初始值。

    （1）函数sem_post( sem_t *sem )用来增加信号量的值。当有线程阻塞在这个信号量上时，调用这个函数会使其中的一个线程不在阻塞，选择机制同样是由线程的调度策略决定的。

    （2）函数sem_wait( sem_t *sem )被用来阻塞当前线程直到信号量sem的值大于0，解除阻塞后将sem的值减一，表明公共资源经使用后减少。函数sem_trywait ( sem_t *sem )是函数sem_wait（）的非阻塞版本，它直接将信号量sem的值减一。

    （3）函数sem_timedwait(sem_t *sem, const struct timespec *abs_timeout) 与 sem_wait() 类似，只不过 abs_timeout 指定一个阻塞的时间上限，如果调用因不能立即执行递减而要阻塞。

    （4）函数sem_destroy(sem_t *sem)用来释放信号量sem。

2. 使用示例代码如下

```
//g++ semtest.cpp -o test -lpthread #include <stdio.h> #include <semaphore.h> #include <pthread.h> #include <unistd.h> #include <sys/time.h> sem_t sem;  /*function:获取当前时间，精确到毫秒• * */ int64_t getTimeMsec() {     struct  timeval    tv;     gettimeofday(&tv, NULL);     return tv.tv_sec * 1000 + tv.tv_usec / 1000; }  void* func_sem_wait(void* arg) {     printf("set wait\n");     sem_wait(&sem);     printf("sem wait success\n");     int *running = (int*)arg;     printf("func_sem_wait running\n");     printf("%d\n", *running); }  void* func_sem_timedwait(void* arg) {     timespec timewait;     timewait.tv_sec = getTimeMsec() / 1000 + 2;     timewait.tv_nsec = 0;     printf("sem_timedwait\n");     int ret = sem_timedwait(&sem, &timewait);     printf("sem_timedwait,ret=%d\n", ret);     printf("func_sem_timedwait running\n"); }  void* func_sem_post(void* arg) {     printf("func_sem_post running\n");     printf("sem post\n");     int *a = (int*)arg;     *a = 6;     sem_post(&sem);     sem_post(&sem); }  int main() {     sem_init(&sem, 0, 0);     pthread_t thread[3];     int a = 5;      pthread_create(&(thread[0]), NULL, func_sem_wait, &a);     printf("thread func_sem_wait\n");      pthread_create(&(thread[2]), NULL, func_sem_timedwait, &a);     printf("thread func_sem_timedwait\n");      sleep(4);      pthread_create(&(thread[1]), NULL, func_sem_post, &a);     printf("thread func_sem_post\n");      pthread_join(thread[0], NULL);     pthread_join(thread[1], NULL);     pthread_join(thread[2], NULL);     sem_destroy(&sem); }
```

#### 1.19 说说僵尸进程和孤儿进程。

**参考回答**

1. 我们知道在unix/linux中，正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程 到底什么时候结束。 当一个 进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。
2. 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。
3. 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵尸进程。

#### 1.20 请介绍进程之间的通信方式。

**参考回答**

进程间通信主要有以下几种方式

1. 管道pipe：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。

    ```
    #include <stdio.h> #include <unistd.h> #include <stdlib.h> #include <string.h> #include <sys/wait.h>  int pipe_default[2];  int main() { pid_t pid; char buffer[32];  memset(buffer, 0, 32); if(pipe(pipe_default) < 0) {   printf("Failed to create pipe!\n");   return 0; }  if(0 == (pid = fork())) {   close(pipe_default[1]); //关闭写端   sleep(2);   if(read(pipe_default[0], buffer, 32) > 0)  {     printf("[Client] Receive data from server: %s \n", buffer);  }   close(pipe_default[0]);  } else {   close(pipe_default[0]);  //关闭读端   char msg[32]="== hello world ==";   if(-1 != write(pipe_default[1], msg, strlen(msg)))  {     printf("[Server] Send data to client: %s \n",msg);  }   close(pipe_default[1]);   waitpid(pid, NULL, 0); } return 1; }
    ```

2. 命名管道FIFO：有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。

    写管道：

    ```
    #include <stdio.h> #include <errno.h> #include <unistd.h> #include <sys/types.h> #include <string.h> #include <stdlib.h> #include <fcntl.h> #include <sys/stat.h>  int main() {  int nFd = 0;  int nWrLen = 0, nReadLen = 0;;  char szBuff[BUFSIZ] = {0};   /* 打开当前目录下的管道文件 */  nFd = open("pipe", O_RDWR);  if (-1 == nFd)  {   perror("Open fifo failed\n");   return 1;  }   while (1)  {   /* 从终端读取数据 */   memset(szBuff,0,BUFSIZ);   nReadLen = read(STDIN_FILENO,szBuff,BUFSIZ);    if(nReadLen > 0)   {    /* 往管道写入数据 */    nWrLen = write(nFd, szBuff, strlen(szBuff)+1);    if (nWrLen > 0)    {     printf("write data successful: %s \n", szBuff);    }    else    {     perror("write failed:");    }   }  } }
    ```

读管道：

```
#include <stdio.h>
#include <errno.h>
#include <unistd.h>
#include <sys/types.h>
#include <string.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/stat.h>

int main()
{
    int nFd = 0;
    int  nReadLen = 0;;
    char szBuff[BUFSIZ] = {0};

    /* 打开当前目录下的管道文件 */
    nFd = open("pipe", O_RDWR);
    if (-1 == nFd)
    {
        perror("Open fifo failed\n");
        return 1;
    }

    while (1)
    {
        /* 从管道读取数据 */
        memset(szBuff,0,BUFSIZ);
        nReadLen = read(nFd,szBuff,BUFSIZ);
        if(nReadLen > 0)
        {
            printf("read pipe data: %s\n", szBuff);
        }   

    }
}
```

1. 消息队列MessageQueue：消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。

    示例：使用消息队列进行进程间通信

    接收信息的程序源文件为msgreceive.c的源代码为：

    ```
    #include <unistd.h>
    #include <stdlib.h>
    #include <stdio.h>
    #include <string.h>
    #include <errno.h>
    #include <sys/msg.h>
    
    struct msg_st
    {
        long int msg_type;
        char text[BUFSIZ];
    };
    
    int main()
    {
        int running = 1;
        int msgid = -1;
        struct msg_st data;
        long int msgtype = 0; //注意1
    
        //建立消息队列
        msgid = msgget((key_t)1234, 0666 | IPC_CREAT);
        if(msgid == -1)
        {
            fprintf(stderr, "msgget failed with error: %d\n", errno);
            exit(EXIT_FAILURE);
        }
        //从队列中获取消息，直到遇到end消息为止
        while(running)
        {
            if(msgrcv(msgid, (void*)&data, BUFSIZ, msgtype, 0) == -1)
            {
                fprintf(stderr, "msgrcv failed with errno: %d\n", errno);
                exit(EXIT_FAILURE);
            }
            printf("You wrote: %s\n",data.text);
            //遇到end结束
            if(strncmp(data.text, "end", 3) == 0)
                running = 0;
        }
        //删除消息队列
        if(msgctl(msgid, IPC_RMID, 0) == -1)
        {
            fprintf(stderr, "msgctl(IPC_RMID) failed\n");
            exit(EXIT_FAILURE);
        }
        exit(EXIT_SUCCESS);
    }
    ```

    发送信息的程序的源文件msgsend.c的源代码为：

    ```
    #include <unistd.h>
    #include <stdlib.h>
    #include <stdio.h>
    #include <string.h>
    #include <sys/msg.h>
    #include <errno.h>
    
    #define MAX_TEXT 512
    struct msg_st
    {
        long int msg_type;
        char text[MAX_TEXT];
    };
    
    int main()
    {
        int running = 1;
        struct msg_st data;
        char buffer[BUFSIZ];
        int msgid = -1;
    
        //建立消息队列
        msgid = msgget((key_t)1234, 0666 | IPC_CREAT);
        if(msgid == -1)
        {
            fprintf(stderr, "msgget failed with error: %d\n", errno);
            exit(EXIT_FAILURE);
        }
    
        //向消息队列中写消息，直到写入end
        while(running)
        {
            //输入数据
            printf("Enter some text: ");
            fgets(buffer, BUFSIZ, stdin);
            data.msg_type = 1;    //注意2
            strcpy(data.text, buffer);
            //向队列发送数据
            if(msgsnd(msgid, (void*)&data, MAX_TEXT, 0) == -1)
            {
                fprintf(stderr, "msgsnd failed\n");
                exit(EXIT_FAILURE);
            }
            //输入end结束输入
            if(strncmp(buffer, "end", 3) == 0)
                running = 0;
            sleep(1);
        }
        exit(EXIT_SUCCESS);
    }
    ```

    运行结果如下：

    ```
    biao@ubuntu:~/test/msgRecvSend$
    biao@ubuntu:~/test/msgRecvSend$ ls
    msgreceive.c  msgsend.c  recv  send
    biao@ubuntu:~/test/msgRecvSend$ ./recv &
    [1] 8753
    biao@ubuntu:~/test/msgRecvSend$ ./send
    Enter some text: helloworld
    You wrote: helloworld
    
    Enter some text: Caibiao Lee
    You wrote: Caibiao Lee
    
    Enter some text: end
    You wrote: end
    
    [1]+  Done                    ./recv
    biao@ubuntu:~/test/msgRecvSend$
    ```

2. 共享存储SharedMemory：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。

    ```
    /* Linux 6.cpp */
    #include <iostream>
    #include <stdlib.h>
    #include <string.h>
    #include <sys/shm.h>
    #include <sys/ipc.h>
    #include <unistd.h>
    
    using namespace std;
    int main()
    {
      char *shmaddr;
      char *shmaddread;
      char str[]="Hello, I am a processing. \n";
      int shmid;
    
      key_t key = ftok(".",1);
      pid_t pid1 = fork();
      if(pid1 == -1){
        cout << "Fork error. " << endl;
        exit(1);
      }
      else if(pid1 == 0){
        //子进程
        shmid = shmget(key,1024,IPC_CREAT | 0600);
        shmaddr = (char*)shmat(shmid, NULL, 0);
                   strcpy(shmaddr, str);
        cout << "[Writer] write: " << shmaddr << endl;
        shmdt(shmaddr);
      }
      else
      {
        //父进程
        pid_t pid2 = fork();
        if(pid2 == -1){
          cout << "Fork error. " << endl;
          exit(1);
        }
        else if(pid2 == 0){
          //子进程
          sleep(2);
          shmid = shmget(key,1024,IPC_CREAT | 0600);
          shmaddread = (char*)shmat(shmid, NULL, 0);        
          cout << "[Reader] read: " << shmaddread << endl;
          shmdt(shmaddread);
        }
      }
      sleep(3);
      return 0;
    }
    ```

3. 信号量Semaphore：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 使用示例代码如下

    ```
    //g++ semtest.cpp -o test -lpthread
     #include <stdio.h>
     #include <semaphore.h>
     #include <pthread.h>
     #include <unistd.h>
     #include <sys/time.h>
     sem_t sem;
    
     /*function:获取当前时间，精确到毫秒  * */
     int64_t getTimeMsec()
     {
         struct  timeval    tv;
         gettimeofday(&tv, NULL);
         return tv.tv_sec * 1000 + tv.tv_usec / 1000;
     }
    
     void* func_sem_wait(void* arg)
     {
         printf("set wait\n");
         sem_wait(&sem);
         printf("sem wait success\n");
         int *running = (int*)arg;
         printf("func_sem_wait running\n");
         printf("%d\n", *running);
     }
    
    void* func_sem_timedwait(void* arg)
     {
         timespec timewait;
         timewait.tv_sec = getTimeMsec() / 1000 + 2;
         timewait.tv_nsec = 0;
         printf("sem_timedwait\n");
         int ret = sem_timedwait(&sem, &timewait);
         printf("sem_timedwait,ret=%d\n", ret);
         printf("func_sem_timedwait running\n");
     }
    
    void* func_sem_post(void* arg)
     {
         printf("func_sem_post running\n");
         printf("sem post\n");
         int *a = (int*)arg;
         *a = 6;
         sem_post(&sem);
         sem_post(&sem);
     }
    
    int main()
     {
         sem_init(&sem, 0, 0);
         pthread_t thread[3];
         int a = 5;
    
        pthread_create(&(thread[0]), NULL, func_sem_wait, &a);
         printf("thread func_sem_wait\n");
    
        pthread_create(&(thread[2]), NULL, func_sem_timedwait, &a);
         printf("thread func_sem_timedwait\n");
    
        sleep(4);
    
        pthread_create(&(thread[1]), NULL, func_sem_post, &a);
         printf("thread func_sem_post\n");
    
        pthread_join(thread[0], NULL);
         pthread_join(thread[1], NULL);
         pthread_join(thread[2], NULL);
         sem_destroy(&sem);
     }
    ```

4. 套接字Socket：套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。

5. 信号 ( sinal ) ： 信号是进程间通信机制中唯一的异步通信机制，可以看作是异步通知，通知接收信号的进程有哪些事情发生了。也可以简单理解为信号是某种形式上的软中断。

    一般情况下，信号的来源可分为以下三种：

- 硬件方式：除数为零、无效的存储访问等硬件异常产生信号。这些事件通常由硬件(如:CPU)检测到，并将其通知给Linux操作系统内核，然后内核生成相应的信号，并把信号发送给该事件发生时正在进行的程序。

- 软件方式：用户在终端下调用kill命令向进程发送任务信号、进程调用kill或sigqueue函数发送信号、当检测到某种软件条件已经具备时发出信号，如由alarm或settimer设置的定时器超时时将生成SIGALRM信号等多种情景均可产生信号。

- 键盘输入：当用户在终端上按下某键时，将产生信号。如按下组合键Ctrl+C将产生一个SIGINT信号，Ctrl+\产生一个SIGQUIT信号等。

    以下列出几个常用的信号：

    | 信号    | 描述                                                         |
    | :------ | :----------------------------------------------------------- |
    | SIGHUP  | 当用户退出终端时，由该终端开启的所有进程都退接收到这个信号，默认动作为终止进程。 |
    | SIGINT  | 程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl+C)时发出，用于通知前台进程组终止进程。 |
    | SIGQUIT | 和SIGINT类似, 但由QUIT字符(通常是Ctrl+\)来控制. 进程在因收到SIGQUIT退出时会产生core文件, 在这个意义上类似于一个程序错误信号。 |
    | SIGKILL | 用来立即结束程序的运行. **本信号不能被阻塞、处理和忽略**。   |
    | SIGTERM | 程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出。 |
    | SIGSTOP | 停止(stopped)进程的执行. 注意它和terminate以及interrupt的区别:该进程还未结束, 只是暂停执行. **本信号不能被阻塞, 处理或忽略**. |

代码示例：

下面的代码收到程序退出信号后会执行用户定义的信号处理函数来替代系统默认的处理程序。

```
#include<stdlib.h>
#include<stdio.h>
#include<signal.h>
#include<sys/types.h>
#include<unistd.h>

void sig_handle(int sig) {
    printf("received signal: %d, quit.\n", sig);
    exit(0);
}

int main () {
    signal(SIGINT, sig_handle);
    signal(SIGKILL, sig_handle);
    signal(SIGSEGV, sig_handle);
    signal(SIGTERM, sig_handle);

    int i = 0;
    while (1) {
        printf("%d\n", ++i);
        sleep(2);
    }

    printf("main quit.");

    return 0;
}
复制代码
```

运行结果：

```
1
2
received signal: 15, quit.
```

#### 1.21 请介绍线程之间的通信方式。

**参考回答**

1. 锁机制：包括互斥锁、条件变量、读写锁互斥锁提供了以排他方式防止数据结构被并发修改的方法。读写锁允许多个线程同时读共享数据，而对写操作是互斥的。条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。
2. 信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量
3. 信号机制(Signal)：类似进程间的信号处理线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。

#### 1.22 说一说进程的状态。

**参考回答**

1. 进程的3种基本状态：**运行、就绪和阻塞。**

（1）就绪：当一个进程获得了除处理机以外的一切所需资源，一旦得到处理机即可运行，则称此进程处于就绪状态。就绪进程可以按多个优先级来划分队列。例如，当一个进程由于时间片用完而进入就绪状态时，排入低优先级队列；当进程由I／O操作完成而进入就绪状态时，排入高优先级队列。

（2）运行：当一个进程在处理机上运行时，则称该进程处于运行状态。处于此状态的进程的数目小于等于处理器的数目，对于单处理机系统，处于运行状态的进程只有一个。在没有其他进程可以执行时（如所有进程都在阻塞状态），通常会自动执行系统的空闲进程。

（3）阻塞：也称为等待或睡眠状态，一个进程正在等待某一事件发生（例如请求I/O而等待I/O完成等）而暂时停止运行，这时即使把处理机分配给进程也无法运行，故称该进程处于阻塞状态。

其转移图如下：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696712432/4963A1412AEAE752861304566EE4026B)

1. 进程的五种状态

    **创建状态**：进程在创建时需要申请一个空白PCB，向其中填写控制和管理进程的信息，完成资源分配。如果创建工作无法完成，比如资源无法满足，就无法被调度运行，把此时进程所处状态称为创建状态

    **就绪状态**：进程已经准备好，已分配到所需资源，只要分配到CPU就能够立即运行

    **执行状态**：进程处于就绪状态被调度后，进程进入执行状态

    **阻塞状态**：正在执行的进程由于某些事件（I/O请求，申请缓存区失败）而暂时无法运行，进程受到阻塞。在满足请求时进入就绪状态等待系统调用

    **终止状态**：进程结束，或出现错误，或被系统终止，进入终止状态。无法再执行

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696735496/D145C434EA8E3E18EEFFB171F559D278)

#### 1.23 CPU调度的最小单位是什么？线程需要CPU调度吗？

**参考回答**

1. 进程是CPU分配资源的最小单位，线程是CPU调度的最小单位。
2. 线程是比进程更小的能独立运行的基本单位，需要通过CPU调度来切换上下文，达到并发的目的。

#### 1.24 进程之间共享内存的通信方式有什么好处？

**参考回答**

采用共享内存通信的一个显而易见的好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据：一次从输入文件到共享内存区，另一次从共享内存区到输出文件。

实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。

#### 1.25 如何杀死一个进程？

**参考回答**

1. 杀死父进程并不会同时杀死子进程：每个进程都有一个父进程。可以使用 pstree 或 ps 工具来观察这一点。

    ```
    # 启动两个虚拟进程
    $ sleep 100 &
    $ sleep 101 &
    
    $ pstree -p
    init(1)-+
            |-bash(29051)-+-pstree(29251)
                          |-sleep(28919)
                          `-sleep(28964)
    
    $ ps j -A
     PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
        0     1     1     1 ?           -1 Ss       0   0:03 /sbin/init
    29051  1470  1470 29051 pts/2     2386 SN    1000   0:00 sleep 100
    29051  1538  1538 29051 pts/2     2386 SN    1000   0:00 sleep 101
    29051  2386  2386 29051 pts/2     2386 R+    1000   0:00 ps j -A
        1 29051 29051 29051 pts/2     2386 Ss    1000   0:00 -bash
    ```

    调用 ps 命令可以显示 PID（进程 ID） 和 PPID（父进程 ID）。

    杀死父进程后，子进程将会成为孤儿进程，而 init 进程将重新成为它的父进程。

2. 杀死进程组或会话中的所有进程

    ```
    $ kill -SIGTERM -- -19701
    ```

    这里用一个负数 -19701 向进程组发送信号。如果传递的是一个正数，这个数将被视为进程 ID 用于终止进程。如果传递的是一个负数，它被视为 PGID，用于终止整个进程组。负数来自系统调用的直接定义。

    杀死会话中的所有进程与之完全不同。即使是具有会话 ID 的系统，例如 Linux，也没有提供系统调用来终止会话中的所有进程。需要遍历 /proc 输出的进程树，收集所有的 SID，然后一一终止进程。

    Pgrep 实现了遍历、收集并通过会话 ID 杀死进程的算法。可以使用以下命令：

    ```
    pkill -s <SID>
    ```

#### 1.26 说一说kill的原理。

**参考回答**

- kill 命令的执行原理是这样的，kill 命令会向操作系统内核发送一个信号（多是终止信号）和目标进程的 PID，然后系统内核根据收到的信号类型，对指定进程进行相应的操作。kill 命令的基本格式如下：

    [root\@localhost ~]# kill [信号] PID

- kill 命令是按照 PID 来确定进程的，所以 kill 命令只能识别 PID，而不能识别进程名。

- kill 命令只是“发送”一个信号，因此，只有当信号被程序成功“捕获”，系统才会执行 kill 命令指定的操作；反之，如果信号被“封锁”或者“忽略”，则 kill 命令将会失效。

#### 1.27 介绍下你知道的锁。

**参考回答**

1. 悲观锁

悲观锁并不是某一个锁，是一个锁类型，无论是否并发竞争资源，都会锁住资源，并等待资源释放下一个线程才能获取到锁。 这明显很悲观，所以就叫悲观锁。这明显可以归纳为一种策略，只要符合这种策略的锁的具体实现，都是悲观锁的范畴。

1. 乐观锁

与悲观锁相对的，乐观锁也是一个锁类型。当线程开始竞争资源时，不是立马给资源上锁，而是进行一些前后值比对，以此来操作资源。例如常见的CAS操作，就是典型的乐观锁。示例如下

```
int cas(long *addr, long old, long new) {
    /* 原子执行 */
    if(*addr != old)
        return 0;
    *addr = new;
    return 1;
}
```

1. 自旋锁

自旋锁是一种基础的同步原语，用于保障对共享数据的互斥访问。与互斥锁的相比，在获取锁失败的时候不会使得线程阻塞而是一直自旋尝试获取锁。当线程等待自旋锁的时候，CPU不能做其他事情，而是一直处于轮询忙等的状态。

自旋锁主要适用于被持有时间短，线程不希望在重新调度上花过多时间的情况。实际上许多其他类型的锁在底层使用了自旋锁实现，例如多数互斥锁在试图获取锁的时候会先自旋一小段时间，然后才会休眠。如果在持锁时间很长的场景下使用自旋锁，则会导致CPU在这个线程的时间片用尽之前一直消耗在无意义的忙等上，造成计算资源的浪费。

```
// 用户空间用 atomic_flag 实现自旋互斥
#include <thread>
#include <vector>
#include <iostream>
#include <atomic>

std::atomic_flag lock = ATOMIC_FLAG_INIT;

void f(int n)
{
    for (int cnt = 0; cnt < 100; ++cnt) {
        while (lock.test_and_set(std::memory_order_acquire))  // 获得锁
             ; // 自旋
        std::cout << "Output from thread " << n << '\n';
        lock.clear(std::memory_order_release);               // 释放锁
    }
}

int main()
{
    std::vector<std::thread> v;
    for (int n = 0; n < 10; ++n) {
        v.emplace_back(f, n);
    }
    for (auto& t : v) {
        t.join();
    }
}
```

1. 公平锁

多个线程竞争同一把锁，如果依照先来先得的原则，那么就是一把公平锁。

1. 非公平锁

多个线程竞争锁资源，抢占锁的所有权。

1. 共享锁

多个线程可以共享这个锁的拥有权。一般用于数据的读操作，防止数据被写修改。共享锁的代码示例如下：

```
    #include <shared_mutex>
    #include <mutex>
    #include <iostream>
    #include <thread>
    #include <chrono>

    std::shared_mutex test_lock;

    std::mutex cout_lock;

    int arr[3] = {11, 22, 33};

    void unique_lock_demo(int id)
    {
        std::unique_lock lock{test_lock};

        for(int i =0; i < 3; i++)
        {
                arr[i] = i + 100 * id;
        }

        for(int i = 0; i < 3; i++)
        {
            std::unique_lock pl(cout_lock);
            std::cout << "In unique: " << id << ": " << arr[i] << std::endl;
            pl.unlock();
            std::this_thread::sleep_for(std::chrono::seconds(1));
        }
    }


    void shared_lock_demo(int id)
    {
        std::shared_lock lock{test_lock};

        for(int i = 0; i < 3; i++)
        {
            std::unique_lock pl(cout_lock);
            std::cout << "In shared " << id << ": " << arr[i] << std::endl;
            pl.unlock();
            std::this_thread::sleep_for(std::chrono::seconds(1));
        }
    }

    int main()
    {

       std::thread t3(unique_lock_demo,3);
       std::thread t4(unique_lock_demo,4);
       std::thread t1(shared_lock_demo,1);
       std::thread t2(shared_lock_demo,2);

       t1.join();
       t2.join();
       t3.join();
       t4.join();
       return 0;
    }
```

输出为：

```
    In unique: 3: 300
    In unique: 3: 301
    In unique: 3: 302
    In shared 1: 300
    In shared 2: 300
    In shared 1: 301
    In shared 2: 301
    In shared 1: 302
    In shared 2: 302
    In unique: 4: 400
    In unique: 4: 401
    In unique: 4: 402
```

从这个输出可以看出：

- 如果一个线程已经获取了*共享锁*，则其他任何线程都无法获取*互斥锁*，但是可以获取*共享锁*。
- 从这个输出可以看出，验证了如果一个线程已经获取了*互斥锁*，则其他线程都无法获取该锁。

1. 死锁

死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。

```
mutex;   //代表一个全局互斥对象
void  A()
{
    mutex.lock();
    //这里操作共享数据
    B();  //这里调用B方法
    mutex.unlock();
    return;
}
void  B()
{
    mutex.lock();
    //这里操作共享数据
    mutex.unlock();
    return;
}
```

#### 1.28 什么情况下会产生死锁？

**参考回答**

如果在计算机系统中同时具备下面四个必要条件时，那么会发生死锁。换句话说，只要下面四个条件有一个不具备，系统就不会出现死锁。

1. 互斥条件。即某个资源在一段时间内只能由一个进程占有，不能同时被两个或两个以上的进程占有。这种独占资源如CD-ROM驱动器，打印机等等，必须在占有该资源的进程主动释放它之后，其它进程才能占有该资源。这是由资源本身的属性所决定的。如独木桥就是一种独占资源，两方的人不能同时过桥。

    ```
    #include <list>
    #include <mutex>
    #include <algorithm>
    
    std::list<int> some_list;    // 1
    std::mutex some_mutex;    // 2
    
    void add_to_list(int new_value)
    {
      std::lock_guard<std::mutex> guard(some_mutex);    // 3
      some_list.push_back(new_value);
    }
    
    bool list_contains(int value_to_find)
    {
      std::lock_guard<std::mutex> guard(some_mutex);    // 4
      return std::find(some_list.begin(),some_list.end(),value_to_find) != some_list.end();
    }
    ```

    代码中有一个全局变量①，这个全局变量被一个全局的互斥量保护②。add_to_list()③和list_contains()④函数中使用std::lock_guard<std::mutex>，使得这两个函数中对数据的访问是互斥的：list_contains()不可能看到正在被add_to_list()修改的列表。

1. 不剥夺条件。进程所获得的资源在未使用完毕之前，资源申请者不能强行地从资源占有者手中夺取资源，而只能由该资源的占有者进程自行释放。如过独木桥的人不能强迫对方后退，也不能非法地将对方推下桥，必须是桥上的人自己过桥后空出桥面（即主动释放占有资源），对方的人才能过桥。

    ```
    
    ```

2. 请求和保持条件。进程至少已经占有一个资源，但又申请新的资源；由于该资源已被另外进程占有，此时该进程阻塞；但是，它在等待新资源之时，仍继续占用已占有的资源。还以过独木桥为例，甲乙两人在桥上相遇。甲走过一段桥面（即占有了一些资源），还需要走其余的桥面（申请新的资源），但那部分桥面被乙占有（乙走过一段桥面）。甲过不去，前进不能，又不后退；乙也处于同样的状况。

    ```
    
    ```

3. 循环等待条件。存在一个进程等待序列{P1，P2，...，Pn}，其中P1等待P2所占有的某一资源，P2等待P3所占有的某一源，......，而Pn等待P1所占有的的某一资源，形成一个进程循环等待环。就像前面的过独木桥问题，甲等待乙占有的桥面，而乙又等待甲占有的桥面，从而彼此循环等待。

    ```
    std::mutex m;
    void f()
    {
    // ....
    std::lock_guard lock(m); // 1 子线程锁住互斥量m
    // ...
    }
    int main()
    {
    std::thread t(f);
    std::lock_guard lock(m); // 2 主线程锁住互斥量m
    // ...
    t.join(); // 3 等待子线程结束
    return 0;
    }
    ```

    上述过程可能导致在2处上锁，然后子线程在1处发生阻塞，最后主线程在3处一直等待子线程结束，无穷等待下去。

  上面提到的这四个条件在死锁时会同时发生。也就是说，只要有一个必要条件不满足，则死锁就可以排除。

#### 1.29 说一说你对自旋锁的理解。

**参考回答**

旋锁的定义：当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用循环加锁 -> 等待的机制被称为自旋锁(spinlock)。

**自旋锁有以下特点**

- 用于临界区互斥
- 在任何时刻最多只能有一个执行单元获得锁
- 要求持有锁的处理器所占用的时间尽可能短
- 等待锁的线程进入忙循环

**自旋锁存在的问题**

- 如果某个线程持有锁的时间过长，就会导致其它等待获取锁的线程进入循环等待，消耗CPU。使用不当会造成CPU使用率极高。
- 无法满足等待时间最长的线程优先获取锁。不公平的锁就会存在“线程饥饿”问题。

**自旋锁的优点**

- 自旋锁不会使线程状态发生切换，一直处于用户态，即线程一直都是active的；不会使线程进入阻塞状态，减少了不必要的上下文切换，执行速度快
- 非自旋锁在获取不到锁的时候会进入阻塞状态，从而进入内核态，当获取到锁的时候需要从内核态恢复，需要线程上下文切换。（线程被阻塞后便进入内核（Linux）调度状态，这个会导致系统在用户态与内核态之间来回切换，严重影响锁的性能）

**自旋锁与互斥锁的区别**

- 自旋锁与互斥锁都是为了实现保护资源共享的机制。
- 无论是自旋锁还是互斥锁，在任意时刻，都最多只能有一个保持者。
- 获取互斥锁的线程，如果锁已经被占用，则该线程将进入睡眠状态；获取自旋锁的线程则不会睡眠，而是一直循环等待锁释放。

#### 1.30 说一说你对悲观锁的理解。

**参考回答**

悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。

#### 1.31 说一说你对乐观锁的理解。

**参考回答**

乐观锁总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。**乐观锁适用于多读的应用类型，这样可以提高吞吐量**，像数据库提供的类似于**write_condition机制**，其实都是提供的乐观锁。

#### 1.32 CAS在什么地方用到过吗？

**参考回答**

- CAS是英文单词**CompareAndSwap**的缩写，中文意思是：比较并替换。CAS需要有3个操作数：内存地址V，旧的预期值A，即将要更新的目标值B。CAS指令执行时，当且仅当内存地址V的值与预期值A相等时，将内存地址V的值修改为B，否则就什么都不做。整个比较并替换的操作是一个原子操作。

- 高并发环境下，对同一个数据的**并发读**（两边都读出余额是100）与**并发写**（一个写回28，一个写回38）导致的数据一致性问题。

    解决方案是在set写回的时候，加上初始状态的条件compare，只有初始状态不变时，才允许set写回成功，这是一种常见的降低读写锁冲突，保证数据一致性的方法。

#### 1.33 谈谈IO多路复用。

**参考回答**

1. IO多路复用是一种同步IO模型，实现一个线程可以监视多个文件句柄；一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；没有文件句柄就绪时会阻塞应用程序，交出cpu。多路是指网络连接，复用指的是同一个线程。

2. IO多路复用有三种实现方式:select, poll, epoll

    (1)select：时间复杂度O(n)，它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以**select具有O(n)的无差别轮询复杂度**，同时处理的流越多，无差别轮询时间就越长。

    ```
     int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
    ```

    select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。

    select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率的降低。

(2)poll：时间复杂度O(n)，poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态， **但是它没有最大连接数的限制**，原因是它是基于链表来存储的。

```
int poll (struct pollfd *fds, unsigned int nfds, int timeout);
```

不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。

```
struct pollfd {     int fd; /* file descriptor */     short events; /* requested events to watch */     short revents; /* returned events witnessed */ };
```

pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。

(3)epoll：时间复杂度O(1)，**epoll可以理解为event poll**，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以说epoll实际上是**事件驱动（每个事件关联上fd）** 的，此时对这些流的操作都是有意义的。

epoll操作过程需要三个接口，分别如下：

```
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

- int epoll_create(int size)：创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。
- int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)：函数是对指定描述符fd执行op操作。- epfd：是epoll_create()的返回值。- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。- fd：是需要监听的fd（文件描述符）- epoll_event：是告诉内核需要监听什么事，
- int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout)：等待epfd上的io事件，最多返回maxevents个事件。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。

1. select、poll、epoll区别

- 支持一个进程所能打开的最大连接数

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696823831/B2F31997E8A28ACD6336D7CE66F256F2)

- FD剧增后带来的IO效率问题

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696832957/8FAD236F286CCF88EEB9B38FAABDC695)

- 消息传递方式

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696842318/E751EB97B99B7E907B992417087DF763)

#### 1.34 谈谈poll和epoll的区别。

**参考回答**

1. poll将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。

    它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有缺点：

    1. 大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。
    2. poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。

2. epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

    epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。其优点有：

    1. 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。
    2. 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。
    3. 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。

#### 1.35 谈谈select和epoll的区别。

**参考回答**

1. select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。

    select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但是这样也会造成效率的降低。

    内核需要传递消息到用户空间，需要内存拷贝

2. 相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

    epoll能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。

​    效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。

#### 1.36 epoll有哪两种模式？

**参考回答**

**epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：**

LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。

ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。

1. **LT模式**

LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket。在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。

1. **ET模式**

ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)。

ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

#### 1.37 说一下epoll的原理，它的查询速度是O(1)的吗？

**参考回答**

epoll是一种更加高效的IO多路复用的方式，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。时间复杂度为O(1)。

epoll的执行过程如图所示：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696856366/9FF8FCB6122C30FCBCF06947168EB5CF)

1. 创建红黑树，调用epoll_create()创建一颗空的红黑树，用于存放FD及其感兴趣事件；
2. 注册感兴趣事件，调用epoll_ctl()向红黑树中添加节点（FD及其感兴趣事件），时间复杂度O(logN)，向内核的中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它添加到就绪队列中。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到就绪队列中了；
3. 获取就绪事件，调用epoll_wait()返回就绪队列中的就绪事件，时间复杂度O(1)；

#### 1.38 介绍域名解析成IP的全过程。

**参考回答**

**第一步：检查浏览器缓存中是否缓存过该域名对应的IP地址**

用户通过浏览器浏览过某网站之后，浏览器就会自动缓存该网站域名对应的地址，当用户再次访问的时候，浏览器就会从缓存中查找该域名对应的IP地址，因为缓存不仅是有大小限制，而且还有时间限制（域名被缓存的时间通过属性来设置），所以存在域名对应的找不到的情况。当浏览器从缓存中找到了该网站域名对应的地址，那么整个解析过程结束，如果没有找到，将进行下一步骤。对于的缓存时间问题，不宜设置太长的缓存时间，时间太长，如果域名对应的发生变化，那么用户将在一段时间内无法正常访问到网站，如果太短，那么又造成频繁解析域名。

**第二步：如果在浏览器缓存中没有找到IP，那么将继续查找本机系统是否缓存过IP**

如果第一个步骤没有完成对域名的解析过程，那么浏览器会去系统缓存中查找系统是否缓存过这个域名对应的地址，也可以理解为系统自己也具备域名解析的基本能力。在系统中，可以通过设置文件来将域名手动绑定到某上，文件位置在。对于普通用户，并不推荐自己手动绑定域名和，对于开发者来说，通过绑定域名和，可以轻松切换环境，可以从测试环境切换到开发环境，方便开发和测试。在系统中，黑客常常修改他的电脑的文件，将用户常常访问的域名绑定到他指定的上，从而实现了本地解析，导致这些域名被劫持。在或者系统中，文件在，修改该文件也可以实现同样的目的。

前两步都是在本机上完成的，所以没有在上面示例图上展示出来，从第三步开始，才正在地向远程DNS服务器发起解析域名的请求。

**第三步：向本地域名解析服务系统发起域名解析的请求**

如果在本机上无法完成域名的解析，那么系统只能请求本地域名解析服务系统进行解析，本地域名系统一般都是本地区的域名服务器，比如你连接的校园网，那么域名解析系统就在你的校园机房里，如果你连接的是电信、移动或者联通的网络，那么本地域名解析服务器就在本地区，由各自的运营商来提供服务。对于本地服务器地址，系统使用命令就可以查看，在和系统下，直接使用命令来查看服务地址。一般都缓存了大部分的域名解析的结果，当然缓存时间也受域名失效时间控制，大部分的解析工作到这里就差不多已经结束了，负责了大部分的解析工作。

**第四步：向根域名解析服务器发起域名解析请求**

本地域名解析器还没有完成解析的话，那么本地域名解析服务器将向根域名服务器发起解析请求。

**第五步：根域名服务器返回gTLD域名解析服务器地址**

本地域名解析向根域名服务器发起解析请求，根域名服务器返回的是所查域的通用顶级域（）地址，常见的通用顶级域有、、、等。

**第六步：向gTLD服务器发起解析请求**

本地域名解析服务器向gTLD服务器发起请求。

**第七步：gTLD服务器接收请求并返回Name Server服务器**

服务器接收本地域名服务器发起的请求，并根据需要解析的域名，找到该域名对应的域名服务器，通常情况下，这个服务器就是你注册的域名服务器，那么你注册的域名的服务商的服务器将承担起域名解析的任务。

**第八步：Name Server服务器返回IP地址给本地服务器**

服务器查找域名对应的地址，将地址连同值返回给本地域名服务器。

**第九步：本地域名服务器缓存解析结果**

本地域名服务器缓存解析后的结果，缓存时间由时间来控制。

#### 1.39 如何在Linux上配置一个IP地址，如果给定端口号如何解析出域名？

**参考回答**

1. 配置Linux系统的IP地址的方法，主要有以下三种：

- ifconfig

    ifconfig 命令主要是用来查看网卡的配置信息，因为用它来配置网卡的IP地址时，只会临时生效（Linux服务器重启后就会失效）

- setup

    setup 命令是 redhat 系列的linux系统（如CentOS）中专有的命令工具。可以使用 setup 命令，来对网络配置中的IP地址、子网掩码、默认网关、DNS服务器进行设置。而且，setup 网络配置工具设置的IP地址会永久生效。

- 修改网卡的配置文件

    直接修改网卡的配置文件，设置方法有两种：

    - 自动获取动态IP地址
    - 手工配置静态的IP地址

1. 使用dig命令解析域名

#### 1.40 解释一下IP地址、子网掩码、网关。

**参考回答**

1. IP地址

IP地址有一个32位的连接地址,由4个8位字段组成,8位字段称为8位位组,每个8位位组之间用点号隔开，用于标识TCP/IP宿主机。每个IP地址都包含两部分:网络ID和主机ID,网络ID 标识在同一个物理网络上的所有宿主机,主机ID标识网络上的每一个宿主机,运行TCP/IP的每个计算机都需要唯一的IP地址。

Intenet委员会定义了五种地址类型以适应不同尺寸的网络。地址类型定义网络ID使用哪些位,它也定义了网络的可能数目和每个网络可能的宿主机数目．

1. 子网掩码(Subnet Mask)

使用子网可以把单个大网分成多个物理网络,并用路由器把它们连接起来。子网掩码用于屏蔽IP地址的一部分,使得TCP/IP能够区别网络ID和宿主机ID。当TCP/IP宿主机要通信时,子网掩码用于判断一个宿主机是在本地网络还是在远程网络。

缺省的子网掩码用于不分成子网的TCP/IP网络,对应于网络ID的所有位都置为1,每个8位位组的十进制数是255,对应于宿主机ID的所有位都置为0。

用于子网掩码的位数决定可能的子网数目和每个子网的宿主机数目,子网掩码的位数越多,则子网越多,但是宿主机也较少。

例:假设A类地址子网数是14,则所需位数至少为4,用于子网的位为: 　　11111111, 11110000, 00000000, 00000000, 子网掩码为255.240.0.0,每个子网的宿主机数目为2^20-2=1,048, 574个。

1. 网关（Gateway）

网关就是一个网络连接到另一个网络的“关口”。 按照不同的分类标准，网关也有很多种。TCP/IP协议里的网关是最常用的，在这里我们所讲的“网关”均指TCP/ IP协议下的网关。

网关实质上是一个网络通向其他网络的IP地址。比如有网络A和网络B，网络A的IP地址范围为“192.168.1.1~192. 168.1.254”，子网掩码为255.255.255.0；网络B的IP地址范围为“192.168.2.1~192. 168.2.254”，子网掩码为255.255.255.0。在没有路由器的情况下，两个网络之间是不能进行TCP/IP通信的，即使是两个网络连接在同一台交换机（或集线器）上，TCP/IP协议也会根据子网掩码（255.255.255.0）判定两个网络中的主机处在不同的网络里。而要实现这两个网络之间的通信，则必须通过网关。

如果网络A中的主机发现数据包的目的主机不在本地网络中，就把数据包转发给它自己的网关，再由网关转发给网络B的网关，网络B的网关再转发给网络B的某个主机。网络B向网络A转发数据包的过程也是如此。而要实现这两个网络之间的通信，则必须通过网关。如果网络A中的主机发现数据包的目的主机不在本地网络中，就把数据包转发给它自己的网关，再由网关转发给网络B的网关，网络B的网关再转发给网络B的某个主机。网络B向网络A转发数据包的过程也是如此 所以说，只有设置好网关的IP地址，TCP/IP协议才能实现不同网络之间的相互通信。那么这个IP地址是哪台机器的IP地址呢？网关的IP地址是具有路由功能的设备的IP地址，具有路由功能的设备有路由器、启用了路由协议的服务器（实质上相当于一台路由器）、代理服务器（也相当于一台路由器）。

#### 1.41 说说IP如何寻址？

**参考回答**

IP寻址包括本地网络寻址和非本地网络寻址两部分

1. 本地网络寻址

    假设有2个主机，他们是属于同一个网段。主机A和主机B，首先主机A通过本机的hosts表或者wins系统或dns系统先将主机B的计算机名转换为IP地址，然后用自己的IP地址与子网掩码计算出自己所出的网段，比较目的主机B的ip地址与自己的子网掩码，发现与自己是出于相同的网段，于是在自己的ARP缓存中查找是否有主机B的mac地址，如果能找到就直接做数据链路层封装并且通过网卡将封装好的以太网帧发送有物理线路上去。

    如果arp缓存中没有主机B的的mac地址，主机A将启动arp协议通过在本地网络上的arp广播来查询主机B的mac地址，获得主机B的mac地址厚写入arp缓存表，进行数据链路层的封装，发送数据。

2. 非本地网络寻址

    假设2个主机不是相同的网段，不同的数据链路层网络必须分配不同网段的IP地址并且由路由器将其连接起来。主机A通过本机的hosts表或wins系统或dns系统先主机B的计算机名转换为IP地址，然后用自己的IP地址与子网掩码计算出自己所处的网段，比较目的目的主机B的IP地址，发现与自己处于不同的网段。于是主机A将知道应该将次数据包发送给自己的缺省网关，即路由器的本地接口。

    主机A在自己的ARP缓存中查找是否有缺省网关的MAC地址，如果能够找到就直接做数据链路层封装并通过网卡，将封装好的以太网数据帧发送到物理线路上去，如果arp缓存表中没有缺省网关的Mac地址，主机A将启动arp协议通过在本地网络上的arp广播来查询缺省网关的mac地址，获得缺省网关的mac地址后写入arp缓存表，进行数据链路层的封装，发送数据。

    数据帧到达路由器的接受接口后首先解封装，变成IP数据包，对IP包进行处理，根据目的IP地址查找路由表，决定转发接口后做适应转发接口数据链路层协议帧的封装，并且发送到下一跳路由器，此过程继续直至到达目的的网络与目的主机。

#### 1.42 操作系统的地址有几种，请具体说明。

**参考回答**

操作系统有物理地址、逻辑地址、线性地址（也叫虚拟地址）三种地址

1. 物理地址

    在存储器里以字节为单位存储信息，为正确地存放或取得信息，每一个字节单元给以一个唯一的存储器地址，称为物理地址（Physical Address），又叫实际地址或绝对地址。

    地址从0开始编号，顺序地每次加1，因此存储器的物理地址空间是呈线性增长的。它是用二进制数来表示的，是无符号整数，书写格式为十六进制数。它是出现在CPU外部地址总线上的寻址物理内存的地址信号，是地址变换的最终结果。用于内存芯片级的单元寻址，与处理器和CPU连接的地址总线相对应。

2. 逻辑地址

    逻辑地址是指在计算机体系结构中是指应用程序角度看到的内存单元（memory cell）、存储单元（storage element）、网络主机（network host）的地址。 逻辑地址往往不同于物理地址（physical address），通过地址翻译器（address translator）或映射函数可以把逻辑地址转化为物理地址。

    在有地址变换功能的计算机中,访问指令给出的地址 (操作数) 叫逻辑地址,也叫相对地址。要经过寻址方式的计算或变换才得到内存储器中的物理地址。把用户程序中使用的地址称为相对地址即逻辑地址。逻辑地址由两个16位的地址分量构成，一个为段基值，另一个为偏移量。两个分量均为无符号数编码。

3. 线性地址

    线性地址（Linear Address）是逻辑地址到物理地址变换之间的中间层。在分段部件中逻辑地址是段中的偏移地址，然后加上基地址就是线性地址。

    线性地址是一个32位无符号整数，可以用来表示高达4GB的地址，也就是，高达4294967296个内存单元。线性地址通常用十六进制数字表示，值的范围从0x00000000到0xffffffff）。程序代码会产生逻辑地址，通过逻辑地址变换就可以生成一个线性地址。如果启用了分页机制，那么线性地址可以再经过变换以产生一个物理地址。当采用4KB分页大小的时候，线性地址的高10位为页目录项在页目录表中的编号，中间10位为页表中的页号，其低12位则为偏移地址。如果是使用4MB分页机制，则高10位页号，低22位为偏移地址。如果没有启用分页机制，那么线性地址直接就是物理地址。

#### 1.43 Linux的静态网络怎么配置？

**参考回答**

网络配置的配置文件在/etc/sysconfig/network-scripts/下，文件名前缀为ifcfg-后面跟的就是网卡的名称，可以使用ifconfig查看，也可以使用命令： ls /etc/sysconfig/network-scripts/ifcfg-* 列出所有的设备配置文件，

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696875652/F754490849B6F7AAD15607BB55D45D91)

比如这里就是ifcfg-eno16777984这个文件，ifcfg-lo是本地回环地址的配置文件，所有计算机都有，不用动他，

现在使用： vim /etc/sysconfig/network-scripts/ifcfg-eno16777984 打开配置文件进行编辑，默认情况是dhcp动态获取的，如下图：

![img](https://uploadfiles.nowcoder.com/images/20190919/56_1568900435177_29C080A5413E925FE3B3CCB4048AB99B)

这时候如果想修改成静态的，首先把BOOTPROTO="dhcp"改成BOOTPROTO="static"表示静态获取，然后在最后追加比如下面的配置：

```
BROADCAST=192.168.1.255 IPADDR=192.168.1.33 NETMASK=255.255.255.0 GATEWAY=192.168.1.1
```

BROADCAST设置的是局域网广播地址，IPADDR就是静态IP，NETMASK是子网掩码，GATEWAY就是网关或者路由地址；需要说明，原来还有个NETWORK配置的是局域网网络号，这个是ifcalc自动计算的，所以这里配置这些就足够了，最终配置如下图：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645696886662/A522AFBB9D320E76CDA929723EB08760)

配置完成之后保存退出，

设置完毕，然后使用命令： /etc/init.d/network restart 或者 service network restart 重启网络服务，重启后如果路由配置了支持静态IP，那么linux就能获取到刚才配置的IP地址，这样静态IP就配置成功了

#### 1.44 DNS用了哪些协议？

**参考回答**

1. DNS在进行区域传输的时候使用TCP协议，其它时候则使用UDP协议；

    DNS的规范规定了2种类型的DNS服务器，一个叫主DNS服务器，一个叫辅助DNS服务器。在一个区中主DNS服务器从自己本机的数据文件中读取该区的DNS数据信息，而辅助DNS服务器则从区的主DNS服务器中读取该区的DNS数据信息。当一个辅助DNS服务器启动时，它需要与主DNS服务器通信，并加载数据信息，这就叫做区传送（zone transfer）。 

2. 为什么既使用TCP又使用UDP？

   UDP报文的最大长度为512字节，而TCP则允许报文长度超过512字节。当DNS查询超过512字节时，协议的TC标志出现删除标志，这时则使用TCP发送。通常传统的UDP报文一般不会大于512字节。 

1. 区域传送时使用TCP，主要有以下两点考虑：

    1. 辅域名服务器会定时（一般时3小时）向主域名服务器进行查询以便了解数据是否有变动。如有变动，则会执行一次区域传送，进行数据同步。区域传送将使用TCP而不是UDP，因为数据同步传送的数据量比一个请求和应答的数据量要多得多。 
    2. TCP是一种可靠的连接，保证了数据的准确性。 

2. 域名解析时使用UDP协议： 

    客户端向DNS服务器查询域名，一般返回的内容都不超过512字节，用UDP传输即可。不用经过TCP三次握手，这样DNS服务器负载更低，响应更快。虽然从理论上说，客户端也可以指定向DNS服务器查询的时候使用TCP，但事实上，很多DNS服务器进行配置的时候，仅支持UDP查询包。

#### 1.45 说一说你对Linux内核的了解。

**参考回答**

内核是操作系统的核心，具有很多最基本功能，它负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。

Linux 内核有 4 项工作：

1. **内存管理：** 追踪记录有多少内存存储了什么以及存储在哪里
2. **进程管理：** 确定哪些进程可以使用中央处理器（CPU）、何时使用以及持续多长时间
3. **设备驱动程序：** 充当硬件与进程之间的调解程序/解释程序
4. **系统调用和安全防护：** 从流程接受服务请求

在正确实施的情况下，内核对于用户是不可见的，它在自己的小世界（称为内核空间）中工作，并从中分配内存和跟踪所有内容的存储位置。用户所看到的内容（例如 Web 浏览器和文件则被称为用户空间。这些应用通过系统调用接口（SCI）与内核进行交互。

*举例来说，* 内核就像是一个为高管（硬件）服务的忙碌的个人助理。助理的工作就是将员工和公众（用户）的消息和请求（进程）转交给高管，记住存放的内容和位置（内存），并确定在任何特定的时间谁可以拜访高管、会面时间有多长。

为了更具象地理解内核，不妨将 Linux 计算机想象成有三层结构：

硬件：物理机（这是系统的底层结构或基础）是由内存（RAM）、处理器（或 CPU）以及输入/输出（I/O）设备（例如存储、网络和图形）组成的。其中，CPU 负责执行计算和内存的读写操作。

Linux 内核：操作系统的核心。它是驻留在内存中的软件，用于告诉 CPU 要执行哪些操作。

用户进程：这些是内核所管理的运行程序。用户进程共同构成了用户空间。用户进程有时也简称为进程。内核还允许这些进程和服务器彼此进行通信（称为进程间通信或 IPC）。

系统执行的代码通过以下两种模式之一在 CPU 上运行：内核模式或用户模式。在内核模式下运行的代码可以不受限制地访问硬件，而用户模式则会限制 SCI 对 CPU 和内存的访问。内存也存在类似的分隔情况（内核空间和用户空间）。这两个小细节构成了一些复杂操作的基础，例如安全防护、构建容器和虚拟机的权限分隔。

这也意味着：如果进程在用户模式下失败，则损失有限，无伤大雅，可以由内核进行修复。另一方面，由于内核进程要访问内存和处理器，因此内核进程的崩溃可能会引起整个系统的崩溃。由于用户进程之间会有适当的保护措施和权限要求，因此一个进程的崩溃通常不会引起太多问题。

#### 1.46 说一说你对Linux内核态与用户态的了解。

**参考回答**

内核态其实从本质上说就是内核，它是一种**特殊的软件程序，控制计算机的硬件资源，例如协调CPU资源，分配内存资源，并且提供稳定的环境供应用程序运行**。

用户态就是提供应用程序运行的空间，为了使应用程序访问到内核管理的资源例如CPU，内存，I/O。内核必须提供一组通用的访问接口，这些接口就叫**系统调用。**

1. **系统调用**是操作系统的最小功能单位。根据不同的应用场景，不同的Linux发行版本提供的系统调用数量也不尽相同，大致在240-350之间。这些系统调用组成了用户态跟内核态交互的基本接口。
2. 从用户态到内核态切换可以通过三种方式：
    1. 系统调用：系统调用本身就是中断，但是是软件中断，跟硬中断不同。
    2. 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。
    3. 外设中断：当外设完成用户的请求时，会向CPU发送中断信号。

#### 1.47 Linux负载是什么？

**参考回答**

负载(load)是linux机器的一个重要指标，直观了反应了机器当前的状态。

在UNIX系统中，系统负载是对当前CPU工作量的度量，被定义为特定时间间隔内运行队列中的平均线程数。load average 表示机器一段时间内的平均load。这个值越低越好。负载过高会导致机器无法处理其他请求及操作，甚至导致死机。

top 或 uptime 等命令会输出系统的平均负载 (Load Average)，一般会有三个值，分别代表 1 分钟，5 分钟和 15 分钟的平均负载。

负载记录的是 CPU 的负荷，能对 CPU 造成负荷的是进程（包括线程）的执行。负载的数值代表的是 CPU 还没处理完的进程的数目。

系统的负载采用的是指数移动平均，计算方法如下：

```
S(0) = 0 S(t) = a * X(t) + (1-a)*S(t-1)
```

其中，X(t) 为最近一次采样的值，a 为最近采样值占的比重，S(t) 则是系统最近一次采样的负载。

指数移动平均的计算方式会累计历史所有的采样值，但离现在越久，占的比重越小。更具体的，Linux 系统上对 1 分钟的平均负载取 a 的取值2为 1 - e^(-5/60)，5 分钟为 1 - e^(-5s/5min)，以此类推。

以一分钟为例，上面的取值能达到的效果是，最近一分钟的采样占所有历史值的比重约为 63%（准确值为 1 - 1/e），5 分钟和 15 分钟也一样。

单核满载是 1，有 n 核满载是 n。一般说线上运行的系统大于 0.7 的时候就要注意了。

#### 1.48 Linux如何设置开机启动？

**参考回答**

1. 编辑rc.loacl脚本

    linux开机之后会执行/etc/rc.local文件中的脚本。

    所以可以直接在/etc/rc.local中添加启动脚本。

    ```
    $ vim /etc/rc.local
    ```

2. 添加一个开机启动服务。

    将启动脚本复制到 /etc/init.d目录下，并设置脚本权限, 假设脚本为test

    ```
     $ mv test /etc/init.d/test  $ sudo chmod 755 /etc/init.d/test
    ```

    将该脚本放倒启动列表中去

    ```
     $ cd .etc/init.d
     $ sudo update-rc.d test defaults 95
    ```

    注：其中数字95是脚本启动的顺序号，按照自己的需要相应修改即可。在有多个启动脚本，而它们之间又有先后启动的依赖关系时就知道这个数字的具体作用了。

    将该脚本从启动列表中剔除

    ```
     $ cd /etc/init.d
     $ sudo update-rc.d -f test remove
    ```

#### 1.49 谈谈Linux的内存管理。

**参考回答**

常见的计算机存储层次如下：

- 寄存器：CPU提供的，读写ns级别，容量字节级别。
- CPU缓存：CPU和CPU间的缓存，读写10ns级别，容量较大一些，百到千节。
- 主存：动态内存，读写100ns级别，容量GB级别。
- 外部存储介质：磁盘、SSD，读写ms级别，容量可扩展到TB级别。

CPU内的缓存示意图如下：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697008978/1B00F9068B04829EE27116497FB4ECCF)

其中 L1d 和 L1i 都是CPU内部的cache，

- L1d 是数据cache。
- L1i 是指令缓存。
- L2是CPU内部的，不区分指令和数据的。
- 由于现代PC有多个CPU，L3缓存多个核心共用一个。

对于编程人员来说，绝大部分观察主存和外部存储介质就可以了。如果要做极致的性能优化，可以关注L1、L2、L3的cache，比如nginx的绑核操作、pthread调度会影响CPU cache等。

**1. 虚拟内存**

物理内存是有限的（即使支持了热插拔）、非连续的，不同的CPU架构对物理内存的组织都不同。这使得直接使用物理内存非常复杂，为了降低使用内存的复杂度，引入了虚拟内存机制。

虚拟内存抽象了应用程序物理内存的细节，只允许物理内存保存所需的信息（按需分页），并提供了一种保护和控制进程间数据共享数据的机制。有了虚拟内存机制之后，每次访问可以使用更易理解的虚拟地址，让CPU转换成实际的物理地址访问内存，降低了直接使用、管理物理内存的门槛。

物理内存按大小被分成页框、页，每块物理内存可以被映射为一个或多个虚拟内存页。这块映射关系，由操作系统的页表来保存，页表是有层级的。层级最低的页表，保存实际页面的物理地址，较高层级的页表包含指向低层级页表的物理地址，指向顶级的页表的地址，驻留在寄存器中。当执行地址转换时，先从寄存器获取顶级页表地址，然后依次索引，找到具体页面的物理地址。

**2. 大页机制**

虚拟地址转换的过程中，需要好几个内存访问，由于内存访问相对CPU较慢，为了提高性能，CPU维护了一个TLB地址转换的cache，TLB是比较重要且珍稀的缓存，对于大内存工作集的应用程序，会因TLB命中率低大大影响到性能。

为了减少TLB的压力，增加TLB缓存的命中率，有些系统会把页的大小设为MB或者GB，这样页的数目少了，需要转换的页表项也小了，足以把虚拟地址和物理地址的映射关系，全部保存于TLB中。

**3. 区域概念**

通常硬件会对访问不同的物理内存的范围做出限制，在某些情况下设备无法对所有的内存区域做DMA。在其他情况下，物理内存的大小也会超过了虚拟内存的最大可寻址大小，需要执行特殊操作，才能访问这些区域。这些情况下，Linux对内存页的可能使用情况将其分组到各自的区域中（方便管理和限制）。比如ZONE_DMA用于指明哪些可以用于DMA的区域，ZONE_HIGHMEM包含未永久映射到内核地址空间的内存，ZONE_NORMAL标识正常的内存区域。

**4. 节点**

多核CPU的系统中，通常是NUMA系统（非统一内存访问系统）。在这种系统中，内存被安排成具有不同访问延迟的存储组，这取决于与处理器的距离。每一个库，被称为一个节点，每个节点Linux构建了一个独立的内存管理子系统。一个节点有自己的区域集、可用页和已用页表和各种统计计数器。

**5. page cache**

从外部存储介质中加载数据到内存中，这个过程是比较耗时的，因为外部存储介质读写性能毫秒级。为了减少外部存储设备的读写，Linux内核提供了Page cache。最常见的操作，每次读取文件时，数据都会被放入页面缓存中，以避免后续读取时所进行昂贵的磁盘访问。同样，当写入文件时，数据被重新放置在缓存中，被标记为脏页，定期的更新到存储设备上，以提高读写性能。

**6. 匿名内存**

匿名内存或者匿名映射表示不受文件系统支持的内存，比如程序的堆栈隐式创立的，或者显示通过mmap创立的。

**7. 内存回收**

贯穿系统的生命周期，一个物理页可存储不同类型的数据，可以是内核的数据结构，或是DMA访问的buffer，或是从文件系统读取的数据，或是用户程序分配的内存等。

根据页面的使用情况，Linux内存管理对其进行了不同的处理，可以随时释放的页面，称之为可回收页面，这类页面为：页面缓存或者是匿名内存（被再次交换到硬盘上）

大多数情况下，保存内部内核数据并用DMA缓冲区的页面是不能重新被回收的，但是某些情况下，可以回收使用内核数据结构的页面。例如：文件系统元数据的内存缓存，当系统处于内存压力情况下，可以从主存中丢弃它们。

释放可回收的物理内存页的过程，被称之为回收，可以同步或者异步的回收操作。当系统负载增加到一定程序时，kswapd守护进程会异步的扫描物理页，可回收的物理页被释放，并逐出备份到存储设备。

**8. compaction**

系统运行一段时间，内存就会变得支离破碎。虽然使用虚拟村内可以将分散的物理页显示为连续的物理页，但有时需要分配较大的物理连续内存区域。比如设备驱动程序需要一个用于DMA的大缓冲区时，或者大页内存机制分页时。内存compact可以解决了内存碎片的问题，这个机制将被占用的页面，从内存区域合适的移动，以换取大块的空闲物理页的过程，由kcompactd守护进程完成。

**9. OOM killer**

机器上的内存可能会被耗尽，并且内核将无法回收足够的内存用于运行新的程序，为了保存系统的其余部分，内核会调用OOM killer杀掉一些进程，以释放内存。

1. 段页机制

段页机制是操作系统管理内存的一种方式，简单的来说，就是如何管理、组织系统中的内存。要理解这种机制，需要了解一下内存寻址的发展历程。

- 直接寻址：早期的内存很小，通过硬编码的形式，直接定位到内存地址。这种方式有着明显的缺点：可控性弱、难以重定位、难以维护
- 分段机制：8086处理器，寻址空间达到1MB，即地址线扩展了20位，由于制作20位的寄存器较为困难，为了能在16位的寄存器的基础上，寻址20位的地址空间，引入了段的概念，即内存地址=段基址左移4位+偏移
- 分页机制：随着寻址空间的进一步扩大、虚拟内存技术的引入，操作系统引入了分页机制。引入分页机制后，逻辑地址经过段机制转换得到的地址仅是中间地址，还需要通过页机制转换，才能得到实际的物理地址。逻辑地址 -->(分段机制) 线性地址 -->(分页机制) 物理地址。

#### 1.50 谈谈内存映射文件。

**参考回答**

1. **内存映射（mmap）** 是一种内存映射文件的方法，即将一个文件或者其他对象映射到进程的地址空间，实现文件磁盘地址和应用程序进程虚拟地址空间中一段虚拟地址的一一映射关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写藏页面到对应的文件磁盘上。应用程序处理映射部分如同访问主存。

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697041234/99D4AF0FEF8B919164E14A385C404BB4)

1. mmap内存映射原理

（1）线程启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域。

先在用户空间调用库函数mmap，并在进程当前进程的虚拟地址空间中，寻找一段空闲的满足要求的连续虚拟地址作为内存虚拟映射区域，对此区域初始化并插入进程的虚拟地址区域链表或树中。

（2）系统在内核空间调用内核函数mmap，实现文件物理地址和进程虚拟地址之间的一一映射关系。

（3）进程发起堆这片映射空间的访问

进程读写操作访问虚拟地址，查询页表，发现这一段地址并不在内存的物理页面上，因为虽然建立了映射关系，但是还没有将文件从磁盘移到内存中。由此发生缺页中断，内核请求从磁盘调入页面。调页过程先在交换缓存空间（swap cache）中查找，若没有则通过nopage函数把缺失页从磁盘调入内存。之后进程会对其做读写操作，若写操作改变了页面内容，一段时间后系统会自动回写脏页面到磁盘中。(修改过的脏页面不会立即更新到文件中，可以调用msync来强制同步，写入文件)

1. mmap和分页文件操作的区别

区别在于分页文件操作在进程访存时是需要先查询页面缓存 **(page cache)** 的，若发生缺页中断，需要通过inode定位文件磁盘地址，先把缺失文件复制到page cache，再从page cache复制到内存中，才能进行访问。这样访存需要经过两次文件复制，写操作也是一样。总结来说，常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。**但mmap的优势在于，把磁盘文件与进程虚拟地址做了映射，这样可以跳过page cache，只使用一次数据拷贝。**

#### 1.51 谈谈虚拟内存模型。

**参考回答**

​    虚拟内存分成五大区，分别为**栈区、堆区、全局区（静态区）、文字常量区（常量存储区）、程序代码区**。五大区特性如下：

1. 栈区（stack）： 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。
2. 堆区（heap）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。
3. 全局区（静态区）（static）：全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。程序结束后由系统释放。
4. 文字常量区（**常量存储区**） ：**常量字符串**就是放在这里的。 程序结束后由系统释放。这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改。
5. 程序代码区：存放函数体的二进制代码。

**答案解析**

​    以32位的操作系统为例，32位的操作系统每个进程对应的虚拟内存为4G（2的32次方），其中内核区1G，用户区3G。结构图如下：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697053852/D15FA7E6CF859F2840E883811E9B7E43)

#### 1.52 什么是物理内存和虚拟内存，为什么要有虚拟内存？

**参考回答**

1. 物理内存及虚拟内存定义

    ​    **物理内存**是相对于虚拟内存而言的。物理内存指通过物理内存条而获得的内存空间，而**虚拟内存**则是指将硬盘的一块区域划分来作为内存。内存主要作用是在计算机运行时为操作系统和各种程序提供临时储存。

2. **为什么要有虚拟内存**

    ​    在早期的计算机中，要运行一个程序，会把这些程序全都装入内存，程序都是直接运行在内存上的，也就是说程序中访问的内存地址都是实际的物理内存地址。当计算机同时运行多个程序时，必须保证这些程序用到的内存总量要小于计算机实际物理内存的大小。    **早期内存分配方法实例：**

    ​    某台计算机总的内存大小是 128M ，现在同时运行两个程序 A 和 B ， A 需占用内存 10M ， B 需占用内存 110 。计算机在给程序分配内存时会采取这样的方法：先将内存中的前 10M 分配给程序 A ，接着再从内存中剩余的 118M 中划分出 110M 分配给程序 B 。这种分配方法可以保证程序 A 和程序 B 都能运行，但是这种简单的内存分配策略问题很多。如下图：

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697069636/72D62CC6E565806AF1FC7F447166E82F)

    ​                                                                                            早期内存分配方法

    ​    早期的内存分配方法存在如下几个问题**（为什么要有虚拟内存的原因）**：

    ​    **问题 1** ：进程地址空间不隔离。由于程序都是直接访问物理内存，所以恶意程序可以随意修改别的进程的内存数据，以达到破坏的目的。有些非恶意的，但是有 bug 的程序也可能不小心修改了其它程序的内存数据，就会导致其它程序的运行出现异常。这种情况对用户来说是无法容忍的，因为用户希望使用计算机的时候，其中一个任务失败了，至少不能影响其它的任务。

    ​    **问题 2** ：内存使用效率低。在 A 和 B 都运行的情况下，如果用户又运行了程序 C，而程序 C 需要 20M 大小的内存才能运行，而此时系统只剩下 8M 的空间可供使用，所以此时系统必须在已运行的程序中选择一个将该程序的数据暂时拷贝到硬盘上，释放出部分空间来供程序 C 使用，然后再将程序 C 的数据全部装入内存中运行。可以想象得到，在这个过程中，有大量的数据在装入装出，导致效率十分低下。

    ​    **问题 3** ：程序运行的地址不确定。当内存中的剩余空间可以满足程序 C 的要求后，操作系统会在剩余空间中随机分配一段连续的 20M 大小的空间给程序 C 使用，因为是随机分配的，所以程序运行的地址是不确定的。

3. 虚拟内存的实现（可以在页式或段式内存管理的基础上实现）

    ​    （1）在装入程序时，不必将其全部装入到内存，而只需将当前要执行的部分页面或段装入到内存，就可让程序开始执行；

    ​    （2）在程序执行过程中，如果需执行的指令或访的数据尚未在内存（称为缺页或缺段)，则由处理器通知操作系线将相应的页面或段调入到内存，然后继续执订程序；

    ​    （3）另一方面，操作系统将内存中暂时不用的页面或段调出保存在外存上，从而腾出更多空困空间存放将要装入的程字以及将要调入的页画或段。

    ​    **虚拟技术基本特征：**大的用户空间（物理内存和外存相结合形成虚拟空间）、部分交换（调入和调出是对部分虚拟地址空间进行的）、不连续性（物理内存分配的不连续，虚拟地址空间使用的不连续）。

**答案解析**

1. **分段**

    ​    为了解决早期内存分配方式带来的问题，人们想到了一种变通的方法，就是增加一个中间层，利用一种间接的地址访问方法访问物理内存。按照这种方法，程序中访问的内存地址不再是实际的物理内存地址，而是一个虚拟地址，然后由操作系统将这个虚拟地址映射到适当的物理内存地址上。这样，只要操作系统处理好虚拟地址到物理内存地址的映射，就可以保证不同的程序最终访问的内存地址位于不同的区域，彼此没有重叠，就可以达到内存地址空间隔离的效果。

    ​    当创建一个进程时，操作系统会为该进程分配一个 4GB 大小的虚拟进程地址空间。之所以是 4GB ，是因为在 32 位的操作系统中，一个指针长度是 4 字节，而 4 字节指针的寻址能力是从 0x00000000~0xFFFFFFFF，最大值 0xFFFFFFFF 表示的即为 4GB 大小的容量。与虚拟地址空间相对的，还有一个物理地址空间，这个地址空间对应的是真实的物理内存。如果你的计算机上安装了 512M 大小的内存，那么这个物理地址空间表示的范围是 0x00000000~0x1FFFFFFF 。当操作系统做虚拟地址到物理地址映射时，只能映射到这一范围，操作系统也只会映射到这一范围。当进程创建时，每个进程都会有一个自己的 4GB 虚拟地址空间。要注意的是这个 4GB 的地址空间是“虚拟”的，并不是真实存在的，而且每个进程只能访问自己虚拟地址空间中的数据，无法访问别的进程中的数据，通过这种方法实现了进程间的地址隔离。那是不是这 4GB 的虚拟地址空间应用程序可以随意使用呢？很遗憾，在 Windows 系统下，这个虚拟地址空间被分成了 4 部分： NULL 指针区、用户区、 64KB 禁入区、内核区。

    （1）**NULL指针区** （0x00000000~0x0000FFFF）: 如果进程中的一个线程试图操作这个分区中的数据，CPU就会引发非法访问。他的作用是，调用 malloc 等内存分配函数时，如果无法找到足够的内存空间，它将返回 NULL。而不进行安全性检查。它只是假设地址分配成功，并开始访问内存地址 0x00000000（NULL）。由于禁止访问内存的这个分区，因此会发生非法访问现象，并终止这个进程的运行。

    （2）**用户模式分区** ( 0x00010000~0xBFFEFFFF)：这个分区中存放进程的私有地址空间。一个进程无法以任何方式访问另外一个进程驻留在这个分区中的数据 （相同 exe，通过 copy-on-write 来完成地址隔离）。（在windows中，所有 .exe 和动态链接库都载入到这一区域。系统同时会把该进程可以访问的所有内存映射文件映射到这一分区）。

    （3）**隔离区** (0xBFFF0000~0xBFFFFFFF)：这个分区禁止进入。任何试图访问这个内存分区的操作都是违规的。微软保留这块分区的目的是为了简化操作系统的现实。

    （4）**内核区** (0xC0000000~0xFFFFFFFF)：这个分区存放操作系统驻留的代码。线程调度、内存管理、文件系统支持、网络支持和所有设备驱动程序代码都在这个分区加载。这个分区被所有进程共享。

    ​    应用程序能使用的只是用户区而已，大约 2GB 左右 ( 最大可以调整到 3GB) 。内核区为 2GB ，内核区保存的是系统线程调度、内存管理、设备驱动等数据，这部分数据供所有的进程共享，但应用程序是不能直接访问的。

    ​    **人们之所以要创建一个虚拟地址空间，目的是为了解决进程地址空间隔离的问题。**但程序要想执行，必须运行在真实的内存上，所以，必须在虚拟地址与物理地址间建立一种映射关系。这样，通过映射机制，当程序访问虚拟地址空间上的某个地址值时，就相当于访问了物理地址空间中的另一个值。人们想到了一种分段(Sagmentation) 的方法，它的思想是在虚拟地址空间和物理地址空间之间做一一映射。比如说虚拟地址空间中某个 10M 大小的空间映射到物理地址空间中某个 10M 大小的空间。这种思想理解起来并不难，操作系统保证不同进程的地址空间被映射到物理地址空间中不同的区域上，这样每个进程最终访问到的。

    ​    物理地址空间都是彼此分开的。通过这种方式，就实现了进程间的地址隔离。还是以实例说明，假设有两个进程 A 和 B ，进程 A 所需内存大小为 10M ，其虚拟地址空间分布在 0x00000000 到 0x00A00000 ，进程 B 所需内存为 100M ，其虚拟地址空间分布为 0x00000000 到 0x06400000 。那么按照分段的映射方法，进程 A 在物理内存上映射区域为 0x00100000 到 0x00B00000 ，，进程 B 在物理内存上映射区域为0x00C00000 到 0x07000000 。于是进程 A 和进程 B 分别被映射到了不同的内存区间，彼此互不重叠，实现了地址隔离。从应用程序的角度看来，进程 A 的地址空间就是分布在 0x00000000 到 0x00A00000 ，在做开发时，开发人员只需访问这段区间上的地址即可。应用程序并不关心进程 A 究竟被映射到物理内存的那块区域上了，所以程序的运行地址也就是相当于说是确定的了。 下图显示的是分段方式的内存映射方法：

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697088098/C7137AEDB2D714390171E81F3F88F4FB)

​                                                                                        分段方式的内存映射方法



​    这种分段的映射方法虽然解决了上述中的问题1和问题3，但并没能解决问题2，即内存的使用效率问题。在分段的映射方法中，每次换入换出内存的都是整个程序， 这样会造成大量的磁盘访问操作，导致效率低下。所以这种映射方法还是稍显粗糙，粒度比较大。实际上，程序的运行有局部性特点，在某个时间段内，程序只是访问程序的一小部分数据，也就是说，程序的大部分数据在一个时间段内都不会被用到。基于这种情况，人们想到了粒度更小的内存分割和映射方法，这种方法就是分页 (Paging) 。

1. **分页**

    ​    分页的基本方法是，将地址空间分成许多的页。每页的大小由 CPU 决定，然后由操作系统选择页的大小。目前 Inter 系列的 CPU 支持 4KB 或 4MB 的页大小，而 PC上目前都选择使用 4KB 。按这种选择， 4GB 虚拟地址空间共可以分成 1048576 页， 512M 的物理内存可以分为 131072 个页。显然虚拟空间的页数要比物理空间的页数多得多。

    ​    在分段的方法中，每次程序运行时总是把程序全部装入内存，而分页的方法则有所不同。分页的思想是程序运行时用到哪页就为哪页分配内存，没用到的页暂时保留在硬盘上。当用到这些页时再在物理地址空间中为这些页分配内存，然后建立虚拟地址空间中的页和刚分配的物理内存页间的映射。下面通过介绍一个可执行文件的装载过程来说明分页机制的实现方法。

    ​    一个可执行文件 (PE 文件 ) 其实就是一些编译链接好的数据和指令的集合，它也会被分成很多页，在 PE 文件执行的过程中，它往内存中装载的单位就是页。当一个 PE 文件被执行时，操作系统会先为该程序创建一个 4GB 的进程虚拟地址空间。前面介绍过，虚拟地址空间只是一个中间层而已，它的功能是利用一种映射机制将虚拟地址空间映射到物理地址空间，所以，创建 4GB 虚拟地址空间其实并不是要真的创建空间，只是要创建那种映射机制所需要的数据结构而已，这种数据结构就是页目和页表。

    ​    当创建完虚拟地址空间所需要的数据结构后，进程开始读取 PE 文件的第一页。在PE 文件的第一页包含了 PE 文件头和段表等信息，进程根据文件头和段表等信息，将 PE 文件中所有的段一一映射到虚拟地址空间中相应的页 (PE 文件中的段的长度都是页长的整数倍 ) 。这时 PE 文件的真正指令和数据还没有被装入内存中，操作系统只是据 PE 文件的头部等信息建立了 PE 文件和进程虚拟地址空间中页的映射关系而已。当 CPU 要访问程序中用到的某个虚拟地址时，当 CPU 发现该地址并没有相相关联的物理地址时， CPU 认为该虚拟地址所在的页面是个空页面， CPU 会认为这是个页错误 (Page Fault) ， CPU 也就知道了操作系统还未给该 PE 页面分配内存，CPU 会将控制权交还给操作系统。操作系统于是为该 PE 页面在物理空间中分配一个页面，然后再将这个物理页面与虚拟空间中的虚拟页面映射起来，然后将控制权再还给进程，进程从刚才发生页错误的位置重新开始执行。由于此时已为 PE 文件的那个页面分配了内存，所以就不会发生页错误了。随着程序的执行，页错误会不断地产生，操作系统也会为进程分配相应的物理页面来满足进程执行的需求。

    ​    分页方法的核心思想就是当可执行文件执行到第 x 页时，就为第 x 页分配一个内存页 y ，然后再将这个内存页添加到进程虚拟地址空间的映射表中 , 这个映射表就相当于一个 y=f(x) 函数。应用程序通过这个映射表就可以访问到 x 页关联的 y 页了。

#### 1.53 内存和缓存有什么区别？

**参考回答**

​    内存和缓存是计算机不同的组成部件。

1. 内存特性

    ​    内存也被称作内存储器，其作用是用于暂时存放CPU的运算数据，以及与硬盘等外部存储交换的数据。只要计算机在运行中，CPU就会把需要进行运算的数据调到内存中进行运算，当运算完成后CPU再将结果传送出来，内存的运行也决定了计算机的稳定运行。

2. 缓存特性

    ​    CPU芯片面积和成本的因素影响，决定了缓存都很小。现在一般的缓存不过几M，CPU缓存的运行频率极高，一般是和处理器同频运作，工作效率远远大于系统内存和硬盘。实际工作时，CPU往往需要重复读取读取同样的数据块，而缓存容量的增大，可以大幅度提升CPU内部读取数据的命中率，而不用再到内存或者硬盘上寻找，以此提高系统性能。

#### 1.54 请你说说缓存溢出。

**参考回答**

1. 缓存溢出及其危害

​    **缓存溢出**是指输入到一个缓冲区或者数据保存区域的数据量超过了其容量，从而导致覆盖了其它区域数据的状况。攻击者造成并利用这种状况使系统崩溃或者通过插入特制的代码来控制系统。被覆盖的区域可能存有其它程序的变量、参数、类似于返回地址或指向前一个栈帧的指针等程序控制流数据。缓冲区可以位于堆、栈或进程的数据段。这种错误可能产生如下后果：

​    （1）破坏程序的数据；

​    （2）改变程序的控制流，因此可能访问特权代码。

​    最终很有可能造成程序终止。当攻击者成功地攻击了一个系统之后，作为攻击的一部分，程序的控制流可能会跳转到攻击者选择的代码处，造成的结果是被攻击的进程可以执行任意的特权代码（比如通过判断输入是否和密码匹配来访问特权代码，如果存在缓冲区漏洞，非法输入导致存放“密码”的内存区被覆盖，从而使得“密码”被改写，因此判断为匹配进而获得了特权代码的访问权）

​    缓冲区溢出攻击是最普遍和最具危害性的计算机安全攻击类型之一。

1. 如何预防缓存溢出

    广义上分为**两类**：

    （1）编译时防御系统，目的是强化系统以抵御潜伏于新程序中的恶意攻击

    （2）运行时预防系统，目的是检测并终止现有程序中的恶意攻击

#### 1.55 深拷贝和浅拷贝的区别是什么，它们各自的使用场景是什么？

**参考回答**

​    **浅拷贝**只是对指针的拷贝，拷贝后两个指针指向同一个内存空间；**深拷贝**不断对指针进行拷贝，而且对指针指向的内容进行拷贝，经深拷贝后的指针是指向两个不同的地址空间。

1. 浅拷贝

    ​    对一个已知对象进行拷贝时，编译系统会自动调用一次构造函数（拷贝构造函数），如果用户未定义拷贝构造函数，则会调用默认拷贝构造函数，调用一次构造函数，调用两次析构函数，两个对象的指针成员所指内存相同，但是程序结束时该内存被释放了两次，会造成内存泄漏问题。

2. 深拷贝

    ​    在对含有指针成员的对象进行拷贝时，必须要自己定义拷贝构造函数，使拷贝后的对象指针成员有自己的内存空间，即进行深拷贝，这样就避免了内存泄漏的发生，调用一次构造函数，一次自定义拷贝构造函数，两次析构函数。两个对象的指针成员所指内容不同。

#### 1.56 说说IO模型。

**参考回答**

1. 什么是IO

    ​    我们都知道unix世界里，一切皆文件。而文件是什么呢？文件就是一串二进制流而已。无论是socket，还是FIFO、管道、终端，对我们来说，一切都是文件，一切都是流。在信息交换的过程中，我们都是对这些流进行数据的收发操作简称为**I/O操作**(input and output)。往流中读出数据，系统调用read；写入数据，系统调用write。

    ​    计算机里有这么多的流，我怎么知道要操作哪个流呢？

    ​    做到这个的就是**文件描述符**，即通常所说的fd，一个fd就是一个整数，所以对这个整数的操作就是对这个文件（流）的操作。我们创建一个socket，通过系统调用会返回一个文件描述符，那么剩下对socket的操作就会转化为对这个描述符的操作。不能不说这又是一种分层和抽象的思想。

2. IO交互

    ​    对于一个输入操作来说，进程IO系统调用后，内核会先看缓冲区有没有相应的缓存数据，没有的话再到设备中读取，因为设备IO一般速度较慢，需要等待，内核缓冲区有数据则直接复制到进程空间。所以，对于一个网络输入操作通常包括**两个不同阶段**：

    （1）等待网络数据到达网卡->读取到内核缓冲区

    （2）从内核缓冲区复制数据->用户空间

    ​    IO有内存IO、网络IO和磁盘IO三种，通常我们所说的IO指的是**网络IO**和**磁盘IO**两者。

3. 五大I/O模型

    Linux有五大I/O模型，分别为**阻塞IO、同步非阻塞IO、IO多路复用、信号驱动IO、异步IO**。五种IO模型特性分别如下：

    （1）阻塞IO（blocking IO）

    ​    最传统的一种IO模型，即在读写数据过程中会发生阻塞现象。

    ​    当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态。

    ​    典型的阻塞IO模型的例子为：

    ​    data = socket.read();

    ​    如果数据没有就绪，就会一直阻塞在read方法。

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697176069/15E8A71DD02F5934C2D617340EABD097)

​                                                                                                 阻塞I/O模型

（2）同步非阻塞IO（nonblocking IO)

​    当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。如果结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。

​    所以事实上，在非阻塞IO模型中，用户线程需要不断地询问内核数据是否就绪，也就说非阻塞IO不会交出CPU，而会一直占用CPU。

典型的非阻塞IO模型一般如下：

```
while(true){    data = socket.read();   if(data!= error){  处理数据  break;  } }
```

​    但是对于非阻塞IO就有一个非常严重的问题，在while循环中需要不断地去询问内核数据是否就绪，这样会导致CPU占用率非常高，因此一般情况下很少使用while循环这种方式来读取数据。

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697189029/0347B150DCD58A539420C4EEFDA38DDA)

​                                                                                                同步非阻塞I/O模型

（3）IO多路复用（IO multiplexing）

​    多路复用IO模型是目前使用得比较多的模型。Java NIO实际上就是多路复用IO。

​    在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线    程和    进程，并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用。

​    在Java NIO中，是通过selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里，因此这种方式会导致用户线程的阻塞。

​    也许有朋友会说，我可以采用多线程+ 阻塞IO 达到类似的效果，但是由于在多线程 + 阻塞IO 中，每个socket对应一个线程，这样会造成很大的资源占用，并且尤其是对于长连接来说，线程的资源一直不会释放，如果后面陆续有很多连接的话，就会造成性能上的瓶颈。

​    而多路复用IO模式，通过一个线程就可以管理多个socket，只有当socket真正有读写事件发生才会占用资源来进行实际的读写操作。因此，多路复用IO比较适合连接数比较多的情况。

另外多路复用IO为何比非阻塞IO模型的效率高是因为在非阻塞IO中，不断地询问socket状态时通过用户线程去进行的，而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多。

​    **注意:**多路复用IO模型是通过轮询的方式来检测是否有事件到达，并且对到达的事件逐一进行响应。因此对于多路复用IO模型来说，一旦事件响应体很大，那么就会导致后续的事件迟迟得不到处理，并且会影响新的事件轮询。

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697204755/99AAF975C142C12C64599E1CC4EA2A26)

​                                                                                            I/O多路复用模型

（4）信号驱动IO（signal driven IO）

​    在信号驱动IO模型中，当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。这个一般用于UDP中，对TCP套接口几乎是没用的，原因是该信号产生得过于频繁，并且该信号的出现并没有说明发生了什么事情。

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697219427/9712953FC0712BDF26B12B1BDB20FEFE)

​                                                                                                信号驱动I/O模型

（5）异步IO（asynchronous IO）

​    异步IO模型才是最理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它收到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，因此不会对用户线程产生任何阻塞。然后，内核会等待数据准备完成，再将数据拷贝到用户线程，当这一切都完成之后，内核会给用户线程发送一个信号，告诉它read操作完成了。也就说用户线程完全不需要关心实际的整个IO操作是如何进行的，只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据了。

​    也就说在异步IO模型中，IO操作的两个阶段都不会阻塞用户线程，这两个阶段都是由内核自动完成，然后发送一个信号告知用户线程操作已完成。用户线程中不需要再次调用IO函数进行具体的读写。这点是和信号驱动模型有所不同的，在信号驱动模型中，当用户线程接收到信号表示数据已经就绪，然后需要用户线程调用IO函数进行实际的读写操作；而在异步IO模型中，收到信号表示IO操作已经完成，不需要再在用户线程中调用IO函数进行实际的读写操作。

​    **注意:**异步IO是需要操作系统的底层支持，在Java 7中，提供了Asynchronous IO（简称AIO）。

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697244750/2DE5E6A07632CB924A60FF35C1183DEB)

​                                                                                                 异步I/O模型

​    前四种IO模型实际上都属于同步IO，只有最后一种是真正的异步IO，因为无论是多路复用IO还是信号驱动模型，IO操作的第2个阶段都会引起用户线程阻塞，也就是内核进行数据拷贝的过程都会让用户线程阻塞。

#### 1.57 Linux中的软链接和硬链接有什么区别？

**参考回答**

1. inode概念

    ​    inode是文件系统中存储文件元信息的区域，中文叫节点索引，每个节点索引包含了文件的创建者，大小，日期等等。可以通过ls -i file 命令查看inode的值。

2. 根据 Linux 系统存储文件的特点，链接的方式分为**软链接和硬链接**2 种

    ​    **软链接**相当于建立了一个新的快捷方式文件，该文件有自己的名称和inode以及物理存储的文件数据，文件数据里记录着如何跳转的设置数据，访问该快捷文件会被重新定向到原始文件，删除原始文件，软链文件失效；**硬链接**相当于为当前文件名对应的文件再建立了一个文件别名，别名对应的inode以及物理数据都是一样的，一旦建立，我们甚至根本无法区分谁是原始文件的原始名称，删除文件的其中一个名称，文件不会丢失，除非把所有的名称都删除。

    如下图：

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697258986/D5F8962059E63DAAE281C4CB9FABBC8D)

    ​    hard link(硬链) 和file 都指向同一个 inode，inode对应了一个实际物理存储的文件。soft link（软链） 对应一个新的inode， 新的inode对应一个新的物理存储文件，物理存储文件又指向了目标文件 file。

3. **软连接和硬链接的区别**

    (1) **软链接**可以为文件和目录（哪怕是不存在的）创建链接；**硬链接**只能为文件创建链接。 (2) **软链接**可以跨文件系统；**硬链接**必须是同一个文件系统 (3) **硬链接**因为只是文件的一个别名，所以不重复占用内存；**软链接**因为只是一个访问文件的快捷方式文件，文件内只包含快捷指向信息，所以占用很小的内存。 (4) **软链接**的文件权限和源文件可以不一样；**硬链接**文件权限肯定是一样的，因为他们本来就是一个文件的不同名称而已。

4. 二者使用场景

    ​    一般比较重要的文件我们担心文件被误删除且传统复制备份方式占用double数量的空间会造成浪费，可以使用硬链做备份来解决；软链接一般被用来设置可执行文件的快捷方式的路径。

**答案解析**

1. 创建硬链接

    ```
    [root@localhost ~]# touch cangls [root@localhost ~]# ln /root/cangls /tmp #建立硬链接文件，目标文件没有写文件名，会和原名一致 #也就是/tmp/cangls 是硬链接文件
    ```

2. 创建软链接

    ```
    [root@localhost ~]# touch bols [root@localhost ~]# In -s /root/bols /tmp #建立软链接文件
    ```

    **注意：**软链接文件的源文件必须写成绝对路径，而不能写成相对路径（硬链接没有这样的要求）；否则软链接文件会报错。

#### 1.58 说说缺页中断机制。

**参考回答**

1. 缺页中断

    ​    在请求分页系统中，可以通过查询页表中的状态位来确定所要访问的页面是否存在于内存中。当所要访问的页面不在内存时，会产生一次**缺页中断**，此时操作系统会根据页表中的外存地址在外存中找到所缺的一页，将其调入内存。 缺页中断的处理流程如下：

    ​    A. 在内存中有空闲物理页面时，分配一物理页帧f，转第E步；

    ​    B. 依据页面置换算法选择被替换的物理页帧f，对应逻辑页q；

    ​    C. 如果q被修改过，则把它写回外存；

    ​    D. 修改q的页表项中驻留位置为0；

    ​    E. 将需要访问的页p装入到物理页f；

    ​    F. 修改p的页表项驻留位为1，物理页帧号为f；

    ​    G. 重新执行产生缺页的指令。

2. **缺页中断与一般的中断存在区别**

    **（1）范围不同**

​        一般中断只需要保护现场然后就直接跳到需及时处理的地方；

​        缺页中断除了保护现场之外，还要判断内存中是否有足够的空间存储所需的页或段，然后再把所需页调进来再使用。

​     **（2）结果不同**

​        一般中断在处理完之后返回时，执行下一条指令；

​        缺页中断返回时，执行产生中断的那一条指令。

​    **（3）次数不同**

​        一般中断只产生一次，发生中断指令后转入相应处理程序进行处理，恢复被中断程序现场;

​        在指令执行期间产生和处理缺页中断信号，一条指令在执行期间，可能产生多次缺页中断。

**答案解析**

​    **产生缺页中断的几种情况**：

1. 当内存管理单元（MMU）中确实没有创建虚拟物理页映射关系，并且在该虚拟地址之后再没有当前进程的线性区（vma）的时候，这将杀掉该进程；
2. 当MMU中确实没有创建虚拟页物理页映射关系，并且在该虚拟地址之后存在当前进程的线性区vma的时候，这很可能是缺页中断，并且可能是栈溢出导致的缺页中断；
3. 当使用malloc/mmap等希望访问物理空间的库函数/系统调用后，由于linux并未真正给新创建的vma映射物理页，此时若先进行写操作，将和2产生缺页中断的情况一样；若先进行读操作虽然也会产生缺页异常，将被映射给默认的零页，等再进行写操作时，仍会产生缺页中断，这次必须分配1物理页了，进入写时复制的流程；
4. 当使用fork等系统调用创建子进程时，子进程不论有无自己的vma，它的vma都有对于物理页的映射，但它们共同映射的这些物理页属性为只读，即linux并未给子进程真正分配物理页，当父子进程任何一方要写相应物理页时，导致缺页中断的写时复制。

#### 1.59 软中断和硬中断有什么区别？

**参考回答**

1. 硬中断

    ​    由与系统相连的外设(比如网卡、硬盘)自动产生的。主要是用来通知操作系统外设状态的变化。比如当网卡收到数据包的时候，就会发出一个中断。我们通常所说的中断指的是硬中断(hardirq)。

2. 软终端

    ​    为了满足实时系统的要求，中断处理应该是越快越好。Linux为了实现这个特点，当中断发生的时候，硬中断处理那些短时间就可以完成的工作，而将那些处理事件比较长的工作，放到中断之后来完成，也就是软中断(softirq)来完成。

3. 中断嵌套

    ​    Linux下**硬中断是可以嵌套的**，但是没有优先级的概念，也就是说任何一个新的中断都可以打断正在执行的中断，但同种中断除外。**软中断不能嵌套**，但相同类型的软中断可以在不同CPU上并行执行。

4. **软中断与硬中断之间的区别**

    （1）**硬中断**是由外部事件引起的因此具有随机性和突发性；**软中断**是执行中断指令产生的，无外面事件中断请求信号，因此软中断的发生不是随机的而是由程序安排好的；

    （2）**硬中断**的中断号是由中断控制器提供的；**软中断**的中断号是由指令直接给出的，无需使用中断控制器。

    （3）**硬中断**的中断响应周期，CPU需要发中断回合信号；**软中断**的中断响应周期，CPU不需要发中断回合信号。

    （4）**硬中断**是可屏蔽的；**软中断**是不可屏蔽的。

#### 1.60 介绍一下你对CopyOnWrite的了解。

**参考回答**

1. CopyOnWrite（写时拷贝技术）

    ​    Linux的fork()使用写时拷贝页来实现新进程的创建，它是一种可推迟甚至避免数据拷贝的技术，开始时内核并不会复制整个地址空间，而是让父子进程共享地址空间，只有在写时才复制地址空间，使得父子进程都拥有独立的地址空间，即资源的复制是在只有需要写入时才会发生。在此之前都是以读的方式去和父进程共享资源，这样，在页根本不会被写入的场景下，fork()立即执行exec()，无需对地址空间进行复制，fork()的实际开销就是复制父进程的一个页表和为子进程创建一个进程描述符，也就是说只有当进程空间中各段的内存内容发生变化时，父进程才将其内容复制一份传给子进程，大大提高了效率。

2. 写时拷贝技术优缺点

    ​    写时拷贝技术是一种很重要的优化手段，核心是懒惰处理实体资源请求，在多个实体资源之间只是共享资源，起初是并不真正实现资源拷贝，只有当实体有需要对资源进行修改时才真正为实体分配私有资源。但写时拷贝技术技术也有它的优点和缺点：

    **优点：**写时拷贝技术可以减少分配和复制大量资源时带来的瞬间延时，但实际上是将这种延时附加到了后续的操作之中。

    **缺点：**写时拷贝技术可以减少不必要的资源分配。比如fork进程时，并不是所有的页面都需要复制，父进程的代码段和只读数据段都不被允许修改，所以无需复制。

**答案解析**

​    写时复制技术详述：

​    现在有一个父进程P1，这是一个主体，那么它是有灵魂也就身体的。现在在其虚拟地址空间（有相应的数据结构表示）上有：正文段，数据段，堆，栈这四个部分，相应的，内核要为这四个部分分配各自的物理块。即：正文段块，数据段块，堆块，栈块。至于如何分配，这是内核去做的事，在此不详述。

1. 现在P1用fork()函数为进程创建一个子进程P2，

    内核：

    （1）复制P1的正文段，数据段，堆，栈这四个部分，注意是其内容相同。

    （2）为这四个部分分配物理块，P2的：正文段－＞PI的正文段的物理块，其实就是不为P2分配正文段块，让P2的正文段指向P1的正文段块，数据段－＞P2自己的数据段块（为其分配对应的块），堆－＞P2自己的堆块，栈－＞P2自己的栈块。如下图所示：从左到右大的方向箭头表示复制内容。

    ​                                         ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697278378/F61CEEB2C15AD97CA366F5A1464CA4F4)

2. 写时复制技术：内核只为新生成的子进程创建虚拟空间结构，它们来复制于父进程的虚拟究竟结构，但是不为这些段分配物理内存，它们共享父进程的物理空间，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间。

    ![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697289695/D91E840D1F92FA5B0D325958DFB9DD1D)

3. vfork()：这个做法更加火爆，内核连子进程的虚拟地址空间结构也不创建了，直接共享了父进程的虚拟空间，当然了，这种做法就顺

    水推舟的共享了父进程的物理空间!

![img](https://uploadfiles.nowcoder.com/images/20220224/4107856_1645697306299/5DF2FB686AADF3B901A609334A3C7E3A)

​    通过以上的分析，相信大家对进程有个深入的认识，它是怎么一层层体现出自己来的，进程是一个主体，那么它就有灵魂与身体，系统必须为实现它创建相应的实体， 灵魂实体与物理实体。这两者在系统中都有相应的数据结构表示，物理实体更是体现了它的物理意义。以下援引LKD

传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单并且效率低下，因为它拷贝的数据也许并不共享，更糟的情况是，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。Linux的fork()使用写时拷贝（copy-on-write）页实现。写时拷贝是一种可以推迟甚至免除拷贝数据的技术。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。只有在需要写入的时候，数据才会被复制，从而使各个进程拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行，在此之前，只是以只读方式共享。这种技术使地址空间上的页的拷贝被推迟到实际发生写入的时候。在页根本不会被写入的情况下—举例来说，fork()后立即调用exec()—它们就无需复制了。fork()的实际开销就是复制父进程的页表以及给子进程创建惟一的进程描述符。在一般情况下，进程创建后都会马上运行一个可执行的文件，这种优化可以避免拷贝大量根本就不会被使用的数据（地址空间里常常包含数十兆的数据）。由于Unix强调进程快速执行的能力，所以这个优化是很重要的。这里补充一点：**Linux COW与exec没有必然联系**。

​    实际上COW技术不仅仅在Linux进程上有应用，其他例如C++的String在有的IDE环境下也支持COW技术，即例如：

```
string str1 = "hello world"; string str2 = str1;
```

之后执行代码:

```
str1[1]='q'; str2[1]='w';
```

​    在开始的两个语句后，str1和str2存放数据的地址是一样的，而在修改内容后，str1的地址发生了变化，而str2的地址还是原来的,这就是C++中的COW技术的应用，不过VS2005似乎已经不支持COW。

#### 1.61 Linux替换文本该如何操作呢？

**参考回答**

1. 通过vi编辑器来替换 **vi/vim 中可以使用 :s 命令来替换字符串。** :s/well/good/ 替换当前行第一个 well 为 good :s/well/good/g 替换当前行所有 well 为 good :n,$s/well/good/ 替换第 n 行开始到最后一行中每一行的第一个 well 为 good :n,$s/well/good/g 替换第 n 行开始到最后一行中每一行所有 well 为 good n 为数字，若 n 为 .，表示从当前行开始到最后一行 :%s/well/good/（等同于 :g/well/s//good/） 替换每一行的第一个 well 为 good :%s/well/good/g（等同于 :g/well/s//good/g） 替换每一行中所有 well 为 good，可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符 :s#well/#good/# 替换当前行第一个 well/ 为 good/ :%s#/usr/bin#/bin#g 可以把文件中所有路径/usr/bin换成/bin

2. 直接替换文件中的字符串。(此法不用打开文件即可替换字符串，而且可以批量替换多个文件。)

    （1）**perl命令替换**，参数含义如下： -a 自动分隔模式，用空格分隔$_并保存到@F中。相当于@F = split ”。分隔符可以使用-F参数指定     -F 指定-a的分隔符，可以使用正则表达式     -e 执行指定的脚本。     -i<扩展名> 原地替换文件，并将旧文件用指定的扩展名备份。不指定扩展名则不备份。     -l 对输入内容自动chomp，对输出内容自动添加换行     -n 自动循环，相当于 while(<>) { 脚本; }     -p 自动循环+自动输出，相当于 while(<>) { 脚本; print; }     **用法示例：**     perl -p -i.bak -e 's/\bfoo\b/bar/g' *.c     将所有C程序中的foo替换成bar，旧文件备份成.bak

    ​    perl -p -i -e "s/shan/hua/g" ./lishan.txt ./lishan.txt.bak     将当前文件夹下lishan.txt和lishan.txt.bak中的“shan”都替换为“hua”

    ​    perl -i.bak -pe 's/(\d+)/ 1 + $1 /ge' file1 file2     将每个文件中出现的数值都加一

    （2）**sed命令下批量替换文件内容**

    ​    格式: sed -i "s/查找字段/替换字段/g" grep 查找字段 -rl 路径 文件名

    ​    -i 表示inplace edit，就地修改文件

    ​     -r 表示搜索子目录

    ​    -l 表示输出匹配的文件名     s表示替换，d表示删除

    ​    **用法示例：**

    ​    sed -i "s/shan/hua/g" lishan.txt

    ​    把当前目录下lishan.txt里的shan都替换为hua

**答案解析**

​    **sed的其他用法如下：**

1. 删除行首空格 sed 's/^[ ]*//g' filename sed 's/^ *//g' filename sed 's/^[[:space:]]*//g' filename
2. 行后和行前添加新行 行后：sed 's/pattern/&\n/g' filename 行前：sed 's/pattern/\n&/g' filename &代表pattern
3. 使用变量替换(使用双引号) sed -e "s/$var1/$var2/g" filename
4. 在第一行前插入文本 sed -i '1 i\插入字符串' filename
5. 在最后一行插入 sed -i '$ a\插入字符串' filename
6. 在匹配行前插入 sed -i '/pattern/ i "插入字符串"' filename
7. 在匹配行后插入 sed -i '/pattern/ a "插入字符串"' filename
8. 删除文本中空行和空格组成的行以及#号注释的行 grep -v ^# filename | sed /^[[:space:]]*$/d | sed /^$/d